{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "precious-column",
   "metadata": {
    "id": "precious-column"
   },
   "source": [
    "# 导包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ivu9Z3WNt4BV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "ivu9Z3WNt4BV",
    "outputId": "a421751c-af12-46f9-a9bd-076e0ee60ef3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.7.10 | packaged by conda-forge | (default, Feb 19 2021, 16:07:37) \\n[GCC 9.3.0]'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys \n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "VNZFCpWIfJ9t",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VNZFCpWIfJ9t",
    "outputId": "404b5dfd-71db-4323-e5a7-e00bf07d5bf9"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "F65Y9KEAiZzD",
   "metadata": {
    "id": "F65Y9KEAiZzD"
   },
   "outputs": [],
   "source": [
    "# !unzip sohu2021_open_data_clean.zip\n",
    "# !unzip chinese_L-12_H-768_A-12.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ZBvllWcDWLZN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZBvllWcDWLZN",
    "outputId": "70daddf4-29ef-48f1-9612-96f639a4c615"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "subject-motorcycle",
   "metadata": {
    "id": "subject-motorcycle"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import sys\n",
    "import re\n",
    "from collections import Counter\n",
    "import random\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "tf.config.run_functions_eagerly(True)\n",
    "tf.get_logger().setLevel(tf.compat.v1.logging.ERROR)\n",
    "from keras.metrics import top_k_categorical_accuracy, binary_accuracy\n",
    "from keras.layers import *\n",
    "from keras.callbacks import *\n",
    "from keras.models import Model, load_model\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.losses import SparseCategoricalCrossentropy, binary_crossentropy\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    TFBertForPreTraining,\n",
    "    TFBertModel,\n",
    ")\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.utils import class_weight\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "patent-winner",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "patent-winner",
    "outputId": "79dceb89-d54d-4a37-dc00-ade7e64c3987"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "067b1c15-860a-477f-953a-f371ca788697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 68250996340265039,\n",
       " name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 10770692224\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 6782669471833480376\n",
       " physical_device_desc: \"device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:05:00.0, compute capability: 6.1\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cdebb3b-13f0-4474-9e85-0d8533b5cc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras import backend as K\n",
    "# K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rolled-process",
   "metadata": {
    "id": "rolled-process"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cubic-statistics",
   "metadata": {
    "id": "cubic-statistics"
   },
   "outputs": [],
   "source": [
    "data_path = \"sohu2021_open_data_clean/\"\n",
    "text_max_length = 512\n",
    "bert_path = r\"chinese_L-12_H-768_A-12\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-portal",
   "metadata": {
    "id": "collect-portal"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overhead-richardson",
   "metadata": {
    "id": "overhead-richardson"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signal-arrow",
   "metadata": {
    "id": "signal-arrow"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-knock",
   "metadata": {
    "id": "clean-knock"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "liked-privilege",
   "metadata": {
    "id": "liked-privilege"
   },
   "source": [
    "# 构建标签表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fresh-aircraft",
   "metadata": {
    "id": "fresh-aircraft"
   },
   "outputs": [],
   "source": [
    "label_to_id = {'0':0, '1':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "removed-medicine",
   "metadata": {
    "id": "removed-medicine"
   },
   "outputs": [],
   "source": [
    "labels = [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pacific-assurance",
   "metadata": {
    "id": "pacific-assurance"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swedish-rescue",
   "metadata": {
    "id": "swedish-rescue"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pediatric-folder",
   "metadata": {
    "id": "pediatric-folder"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anticipated-lunch",
   "metadata": {
    "id": "anticipated-lunch"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "natural-bibliography",
   "metadata": {
    "id": "natural-bibliography"
   },
   "source": [
    "# 构建原数据文本迭代器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "boolean-arrangement",
   "metadata": {
    "id": "boolean-arrangement"
   },
   "outputs": [],
   "source": [
    "def _transform_text(text):\n",
    "   text = text.strip().replace('\\n', '。').replace('\\t', '').replace('\\u3000', '')\n",
    "   return re.sub(r'。+', '。', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "young-facility",
   "metadata": {
    "id": "young-facility"
   },
   "outputs": [],
   "source": [
    "def get_data_iterator(data_path, file_name):\n",
    "    # TODO: 随机取\n",
    "    for category in os.listdir(data_path):\n",
    "        category_path = os.path.join(data_path, category)\n",
    "        if not os.path.isdir(category_path):\n",
    "            continue\n",
    "            \n",
    "        file_path = os.path.join(category_path, file_name)\n",
    "        if not os.path.isfile(file_path):\n",
    "            continue\n",
    "        \n",
    "#         print(file_path)\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line)\n",
    "                \n",
    "                data['source'] = _transform_text(data['source'])\n",
    "                if len(data['source']) == 0:\n",
    "                    print('source:', line, data)\n",
    "                    break\n",
    "#                     continue\n",
    "                    \n",
    "                data['target'] = _transform_text(data['target'])\n",
    "                if len(data['target']) == 0:\n",
    "                    print('target:', line, data)\n",
    "                    break\n",
    "#                     continue\n",
    "                \n",
    "                label_name_list = list(key for key in data.keys() if key[:5]=='label')\n",
    "                if len(label_name_list) != 1:\n",
    "                    print('label_name_list:', line, data)\n",
    "                    break\n",
    "#                     continue\n",
    "                label_name = label_name_list[0]\n",
    "                if data[label_name] not in label_to_id.keys():\n",
    "                    print('label_name:', line, data, label_name)\n",
    "                    break\n",
    "#                     continue\n",
    "                    \n",
    "                yield data['source'], data['target'], label_to_id[data[label_name]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "apparent-incidence",
   "metadata": {
    "id": "apparent-incidence"
   },
   "outputs": [],
   "source": [
    "it = get_data_iterator(data_path, \"train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "soviet-arrangement",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "soviet-arrangement",
    "outputId": "2d42777c-c873-4a8a-c82d-c8b4d08f4819"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('最近 你有没有约上好友一起去看电影？ 观影的感觉是不是很爽…… 不过 消防蜀黍在这里还得啰嗦几句 电影院属于相对密闭的人员密集场所， 一旦发生火灾 极易造成群死群伤 后果不堪设想 01 电影院火灾案例一 2020年8月21日，广东一电影院突发火灾，现场烟雾弥漫，影院工作人员紧急疏散观众。 02 电影院火灾案例二 2020年9月4日，浙江台州一电影院荧幕突然起火，观众紧急撤离。 电影院一般装修豪华 电气线路敷设复杂 可燃易燃物质较多 如果消防安全管理稍有不慎 就有可能引发火灾事故  电影院的火灾危险性：  由于电影院空间高大，空气对流好，舞台可燃物多，一旦发生火灾，火势极易蔓延扩大。火势会通过舞台的幕布、放映室等各个部位的大量可燃物，迅速向吊顶蔓延而形成立体燃烧。 1   电影院由于可燃物多，特别如化纤、海绵等，火灾发生时能产生大量的有毒气体，造成人员中毒伤亡。 2   电影院人员高度密集，环境封闭，能见度低，有序疏散非常困难。 3  那么，在电影院遇到火灾，又怎么逃生呢？ 作为观影者 我们需要了解一些火灾发生时的逃生方法  观影前，应注意影院所在楼层位置，附近有几个安全疏散通道。如果是深夜观影，应问清工作人员，哪个通道直通楼外。 进入影厅后，注意查看是否有其他安全出口，在发生火灾时，可以分散逃生，查看是否有卫生间。 可以在有浓烟的情况下打湿衣物捂住口鼻，尽量减少有毒气体的吸入。 由于影厅内使用的材质多是可燃物质，发生火灾后，火势蔓延迅速，浓烟生成较快，所以逃生时切忌高声呼喊、上蹿下跳，应尽量保持低姿前行，避免大量吸入有毒气体，导致晕厥甚至窒息。 发生火灾时应沉着冷静对待，在工作人员的组织下，往安全出口或进出口方向快速、有序逃生，切忌堵塞通道，造成不必要的人员伤亡。 如果第一时间未能撤离，应选择火势蔓延相反方向进行躲避，尽量打湿衣物，裹住面部，靠近承重墙蹲下或趴下，选择时机逃生或等待救援。 以上内容你学会了吗？',\n",
       " '最近 你有没有约上好友一起去看电影？ 观影的感觉是不是很爽…… 不过 消防蜀黍在这里还得啰嗦几句 电影院属于相对密闭的人员密集场所， 一旦发生火灾 极易造成群死群伤 后果不堪设想 01 电影院火灾案例一 2020年8月21日，广东一电影院突发火灾，现场烟雾弥漫，影院工作人员紧急疏散观众。 02 电影院火灾案例二 2020年9月4日，浙江台州一电影院荧幕突然起火，观众紧急撤离。 电影院一般装修豪华 电气线路敷设复杂 可燃易燃物质较多 如果消防安全管理稍有不慎 就有可能引发火灾事故  电影院的火灾危险性：  由于电影院空间高大，空气对流好，舞台可燃物多，一旦发生火灾，火势极易蔓延扩大。火势会通过舞台的幕布、放映室等各个部位的大量可燃物，迅速向吊顶蔓延而形成立体燃烧。 1   电影院由于可燃物多，特别如化纤、海绵等，火灾发生时能产生大量的有毒气体，造成人员中毒伤亡。 2   电影院人员高度密集，环境封闭，能见度低，有序疏散非常困难。 3  那么，在电影院遇到火灾，又怎么逃生呢？ 作为观影者 我们需要了解一些火灾发生时的逃生方法  观影前，应注意影院所在楼层位置，附近有几个安全疏散通道。如果是深夜观影，应问清工作人员，哪个通道直通楼外。 进入影厅后，注意查看是否有其他安全出口，在发生火灾时，可以分散逃生，查看是否有卫生间。 可以在有浓烟的情况下打湿衣物捂住口鼻，尽量减少有毒气体的吸入。 由于影厅内使用的材质多是可燃物质，发生火灾后，火势蔓延迅速，浓烟生成较快，所以逃生时切忌高声呼喊、上蹿下跳，应尽量保持低姿前行，避免大量吸入有毒气体，导致晕厥甚至窒息。 发生火灾时应沉着冷静对待，在工作人员的组织下，往安全出口或进出口方向快速、有序逃生，切忌堵塞通道，造成不必要的人员伤亡。 如果第一时间未能撤离，应选择火势蔓延相反方向进行躲避，尽量打湿衣物，裹住面部，靠近承重墙蹲下或趴下，选择时机逃生或等待救援。 以上内容你学会了吗？ ▌来源：河南消防 ▌编辑：车晓玲',\n",
       " 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fantastic-perry",
   "metadata": {
    "id": "fantastic-perry"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjusted-strip",
   "metadata": {
    "id": "adjusted-strip"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virtual-sensitivity",
   "metadata": {
    "id": "virtual-sensitivity"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rental-spotlight",
   "metadata": {
    "id": "rental-spotlight"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "significant-person",
   "metadata": {
    "id": "significant-person"
   },
   "source": [
    "# 获取数据集样本个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "perfect-identifier",
   "metadata": {
    "id": "perfect-identifier"
   },
   "outputs": [],
   "source": [
    "def get_sample_num(data_path, file_name):\n",
    "    count = 0\n",
    "    it = get_data_iterator(data_path, file_name)\n",
    "    for data in tqdm(it):\n",
    "        count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "addressed-shanghai",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "addressed-shanghai",
    "outputId": "e60a8526-b0c8-404c-cf12-0ae5c09c04aa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "59638it [00:02, 21011.81it/s]\n"
     ]
    }
   ],
   "source": [
    "train_sample_count = get_sample_num(data_path, \"train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "equivalent-favor",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "equivalent-favor",
    "outputId": "ccb6404d-2571-4211-cc3a-36b4925cf5d1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9940it [00:00, 19764.08it/s]\n"
     ]
    }
   ],
   "source": [
    "dev_sample_count = get_sample_num(data_path, \"valid.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "destroyed-orchestra",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "destroyed-orchestra",
    "outputId": "6af205bb-7fae-4b47-b699-176e0ffaaced"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59638, 9940)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sample_count, dev_sample_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unsigned-regulation",
   "metadata": {
    "id": "unsigned-regulation"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "racial-italy",
   "metadata": {
    "id": "racial-italy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intelligent-hybrid",
   "metadata": {
    "id": "intelligent-hybrid"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-talent",
   "metadata": {
    "id": "mounted-talent"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "medieval-polymer",
   "metadata": {
    "id": "medieval-polymer"
   },
   "source": [
    "# 构建数据迭代器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "italic-membership",
   "metadata": {
    "id": "italic-membership"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(bert_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "above-mobility",
   "metadata": {
    "id": "above-mobility"
   },
   "outputs": [],
   "source": [
    "def _get_indices(text, text_pair=None):\n",
    "    return tokenizer.encode(text=text,\n",
    "                            text_pair=text_pair,\n",
    "                            max_length=text_max_length, \n",
    "                            add_special_tokens=True, \n",
    "                            padding='max_length', \n",
    "#                             truncation_strategy='only_first', \n",
    "                            truncation=True,\n",
    "#                                          return_tensors='tf'\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "sunrise-resistance",
   "metadata": {
    "id": "sunrise-resistance"
   },
   "outputs": [],
   "source": [
    "def get_keras_bert_iterator(data_path, file_name, tokenizer):\n",
    "    while True:\n",
    "        data_it = get_data_iterator(data_path, file_name)\n",
    "        for source, target, label in data_it:\n",
    "            indices = _get_indices(text=source, \n",
    "                                   text_pair=target)\n",
    "            yield indices, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "anonymous-railway",
   "metadata": {
    "id": "anonymous-railway"
   },
   "outputs": [],
   "source": [
    "it = get_keras_bert_iterator(data_path, \"train.txt\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ethical-documentation",
   "metadata": {
    "id": "ethical-documentation"
   },
   "outputs": [],
   "source": [
    "# next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corresponding-glance",
   "metadata": {
    "id": "corresponding-glance"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frequent-twenty",
   "metadata": {
    "id": "frequent-twenty"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norman-track",
   "metadata": {
    "id": "norman-track"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-pillow",
   "metadata": {
    "id": "cordless-pillow"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "rough-poland",
   "metadata": {
    "id": "rough-poland"
   },
   "source": [
    "# 构建批次数据迭代器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "former-earthquake",
   "metadata": {
    "id": "former-earthquake"
   },
   "outputs": [],
   "source": [
    "def batch_iter(data_path, file_name, tokenizer, batch_size=64, shuffle=True):\n",
    "    \"\"\"生成批次数据\"\"\"\n",
    "    keras_bert_iter = get_keras_bert_iterator(data_path, file_name, tokenizer)\n",
    "    while True:\n",
    "        data_list = []\n",
    "        for _ in range(batch_size):\n",
    "            data = next(keras_bert_iter)\n",
    "            data_list.append(data)\n",
    "        if shuffle:\n",
    "            random.shuffle(data_list)\n",
    "        \n",
    "        indices_list = []\n",
    "        label_list = []\n",
    "        for data in data_list:\n",
    "            indices, label = data\n",
    "            indices_list.append(indices)\n",
    "            label_list.append(label)\n",
    "\n",
    "        yield np.array(indices_list), np.array(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fossil-prediction",
   "metadata": {
    "id": "fossil-prediction"
   },
   "outputs": [],
   "source": [
    "it = batch_iter(data_path, \"train.txt\", tokenizer, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "downtown-pointer",
   "metadata": {
    "id": "downtown-pointer"
   },
   "outputs": [],
   "source": [
    "# next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "documented-amount",
   "metadata": {
    "id": "documented-amount"
   },
   "outputs": [],
   "source": [
    "it = batch_iter(data_path, \"train.txt\", tokenizer, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "suitable-tampa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "suitable-tampa",
    "outputId": "81f0a7a6-f702-4d4e-edc0-5a7f01d57c6f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 101, 3297, 6818, ..., 8024, 4125,  102],\n",
       "        [ 101, 2218, 5050, ..., 1059, 1377,  102]]),\n",
       " array([1, 0]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "advisory-hopkins",
   "metadata": {
    "id": "advisory-hopkins"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'bert.embeddings.position_ids', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# bert_model = TFBertModel.from_pretrained(bert_path, from_pt=True,\n",
    "#                                          trainable=True,\n",
    "# #                                          return_dict=True\n",
    "#                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "colonial-entertainment",
   "metadata": {
    "id": "colonial-entertainment"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor: shape=(1, 512, 768), dtype=float32, numpy=\n",
       "array([[[ 0.81342965,  0.11446221, -1.3161695 , ..., -0.3807172 ,\n",
       "         -0.11636969,  0.09716403],\n",
       "        [-0.32854915,  0.4526521 ,  0.44791842, ..., -1.0548452 ,\n",
       "         -0.4015216 ,  0.07144768],\n",
       "        [-0.40069616,  0.00229135, -1.2190996 , ...,  0.8750916 ,\n",
       "          1.0793246 , -0.10630547],\n",
       "        ...,\n",
       "        [ 0.12962021, -0.07015229, -1.0330786 , ...,  0.92563105,\n",
       "         -0.33958027,  0.16176102],\n",
       "        [ 0.3947732 , -0.303226  , -1.3745954 , ..., -0.30742544,\n",
       "          0.4627821 , -0.04974386],\n",
       "        [ 0.0482906 , -0.23322865, -0.48208088, ...,  0.22937274,\n",
       "          0.22010675, -0.18900992]]], dtype=float32)>, pooler_output=<tf.Tensor: shape=(1, 768), dtype=float32, numpy=\n",
       "array([[ 0.9980097 , -0.68254197,  0.99999696, -0.90211046, -0.8536456 ,\n",
       "        -0.80917877,  0.9048096 ,  0.9579247 , -0.5031664 , -0.9998114 ,\n",
       "         0.9999901 ,  0.99999976,  0.11955918,  0.99457145,  0.9999393 ,\n",
       "        -0.9980964 ,  0.9879242 , -0.71273696,  0.9190757 , -0.19530167,\n",
       "        -0.9980974 , -0.99969596,  0.9161084 , -0.9999766 , -0.74816823,\n",
       "         0.9774881 ,  0.81192124,  0.6708386 , -0.35599545,  0.9998627 ,\n",
       "         0.77492887,  0.9999894 , -0.52803725,  0.7825919 , -0.9978311 ,\n",
       "        -0.30176607,  0.09031328,  0.9855125 , -0.12707986,  0.84656477,\n",
       "        -0.44063774,  0.9906921 , -0.96102405, -0.9987185 ,  0.9154695 ,\n",
       "         0.9945757 , -0.9999986 , -0.1720499 , -0.8456705 , -0.9792802 ,\n",
       "        -0.54399306, -0.98065287,  0.99999195, -0.9999973 ,  0.18335027,\n",
       "         0.99641836, -0.9382407 ,  0.9980983 ,  0.99999946, -0.9296556 ,\n",
       "         0.9999812 ,  0.99768263, -0.853929  , -0.99929196,  0.99992067,\n",
       "        -0.9978163 , -0.9984082 , -0.9954973 , -0.96357983,  0.99990404,\n",
       "        -0.9999931 , -0.9449376 ,  0.9999816 , -0.950514  ,  0.7710105 ,\n",
       "         0.99999976, -0.92575306,  0.9128304 , -0.9999049 ,  0.94782186,\n",
       "         0.99998426,  0.87933064, -0.9767128 ,  0.01134058, -0.9488802 ,\n",
       "        -0.9999483 ,  0.9672212 , -0.06752896,  0.20372197, -0.6284662 ,\n",
       "        -0.91826576, -0.9996744 , -0.9999913 ,  0.99958706, -0.9973918 ,\n",
       "        -0.7112479 ,  0.77301854,  0.98026717,  0.96075374, -0.999927  ,\n",
       "        -0.97637737, -0.1260477 , -0.99999976, -0.99999976,  0.4058059 ,\n",
       "         0.99845093,  0.98015314, -0.99985737,  0.9659587 , -0.91368175,\n",
       "        -0.99999756, -0.9087327 , -0.99999976,  0.83486277, -0.9970722 ,\n",
       "         0.9999713 ,  0.13131127,  0.9876182 ,  0.99997205, -0.99999976,\n",
       "         0.9992476 , -0.879631  ,  0.9617929 ,  0.9451579 ,  0.8680771 ,\n",
       "         0.99985456,  0.9996915 ,  0.9537383 ,  0.99999976,  0.99997884,\n",
       "         0.2680082 ,  0.9794338 , -0.9996542 ,  0.9959448 , -0.9482228 ,\n",
       "         0.95418733, -0.85837644, -0.34880665,  0.9999971 ,  0.8305549 ,\n",
       "         0.9852367 ,  0.9670326 , -0.81854576, -0.95841694,  0.9998512 ,\n",
       "        -0.9998052 ,  0.99987876, -0.9999922 , -0.08456866, -0.98864204,\n",
       "        -0.8765709 ,  0.99999344,  0.96925753,  0.03559167,  0.7903975 ,\n",
       "        -0.10437804, -0.6879759 , -0.9999099 , -0.17749907, -0.99994695,\n",
       "        -0.3022678 ,  0.6363165 ,  0.9352917 , -0.69431186, -0.9999995 ,\n",
       "         0.9999854 , -0.7004713 ,  0.99990094,  0.9946637 ,  0.5970748 ,\n",
       "        -0.9995353 , -0.41726077, -0.38181365, -0.99806046,  0.9309886 ,\n",
       "         0.9986608 , -0.6556821 ,  0.99982655,  0.9411333 , -0.9995872 ,\n",
       "        -0.03052487,  0.9378012 ,  0.833753  ,  0.9763431 , -0.6546218 ,\n",
       "        -0.9149109 , -0.9910685 ,  0.9977539 ,  0.62685764, -0.8977102 ,\n",
       "        -0.9949683 ,  0.99999785,  0.877032  ,  0.99967813,  0.9830225 ,\n",
       "        -0.1077081 ,  0.99894744,  0.9505956 , -0.99939436, -0.39766556,\n",
       "        -0.13271964, -0.83965456, -0.9998749 , -0.90071386, -0.9999354 ,\n",
       "         0.84607035,  0.99610716,  0.9706535 ,  0.9103836 , -0.998456  ,\n",
       "         0.9970533 , -0.93041635, -0.9998168 , -0.9233251 ,  0.76642585,\n",
       "         0.19922423, -0.07121214,  0.46147928,  0.41432175, -0.99958146,\n",
       "         0.95782405, -0.9867407 ,  0.70327073, -0.99789625,  0.8758675 ,\n",
       "         0.9999988 , -0.97911286,  0.9999593 ,  0.81070775, -0.99432516,\n",
       "        -0.99999493,  0.99999976, -0.9120102 , -0.9999965 , -0.73261505,\n",
       "         0.9745918 ,  0.93530613,  0.9999848 , -0.99333805, -0.93158406,\n",
       "         0.99999064, -0.9376791 ,  0.9999973 ,  0.9516475 , -0.99873984,\n",
       "        -0.9995504 ,  0.9999934 ,  0.9134183 ,  0.9996769 , -0.29388747,\n",
       "         0.9876789 , -0.53248596,  0.8758837 , -0.99998915,  0.9606103 ,\n",
       "         0.36040038,  0.99924374,  0.9998121 , -0.91441536, -0.9960843 ,\n",
       "         0.95246196, -0.99942064,  0.9999904 ,  0.99766403,  0.9998566 ,\n",
       "         0.99996805,  0.9523956 ,  0.663202  ,  0.49645087,  0.9706543 ,\n",
       "        -0.9986761 ,  0.74981356,  0.31512415, -0.4102557 , -0.99997574,\n",
       "        -0.89579725, -0.99804914, -0.99998224, -0.05198462,  0.9999638 ,\n",
       "         0.9904795 ,  0.2841737 ,  0.9992482 , -0.91120726,  0.30125305,\n",
       "         0.6159668 ,  0.9755946 , -0.9999945 ,  0.99998885, -0.9993607 ,\n",
       "         0.937688  , -0.9994514 , -0.99984354, -0.02087023, -0.7949979 ,\n",
       "         0.95929706, -0.99999976,  0.35410184, -0.99993044,  0.70523936,\n",
       "        -0.8591039 , -0.97710764,  0.9929127 , -0.9428261 , -0.9997662 ,\n",
       "         0.8187483 , -0.0700353 ,  0.7732681 , -0.9937663 , -0.98761994,\n",
       "         0.9999814 , -0.5533359 ,  0.99999475, -0.85447305,  0.9999883 ,\n",
       "        -0.9866983 , -0.45386115,  0.81089044, -0.05263971, -0.49073347,\n",
       "        -0.70430446,  0.7746357 ,  0.9085466 ,  0.75611264,  0.85911834,\n",
       "        -0.99911535, -0.45335054, -0.98941535, -0.02947457,  0.45267165,\n",
       "         0.99005586, -0.8152175 , -0.60066193, -0.956377  ,  0.99958783,\n",
       "        -0.99979556,  0.61284894,  0.9999864 ,  0.99987644,  0.8505332 ,\n",
       "         0.9102302 , -0.9648666 , -0.9632251 , -0.9999765 ,  0.9978981 ,\n",
       "        -0.9994769 ,  0.9996584 ,  0.72057575, -0.9695313 , -0.77848727,\n",
       "        -0.95976955,  0.9999916 ,  0.36380422,  0.7994456 ,  0.7997046 ,\n",
       "         0.84139526, -0.80671203, -0.08671191, -0.923402  ,  0.9719865 ,\n",
       "         0.96931595,  0.9983845 , -0.99999976, -0.99999845,  0.9881267 ,\n",
       "        -0.99981266, -0.99855   , -0.99998564, -0.75301427, -0.941521  ,\n",
       "        -0.99968994,  0.21725856, -0.8673871 , -0.13832226,  0.9452987 ,\n",
       "        -0.6192001 ,  0.90105593,  0.97554237,  0.92649263,  0.816464  ,\n",
       "        -0.7739514 , -0.999953  , -0.98941326, -0.99997467, -0.99225914,\n",
       "        -0.9358948 , -0.27171698, -0.18659276,  0.99999976, -0.99999064,\n",
       "         0.10301736,  0.11119299,  0.5075175 ,  0.60725033,  0.99864066,\n",
       "         0.5438558 ,  0.9999944 ,  0.82303095,  0.9999993 ,  0.9994578 ,\n",
       "        -0.2684386 , -0.99983907, -0.9974755 ,  0.8261049 ,  0.6372056 ,\n",
       "        -0.9999652 , -0.9051795 ,  0.85703284,  0.41568053, -0.9999895 ,\n",
       "         0.9832443 ,  0.9905609 ,  0.99995846,  0.87389797, -0.9994056 ,\n",
       "         0.5357605 , -0.9999561 ,  0.719251  ,  0.99999976,  0.9511745 ,\n",
       "        -0.99999535,  0.5137048 , -0.50510657,  0.9999526 ,  0.68582577,\n",
       "         0.20668118, -0.6768071 , -0.12165793,  0.96686155, -0.9954263 ,\n",
       "        -0.9608418 ,  0.9999722 , -0.07203539, -0.9999916 ,  0.6815159 ,\n",
       "        -0.5747775 , -0.8109397 , -0.6791851 ,  0.9049643 ,  0.18723592,\n",
       "         0.08918504, -0.6641896 , -0.99999106,  0.8329866 ,  0.96720546,\n",
       "         0.887015  ,  0.9958069 , -0.96046495, -0.95091975, -0.9721434 ,\n",
       "        -0.9575551 , -0.619661  ,  0.54361445, -0.9988531 ,  0.999821  ,\n",
       "        -0.9941973 ,  0.90433824, -0.99999976,  0.9999756 , -0.94989455,\n",
       "        -0.631509  , -0.9316463 ,  0.9328311 , -0.0659802 ,  0.5855702 ,\n",
       "         0.9999298 ,  0.99999094,  0.93146133, -0.28284916, -0.9998429 ,\n",
       "        -0.9804936 , -0.969084  , -0.8497713 , -0.97542924, -0.99601865,\n",
       "        -0.99964195, -0.79149956, -0.4844497 ,  0.999998  , -0.28477156,\n",
       "        -0.7341144 ,  0.5661077 , -0.9996098 ,  0.999627  ,  0.40555832,\n",
       "         0.38455275,  0.69233596, -0.9999928 , -0.99979657, -0.06012695,\n",
       "        -0.9928007 ,  0.989059  ,  0.74645096, -0.9922424 ,  0.9891227 ,\n",
       "         0.9010455 , -0.9998284 ,  0.8062091 ,  0.949299  ,  0.94033945,\n",
       "         0.9999938 ,  0.9904757 ,  0.9993931 , -0.9705225 , -0.6515409 ,\n",
       "        -0.20172995,  0.948287  ,  0.9956235 ,  0.8634458 ,  0.3568637 ,\n",
       "        -0.9863244 ,  0.20322402, -0.94150084,  0.9999049 ,  0.99548554,\n",
       "         0.8436317 ,  0.99995446, -0.5077331 ,  0.99734646,  0.6981831 ,\n",
       "        -0.61535555,  0.6526864 ,  0.99991953, -0.94934386, -0.98510367,\n",
       "        -0.9351334 ,  0.99166477,  0.99999475,  0.22856419,  0.8592533 ,\n",
       "        -0.12246269, -0.830241  , -0.07936198,  0.33977515,  0.9999893 ,\n",
       "         0.99774194,  0.83083075, -0.9873434 , -0.47326908, -0.9671879 ,\n",
       "         0.9999974 , -0.34370705,  0.99991506,  0.8597418 , -0.99181944,\n",
       "         0.1969966 ,  0.40851128,  0.9863536 ,  0.86096954, -0.9628901 ,\n",
       "         0.9999433 , -0.9998305 , -0.99992853, -0.9999967 ,  0.9999846 ,\n",
       "         0.9762161 ,  0.87289745, -0.99999714,  0.9999502 ,  0.84218645,\n",
       "        -0.94860834,  0.999979  , -0.99941885,  0.81901693, -0.29514056,\n",
       "        -0.9964666 ,  0.70237345, -0.61494696,  0.9442799 , -0.52975315,\n",
       "         0.9999616 , -0.99014086, -0.94625944,  0.999984  ,  0.98575705,\n",
       "        -0.5614275 ,  0.8300821 ,  0.15920001, -0.5109406 ,  0.9886547 ,\n",
       "        -0.9992204 , -0.9981428 , -0.51879007,  0.99619156, -0.9970194 ,\n",
       "         0.98591006,  0.99990326,  0.91175354,  0.9999554 , -0.999988  ,\n",
       "         0.99999696, -0.9960875 ,  0.9997852 , -0.89744216, -0.98449576,\n",
       "         0.62975943, -0.9937446 , -0.9311121 , -0.324142  ,  0.99995863,\n",
       "         0.96230555,  0.9180775 ,  0.88218194,  0.98058724, -0.6396013 ,\n",
       "        -0.98878855,  0.99906456,  0.91435695,  0.99277216, -0.28759235,\n",
       "         0.43873435,  0.21637423,  0.84847414,  0.9825189 ,  0.98123175,\n",
       "        -0.9749892 ,  0.99996144,  0.7336701 , -0.9999904 , -0.91586703,\n",
       "        -0.9984013 , -0.70971507,  0.85481364, -0.53029996, -0.09466431,\n",
       "        -0.99995375, -0.80943424, -0.9263598 , -0.99999976,  0.7327328 ,\n",
       "         0.07218729,  0.95174503, -0.8364362 ,  0.9983108 , -0.77607065,\n",
       "        -0.799391  , -0.9841482 , -0.95400566,  0.9258329 ,  0.9232018 ,\n",
       "        -0.84481955, -0.9999698 ,  0.36655527,  0.9521975 , -0.99999976,\n",
       "         0.55415124, -0.9999718 ,  0.9999873 , -0.42240518,  0.07635681,\n",
       "         0.65555173, -0.92512155,  0.9347616 ,  0.6911385 , -0.9998655 ,\n",
       "        -0.7648215 , -0.70385754, -0.20162663, -0.9980076 , -0.9999712 ,\n",
       "         0.55370927, -0.9998608 ,  0.28214887,  0.53903395,  0.8333252 ,\n",
       "        -0.9013818 ,  0.9998225 ,  0.9999857 , -0.9988121 , -0.99926084,\n",
       "        -0.99999976,  0.9893746 ,  0.9996426 ,  0.99970907,  0.99994344,\n",
       "         0.5757614 , -0.90296364,  0.9997662 , -0.6825119 ,  0.9647915 ,\n",
       "        -0.994503  , -0.99994814,  0.65119493,  0.96994054, -0.9697649 ,\n",
       "         0.48250726,  0.10330518,  0.06449162, -0.70209605, -0.9999441 ,\n",
       "        -0.1517342 , -0.48050475,  0.99999976,  0.9999182 , -0.9613841 ,\n",
       "         0.9998464 , -0.9994475 ,  0.73695475, -0.9903815 ,  0.9999747 ,\n",
       "        -0.9999189 ,  0.9996462 ,  0.9999953 ,  0.4480963 ,  0.99999046,\n",
       "        -0.99985564,  0.4313974 , -0.97038096, -0.9937759 ,  0.86012435,\n",
       "        -0.9939772 , -0.99957126, -0.969616  ,  0.99999976,  0.8324655 ,\n",
       "         0.9999919 ,  0.7713653 ,  0.9929767 , -0.8594201 , -0.98657066,\n",
       "        -0.95715517,  0.9303501 , -0.99999976, -0.940156  ,  0.6916515 ,\n",
       "        -0.35249755,  0.9999849 ,  0.9077284 , -0.8839855 , -0.9998842 ,\n",
       "        -0.9484247 , -0.70325106, -0.93874806, -0.54139626,  0.8274742 ,\n",
       "         0.99979496,  0.99999976,  0.55112016, -0.87254184,  0.84365296,\n",
       "         0.8343715 , -0.77636915,  0.86859506,  0.34020415, -0.7589801 ,\n",
       "        -0.8357674 ,  0.9620932 , -0.99999976,  0.71121454,  0.994504  ,\n",
       "         0.4712188 , -0.39335838,  0.7002399 ,  0.9091621 , -0.99996287,\n",
       "         0.99999976,  0.8428887 ,  0.84889126,  0.99999964,  0.9988046 ,\n",
       "        -0.9983871 ,  0.9998321 , -0.7797298 , -0.9996919 , -0.02793722,\n",
       "        -0.9999861 , -0.9992769 , -0.28892267]], dtype=float32)>, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it = batch_iter(data_path, \"train.txt\", tokenizer, batch_size=1)\n",
    "# data = next(it)\n",
    "# bert_model(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absolute-perception",
   "metadata": {
    "id": "absolute-perception"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-heritage",
   "metadata": {
    "id": "forced-heritage"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "premium-intro",
   "metadata": {
    "id": "premium-intro"
   },
   "source": [
    "# 定义base模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "necessary-photograph",
   "metadata": {
    "id": "necessary-photograph"
   },
   "outputs": [],
   "source": [
    "# !transformers-cli convert --model_type bert \\\n",
    "#   --tf_checkpoint chinese_L-12_H-768_A-12/bert_model.ckpt \\\n",
    "#   --config chinese_L-12_H-768_A-12/bert_config.json \\\n",
    "#   --pytorch_dump_output chinese_L-12_H-768_A-12/pytorch_model.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "separated-meaning",
   "metadata": {
    "id": "separated-meaning"
   },
   "outputs": [],
   "source": [
    "# bert_model = TFBertForPreTraining.from_pretrained(\"./chinese_L-12_H-768_A-12/\", from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "supported-questionnaire",
   "metadata": {
    "id": "supported-questionnaire"
   },
   "outputs": [],
   "source": [
    "# # it = get_keras_bert_iterator(r\"data/keras_bert_train.txt\", cat_to_id, tokenizer)\n",
    "# it = batch_iter(r\"data/keras_bert_train.txt\", cat_to_id, tokenizer, batch_size=1)\n",
    "# out = bert_model(next(it)[0])\n",
    "# out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "equipped-recruitment",
   "metadata": {
    "id": "equipped-recruitment"
   },
   "outputs": [],
   "source": [
    "def get_model(label_list):\n",
    "    K.clear_session()\n",
    "    \n",
    "    bert_model = TFBertForPreTraining.from_pretrained(bert_path, from_pt=True)\n",
    " \n",
    "    input_indices = Input(shape=(None,), dtype='int32')\n",
    " \n",
    "    bert_output = bert_model(input_indices)\n",
    "    projection_logits = bert_output[0]\n",
    "    bert_cls = Lambda(lambda x: x[:, 0])(projection_logits) # 取出[CLS]对应的向量用来做分类\n",
    "    \n",
    "    dropout = Dropout(0.5)(bert_cls)\n",
    "    output = Dense(len(label_list), activation='softmax')(dropout)\n",
    " \n",
    "    model = Model(input_indices, output)\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer=Adam(1e-5),    #用足够小的学习率\n",
    "                  metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "solar-lodging",
   "metadata": {
    "id": "solar-lodging"
   },
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)   #早停法，防止过拟合\n",
    "plateau = ReduceLROnPlateau(monitor=\"val_accuracy\", verbose=1, mode='max', factor=0.5, patience=2) #当评价指标不在提升时，减少学习率\n",
    "checkpoint = ModelCheckpoint('trained_model/keras_bert_sohu.hdf5', monitor='val_loss',verbose=2, save_best_only=True, mode='max', save_weights_only=True) #保存最好的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advisory-norway",
   "metadata": {
    "id": "advisory-norway"
   },
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "innocent-cookie",
   "metadata": {
    "id": "innocent-cookie"
   },
   "outputs": [],
   "source": [
    "def get_step(sample_count, batch_size):\n",
    "    step = sample_count // batch_size\n",
    "    if sample_count % batch_size != 0:\n",
    "        step += 1\n",
    "    return step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "laden-prize",
   "metadata": {
    "id": "laden-prize"
   },
   "outputs": [],
   "source": [
    "# batch_size = 2\n",
    "# train_step = get_step(train_sample_count, batch_size)\n",
    "# dev_step = get_step(dev_sample_count, batch_size)\n",
    "\n",
    "# train_dataset_iterator = batch_iter(data_path, \"train.txt\", tokenizer, batch_size)\n",
    "# dev_dataset_iterator = batch_iter(data_path, \"valid.txt\", tokenizer, batch_size)\n",
    "\n",
    "# model = get_model(labels)\n",
    "\n",
    "# #模型训练\n",
    "# model.fit(\n",
    "#     train_dataset_iterator,\n",
    "#     steps_per_epoch=10,\n",
    "# #     steps_per_epoch=train_step,\n",
    "#     epochs=5,\n",
    "#     validation_data=dev_dataset_iterator,\n",
    "#     validation_steps=2,\n",
    "# #     validation_steps=dev_step,\n",
    "#     callbacks=[early_stopping, plateau, checkpoint],\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# model.save_weights(\"trained_model/keras_bert_sohu_final.weights\")\n",
    "# model.save(\"trained_model/keras_bert_sohu_final.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brave-purse",
   "metadata": {
    "id": "brave-purse"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-substance",
   "metadata": {
    "id": "polish-substance"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-strike",
   "metadata": {
    "id": "institutional-strike"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "skilled-possibility",
   "metadata": {
    "id": "skilled-possibility"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "voluntary-pittsburgh",
   "metadata": {
    "id": "voluntary-pittsburgh"
   },
   "source": [
    "# 多任务分支模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protective-script",
   "metadata": {
    "id": "protective-script"
   },
   "source": [
    "## 构建数据迭代器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "minimal-sierra",
   "metadata": {
    "id": "minimal-sierra"
   },
   "outputs": [],
   "source": [
    "label_type_to_id = {'labelA':0, 'labelB':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "hungry-principal",
   "metadata": {
    "id": "hungry-principal"
   },
   "outputs": [],
   "source": [
    "def get_text_iterator(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            yield line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "pregnant-force",
   "metadata": {
    "id": "pregnant-force"
   },
   "outputs": [],
   "source": [
    "def get_data_iterator(data_path, file_names):\n",
    "    # TODO: 随机取\n",
    "    file_iters = []\n",
    "    for file_name in file_names:\n",
    "      for category in os.listdir(data_path):\n",
    "          category_path = os.path.join(data_path, category)\n",
    "          if not os.path.isdir(category_path):\n",
    "              continue\n",
    "              \n",
    "          file_path = os.path.join(category_path, file_name)\n",
    "          if not os.path.isfile(file_path):\n",
    "              continue\n",
    "              \n",
    "          \n",
    "          file_iter = get_text_iterator(file_path)\n",
    "          cat_source = 0\n",
    "          if category[0] == '长':\n",
    "            cat_source = 1\n",
    "          cat_target = 0\n",
    "          if category[1] == '长':\n",
    "            cat_target = 1\n",
    "          file_iters.append((file_iter, cat_source, cat_target))\n",
    "        \n",
    "    while len(file_iters) > 0:\n",
    "        i = random.randrange(len(file_iters))\n",
    "        line = next(file_iters[i][0], None)\n",
    "        cat_source = file_iters[i][1]\n",
    "        cat_target = file_iters[i][2]\n",
    "        if line is None:\n",
    "            del file_iters[i]\n",
    "            continue\n",
    "            \n",
    "        data = json.loads(line)\n",
    "\n",
    "        data['source'] = _transform_text(data['source'])\n",
    "        if len(data['source']) == 0:\n",
    "            print('source:', line, data)\n",
    "            break\n",
    "#                     continue\n",
    "\n",
    "        data['target'] = _transform_text(data['target'])\n",
    "        if len(data['target']) == 0:\n",
    "            print('target:', line, data)\n",
    "            break\n",
    "#                     continue\n",
    "\n",
    "        label_name_list = list(key for key in data.keys() if key[:5]=='label')\n",
    "        if len(label_name_list) != 1:\n",
    "            print('label_name_list:', line, data)\n",
    "            break\n",
    "#                     continue\n",
    "        label_name = label_name_list[0]\n",
    "        if data[label_name] not in label_to_id.keys():\n",
    "            print('label_name:', line, data, label_name)\n",
    "            break\n",
    "#                     continue\n",
    "        \n",
    "        label_dict = {key: -1 for key in label_type_to_id.keys()}\n",
    "        label_dict[label_name] = label_to_id[data[label_name]]\n",
    "        if label_dict['labelA'] == 0:\n",
    "            label_dict['labelB'] = 0\n",
    "        if label_dict['labelB'] == 1:\n",
    "            label_dict['labelA'] = 1\n",
    "\n",
    "        label_dict['labelC'] = 0\n",
    "        if label_dict['labelA'] == 1 or label_dict['labelB'] == 1:\n",
    "          label_dict['labelC'] = 1\n",
    "\n",
    "        yield data['source'], data['target'], cat_source, cat_target, label_dict['labelA'], label_dict['labelB'], label_dict['labelC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "advised-breathing",
   "metadata": {
    "id": "advised-breathing"
   },
   "outputs": [],
   "source": [
    "it = get_data_iterator(data_path, [\"train.txt\", \"valid.txt\", \"round2.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fossil-recording",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fossil-recording",
    "outputId": "0a9d7206-30af-4b7b-b33a-f62f9674c52c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('48岁吴绮莉现身避谈小龙女，主动提生父，对方墓地买好不愿回港 48岁吴绮莉现身避谈小龙女，主动提生父，对方墓地买好不愿回港 重新回到香港生活的吴绮莉，虽然时不时就会因为小龙女吴卓林的事情上头条，但是这几年慢慢回归事业的她，状态倒是越来越好了。曾经她也说过女儿已经成年了，自己也应该有自己的生活了。  48岁的吴绮莉现身某电视台，为自己即将上映的新片做宣传，当天吴绮莉的造型也是非常的保暖，一身长款的黑衣外套，看起来气色不错。虽然刚带着小龙女回港的那几年，吴绮莉也是非常沧桑，但是经过这几年的保养，状态已经今时不同往日了，毕竟她也曾是港姐冠军，底子还是很好的。  吴绮莉这次宣传的新剧，正是她去年的时候参与拍摄的《太平纹身店》。这部剧杀青也是大半年了，主要还是因为疫情的影响，所以才会一直拖到现在，才开始宣传和上映，这也是吴绮莉复出后的首部影视作品，因此她也是非常看重！  事实上，港姐出身的吴绮莉曾经也像大多数港姐一样，签约了TVB成为演员，后来生下小龙女之后，她除了《雷霆扫军》就没有其他作品了，在离开TVB之后更是选择在电台工作。  去年5月份的时候，吴绮莉选择再次回归演员的身份，但合作的却不是TVB，而且老东家的对手台某TV，拍摄的正是这部《太平纹身店》，虽然吴绮莉已经有七年没有拍过戏了，但她也曾经是一名专业的演员，可以期待一下。  不过相比吴绮莉的新作品，还是她和小龙女的事情更受媒体的关注，我们都知道在小龙女的成长过程中，和吴绮莉的母女关系也是发生很多的变化，特别有了交往和结婚对象之后，母女两的关系更是越来越差。  这次媒体也问起了吴绮莉和小龙女目前的情况，但是吴绮莉表示不想谈和女儿的私事，甚至为了避免谈小龙女的事情，主动提起了自己生父的事情。  前段时间，吴绮莉突然在自己的社交账号上自曝，称已经失联37年的父亲突然给她打了电话，让她又惊又喜。据吴绮莉的描述，从她记事起，这辈子只见过父亲两次，一次是妈妈离婚带走自己的时候，还有一次则是11岁的时候，在街头匆匆的偶遇了一回。  看得出这次久违的联系，父女两的对话也是让人动容，吴绮莉更是没有因为多年不联系而觉得陌生或者排斥，反而有点珍惜这份来之不易的“父爱”。吴绮莉在活动中说道，父亲定居美国多年，但如今身边的亲朋好友们都已经搬走了，只剩下他一个人独居，所以吴绮莉有打算将八十来岁的老父亲接回香港照顾。  只是吴绮莉的父亲似乎不愿意回到香港，而且还表示自己的墓地都已经买好了，还让吴绮莉要小心身体。而吴绮莉也考虑到现在这个特殊时期，把父亲接回来还是有安全隐患的，而且自己也恐惧坐飞机，又不能坐船，最重要的还是父亲并不打算回来，因此她就放弃了这个想法了。  根据港媒的消息，吴绮莉的父亲叫做吴一鹏，而母亲则叫做郑黎明，在吴绮莉1岁的时候，父母的婚姻就破裂了。不过由于吴绮莉的母亲郑黎明是个厉害的女强人，因此即使没有父亲，吴绮莉小时候依旧过得很幸福，不愁吃穿。  但由于自己的婚姻不幸，所以郑黎明自己对于婚姻完全不信任，甚至还把这种观念灌输给女儿吴绮莉。后来吴绮莉不顾母亲反对参加港姐选美进入娱乐圈，不幸地遇到坏男人，天真的以为怀孕了可以逼供上位，可惜却一败涂地。最终吴绮莉也只能带着自己的女儿去投靠母亲郑黎明。  但很明显投靠母亲的日子并不好过，曾经郑黎明就曾因为吴绮莉长得像爸爸，而对她恶言相向，表示如果她是男孩的话，自己肯定不会要她的。  而面对带着私生女回来的女儿，郑黎明平时对她也是态度很恶劣，恶毒的辱骂她“你连做X都没人要”。每次郑黎明给吴绮莉钱的时候更是极尽羞辱，将一沓的钞票撒满一地，然后让吴绮莉跪着去一张一张地捡。  不过后来郑黎明因病去世后，临终前还是把自己所有的遗产都留给了吴绮莉。而吴绮莉也没有怨恨母亲，尽心尽力地为她奔走后事。可惜的是，吴绮莉一生就像是步上母亲的后尘，没有遇到让自己幸福的男人，和女儿的关系也处得很糟糕。  【图片来源网络】 【文章编辑】丹波小神探 收藏 举报',\n",
       " '霍震霆与朱玲玲离婚后多年未娶妻，也没传绯闻，曝出实情令人遗憾。霍震霆子承父业，在香港也是非常著名的大富豪，是个地地道道的富二代，他的父亲霍英东与赌王一同创业，而且还是非常要好的同学与朋友关系，尤其是一同投资奥博方面，他俩都挣到了许多的钱财。尤其是霍英东，投资于体育事业更是财源广进！霍震霆是霍英东的长子，继承了父亲的财产之后又发扬光大，所以这些年在体育事业方面确实很有建树。 前些年他与广东电视台一枝花侯玉婷传出绯闻之后便与朱玲玲产生了矛盾，两人离婚。候玉婷是广东电视台的著名主持人，曾经主持过多次全运会，还参加奥运会的主持工作，所以与霍震霆有过多次的接触，于是就传出绯闻信息。当霍震霆的妻子朱玲玲与霍震霆关于这方面事情发生纠纷的时候，霍震霆由于对此事缺少必要的解释，所以两人产生矛盾，最后分居，再之后就分手了。   分手8年之后，朱玲玲嫁给了罗康瑞，但是与霍家的关系依然保持着密切的联系。尤其是与霍震霆的母亲吕燕妮关系非常好，相互依靠互相关照，在婆婆逝世的时候，朱玲玲还亲自到场洒泪送别。在霍家娶儿媳妇的时候，朱玲玲以老婆婆的身份自居，与儿媳妇郭晶晶关系相处的更是非常密切。由于朱玲玲虽然离婚，但是与霍家依然保留着相当不错的往来关系，所以致使霍震霆也觉得之前离婚的因素与他有关系。 甚至还缺少必要的澄清与解释，所以也有很多的内疚之感，在他的心里一直也放不下朱玲玲，所以这些年他不能再娶第2个人，心里也装不下第2个人，所以就一直没有绯闻，也没有再娶其他人，致使这些年霍震霆依然是洁身一人。说到这件事的来龙去脉也是让人感觉非常遗憾。  【免责声明】如涉及作品内容、版权和其它问题，请在30日内与本站联系，我们将在第一时间删除内容！',\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "pediatric-bookmark",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pediatric-bookmark",
    "outputId": "d0df3034-3e43-4cc6-98ac-ecb131b7dee2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "119168it [00:06, 19715.02it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "119168"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_count = get_sample_num(data_path, [\"train.txt\", \"valid.txt\", \"round2.txt\"])\n",
    "sample_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2zHQy8QwageB",
   "metadata": {
    "id": "2zHQy8QwageB"
   },
   "outputs": [],
   "source": [
    "def get_sample_y(data_path, file_names):\n",
    "    labelA_list = []\n",
    "    labelB_list = []\n",
    "    labelC_list = []\n",
    "    it = get_data_iterator(data_path, file_names)\n",
    "    for source, target, cat_source, cat_target, labelA, labelB, labelC in tqdm(it):\n",
    "        if labelA != -1:\n",
    "          labelA_list.append(labelA)\n",
    "        if labelB != -1:\n",
    "          labelB_list.append(labelB)\n",
    "        labelC_list.append(labelC)\n",
    "    return labelA_list, labelB_list, labelC_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "whljl4m-cn3l",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "whljl4m-cn3l",
    "outputId": "e2ada33c-7ae4-437e-b984-b555d060d7ab"
   },
   "outputs": [],
   "source": [
    "# labelA_list, labelB_list, labelC_list = get_sample_y(data_path, [\"train.txt\", \"valid.txt\"])\n",
    "# labelA_class_weights = class_weight.compute_class_weight('balanced', np.unique(labelA_list), labelA_list)\n",
    "# labelB_class_weights = class_weight.compute_class_weight('balanced', np.unique(labelB_list), labelB_list)\n",
    "# labelC_class_weights = class_weight.compute_class_weight('balanced', np.unique(labelC_list), labelC_list)\n",
    "# labelA_class_weights, labelB_class_weights, labelC_class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "streaming-amsterdam",
   "metadata": {
    "id": "streaming-amsterdam"
   },
   "outputs": [],
   "source": [
    "def _get_indices(text, text_pair=None):\n",
    "    return tokenizer.encode_plus(text=text,\n",
    "                            text_pair=text_pair,\n",
    "                            max_length=text_max_length, \n",
    "                            add_special_tokens=True, \n",
    "                            padding='max_length', \n",
    "#                             truncation_strategy='longest_first', \n",
    "                            truncation=True,\n",
    "#                                          return_tensors='tf',\n",
    "                            return_token_type_ids=True\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "paperback-dynamics",
   "metadata": {
    "id": "paperback-dynamics"
   },
   "outputs": [],
   "source": [
    "def get_keras_bert_iterator(data_path, file_names, tokenizer):\n",
    "    while True:\n",
    "        data_it = get_data_iterator(data_path, file_names)\n",
    "        for source, target, cat_source, cat_target, labelA, labelB, labelC in data_it:\n",
    "            data_source = _get_indices(text=source)\n",
    "            data_target = _get_indices(text=target)\n",
    "#             print(indices, type(indices), len(indices))\n",
    "            yield data_source['input_ids'], data_source['token_type_ids'], data_source['attention_mask'], data_target['input_ids'], data_target['token_type_ids'], data_target['attention_mask'], cat_source, cat_target, labelA, labelB, labelC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "chinese-persian",
   "metadata": {
    "id": "chinese-persian"
   },
   "outputs": [],
   "source": [
    "it = get_keras_bert_iterator(data_path, [\"train.txt\", \"valid.txt\", \"round2.txt\"], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "complete-explanation",
   "metadata": {
    "id": "complete-explanation"
   },
   "outputs": [],
   "source": [
    "# next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "suburban-priority",
   "metadata": {
    "id": "suburban-priority"
   },
   "outputs": [],
   "source": [
    "def batch_iter(data_path, file_names, tokenizer, batch_size=64, shuffle=True):\n",
    "    \"\"\"生成批次数据\"\"\"\n",
    "    keras_bert_iter = get_keras_bert_iterator(data_path, file_names, tokenizer)\n",
    "    while True:\n",
    "        data_list = []\n",
    "        for _ in range(batch_size):\n",
    "            data = next(keras_bert_iter)\n",
    "            data_list.append(data)\n",
    "        if shuffle:\n",
    "            random.shuffle(data_list)\n",
    "        \n",
    "        input_ids_texta_list = []\n",
    "        token_type_ids_texta_list = []\n",
    "        attention_mask_texta_list = []\n",
    "        input_ids_textb_list = []\n",
    "        token_type_ids_textb_list = []\n",
    "        attention_mask_textb_list = []\n",
    "        cat_texta_list = []\n",
    "        cat_textb_list = []\n",
    "        labelA_list = []\n",
    "        labelB_list = []\n",
    "        labelC_list = []\n",
    "        for data in data_list:\n",
    "            input_ids_texta, token_type_ids_texta, attention_mask_texta, input_ids_textb, token_type_ids_textb, attention_mask_textb, cat_texta, cat_textb, labelA, labelB, labelC = data\n",
    "#             print(indices, type(indices))\n",
    "            input_ids_texta_list.append(input_ids_texta)\n",
    "            token_type_ids_texta_list.append(token_type_ids_texta)\n",
    "            attention_mask_texta_list.append(attention_mask_texta)\n",
    "            input_ids_textb_list.append(input_ids_textb)\n",
    "            token_type_ids_textb_list.append(token_type_ids_textb)\n",
    "            attention_mask_textb_list.append(attention_mask_textb)\n",
    "            cat_texta_list.append(cat_texta)\n",
    "            cat_textb_list.append(cat_textb)\n",
    "            labelA_list.append(labelA)\n",
    "            labelB_list.append(labelB)\n",
    "            labelC_list.append(labelC)\n",
    "\n",
    "        yield [np.array(input_ids_texta_list), np.array(token_type_ids_texta_list), np.array(attention_mask_texta_list), \n",
    "               np.array(input_ids_textb_list), np.array(token_type_ids_textb_list), np.array(attention_mask_textb_list), \n",
    "               np.array(cat_texta_list), np.array(cat_textb_list)], \\\n",
    "            [np.array(labelA_list, dtype=np.int32), np.array(labelB_list, dtype=np.int32), np.array(labelC_list, dtype=np.int32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "artificial-truck",
   "metadata": {
    "id": "artificial-truck"
   },
   "outputs": [],
   "source": [
    "it = batch_iter(data_path, [\"train.txt\", \"valid.txt\", \"round2.txt\"], tokenizer, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "broken-guinea",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "broken-guinea",
    "outputId": "d308a85d-fe68-4f14-dc2a-138e1e387f22"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([[ 101, 7716, 1305, ...,    0,    0,    0],\n",
       "         [ 101,  523, 3217, ...,    0,    0,    0]]),\n",
       "  array([[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]]),\n",
       "  array([[1, 1, 1, ..., 0, 0, 0],\n",
       "         [1, 1, 1, ..., 0, 0, 0]]),\n",
       "  array([[ 101,  791, 1921, ..., 3209, 8024,  102],\n",
       "         [ 101,  517, 1173, ...,    0,    0,    0]]),\n",
       "  array([[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]]),\n",
       "  array([[1, 1, 1, ..., 1, 1, 1],\n",
       "         [1, 1, 1, ..., 0, 0, 0]]),\n",
       "  array([0, 0]),\n",
       "  array([1, 0])],\n",
       " [array([0, 1], dtype=int32),\n",
       "  array([ 0, -1], dtype=int32),\n",
       "  array([0, 1], dtype=int32)])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZcPnAGELrtyP",
   "metadata": {
    "id": "ZcPnAGELrtyP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oOc8KEL8rt7q",
   "metadata": {
    "id": "oOc8KEL8rt7q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "o1Yyn--vrt_z",
   "metadata": {
    "id": "o1Yyn--vrt_z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZFA6-3RYruEk",
   "metadata": {
    "id": "ZFA6-3RYruEk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "qhRn-MB4ruH6",
   "metadata": {
    "id": "qhRn-MB4ruH6"
   },
   "outputs": [],
   "source": [
    "def get_test_data_iterator(data_path, file_name):\n",
    "  # print(data_path)\n",
    "  for category in os.listdir(data_path):\n",
    "    category_path = os.path.join(data_path, category)\n",
    "    # print(category_path)\n",
    "    if not os.path.isdir(category_path):\n",
    "      # print(f\"{category_path} not dir\")\n",
    "      continue\n",
    "        \n",
    "    file_path = os.path.join(category_path, file_name)\n",
    "    # print(file_path)\n",
    "    if not os.path.isfile(file_path):\n",
    "      # print(f\"{file_path} not file\")\n",
    "      continue\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "      for line in f:\n",
    "        # print(line)\n",
    "        data = json.loads(line)\n",
    "        \n",
    "        data['source'] = _transform_text(data['source'])\n",
    "        if len(data['source']) == 0:\n",
    "          print('source:', line, data)\n",
    "          break\n",
    "            \n",
    "        data['target'] = _transform_text(data['target'])\n",
    "        if len(data['target']) == 0:\n",
    "          print('target:', line, data)\n",
    "          break\n",
    "\n",
    "        cat_source = 0\n",
    "        if category[0] == '长':\n",
    "          cat_source = 1\n",
    "        cat_target = 0\n",
    "        if category[1] == '长':\n",
    "          cat_target = 1\n",
    "            \n",
    "        yield data['source'], data['target'], cat_source, cat_target, data['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8jsAuwC8ruSJ",
   "metadata": {
    "id": "8jsAuwC8ruSJ"
   },
   "outputs": [],
   "source": [
    "def get_test_keras_bert_iterator(data_path, file_name):\n",
    "  it = get_test_data_iterator(data_path, file_name)\n",
    "  for source, target, cat_source, cat_target, id in it:\n",
    "    data_source = _get_indices(text=source)\n",
    "    data_target = _get_indices(text=target)\n",
    "    yield data_source['input_ids'], data_source['token_type_ids'], data_source['attention_mask'], data_target['input_ids'], data_target['token_type_ids'], data_target['attention_mask'], cat_source, cat_target, id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "A3aaKzFtrubI",
   "metadata": {
    "id": "A3aaKzFtrubI"
   },
   "outputs": [],
   "source": [
    "def get_test_iterator(data_path, file_name):\n",
    "  it = get_test_keras_bert_iterator(data_path, file_name)\n",
    "  for input_ids_texta, token_type_ids_texta, attention_mask_texta, input_ids_textb, token_type_ids_textb, attention_mask_textb, cat_source, cat_target, id in it:\n",
    "    yield [np.array([input_ids_texta]), np.array([token_type_ids_texta]), np.array([attention_mask_texta]), np.array([input_ids_textb]), np.array([token_type_ids_textb]), np.array([attention_mask_textb]), np.array([cat_source]), np.array([cat_target])], id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "NTmqSZRDv065",
   "metadata": {
    "id": "NTmqSZRDv065"
   },
   "outputs": [],
   "source": [
    "it = get_test_iterator(data_path, \"test_with_id.txt\")\n",
    "# next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "iH42Aoz9r8Sz",
   "metadata": {
    "id": "iH42Aoz9r8Sz"
   },
   "outputs": [],
   "source": [
    "def save_test_result(model, result_file, result_file_1):\n",
    "  it = get_test_iterator(data_path, \"test_with_id.txt\")\n",
    "#   print(\"      \", end=\"\")\n",
    "#   count = 0\n",
    "  with open(result_file, 'w', encoding='utf-8') as f, open(result_file_1, 'w', encoding='utf-8') as f_1:\n",
    "    f.write(f\"id,label\\n\")\n",
    "    f_1.write(f\"id,label\\n\")\n",
    "    for data, id in tqdm(it):\n",
    "      predict = model.predict(data)\n",
    "      if id[-1] == 'a':\n",
    "        predict_cls = 1 if predict[0][0][0] > 0.5 else 0\n",
    "      elif id[-1] == 'b':\n",
    "        predict_cls = 1 if predict[1][0][0] > 0.5 else 0\n",
    "      else:\n",
    "        print(id)\n",
    "        continue\n",
    "      f.write(f\"{id},{predict_cls}\\n\")\n",
    "      \n",
    "      predict_cls_1 = 0\n",
    "      if predict[2][0][0] > 0.5:\n",
    "            predict_cls_1 = 1\n",
    "      f_1.write(f\"{id},{predict_cls_1}\\n\")\n",
    "#       count += 1\n",
    "#       print(f\"\\b\\b\\b\\b\\b\\b{count}\", end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6O6gbcVr8ZC",
   "metadata": {
    "id": "e6O6gbcVr8ZC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jSB9mibcr8cP",
   "metadata": {
    "id": "jSB9mibcr8cP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zHTMitEQr8gT",
   "metadata": {
    "id": "zHTMitEQr8gT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oIln0I3Aru0R",
   "metadata": {
    "id": "oIln0I3Aru0R"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "recent-yahoo",
   "metadata": {
    "id": "recent-yahoo"
   },
   "source": [
    "## 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "kOcnb7QxgxZX",
   "metadata": {
    "id": "kOcnb7QxgxZX"
   },
   "outputs": [],
   "source": [
    "def variant_focal_loss(gamma=2., alpha=0.5, rescale = False):\n",
    "\n",
    "    gamma = float(gamma)\n",
    "    alpha = float(alpha)\n",
    "\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        # print(y_true)\n",
    "        \"\"\"\n",
    "        Focal loss for bianry-classification\n",
    "        FL(p_t)=-rescaled_factor*alpha_t*(1-p_t)^{gamma}log(p_t)\n",
    "        \n",
    "        Notice: \n",
    "        y_pred is probability after sigmoid\n",
    "\n",
    "        Arguments:\n",
    "            y_true {tensor} -- groud truth label, shape of [batch_size, 1]\n",
    "            y_pred {tensor} -- predicted label, shape of [batch_size, 1]\n",
    "\n",
    "        Keyword Arguments:\n",
    "            gamma {float} -- (default: {2.0})  \n",
    "            alpha {float} -- (default: {0.5})\n",
    "\n",
    "        Returns:\n",
    "            [tensor] -- loss.\n",
    "        \"\"\"\n",
    "        epsilon = 1.e-9  \n",
    "        y_true = tf.convert_to_tensor(y_true, tf.float32)\n",
    "        y_pred = tf.convert_to_tensor(y_pred, tf.float32)\n",
    "        model_out = tf.clip_by_value(y_pred, epsilon, 1.-epsilon)  # to advoid numeric underflow\n",
    "        \n",
    "        # compute cross entropy ce = ce_0 + ce_1 = - (1-y)*log(1-y_hat) - y*log(y_hat)\n",
    "        ce_0 = tf.multiply(tf.subtract(1., y_true), -tf.math.log(tf.subtract(1., model_out)))\n",
    "        ce_1 = tf.multiply(y_true, -tf.math.log(model_out))\n",
    "\n",
    "        # compute focal loss fl = fl_0 + fl_1\n",
    "        # obviously fl < ce because of the down-weighting, we can fix it by rescaling\n",
    "        # fl_0 = -(1-y_true)*(1-alpha)*((y_hat)^gamma)*log(1-y_hat) = (1-alpha)*((y_hat)^gamma)*ce_0\n",
    "        fl_0 = tf.multiply(tf.pow(model_out, gamma), ce_0)\n",
    "        fl_0 = tf.multiply(1.-alpha, fl_0)\n",
    "        # fl_1= -y_true*alpha*((1-y_hat)^gamma)*log(y_hat) = alpha*((1-y_hat)^gamma*ce_1\n",
    "        fl_1 = tf.multiply(tf.pow(tf.subtract(1., model_out), gamma), ce_1)\n",
    "        fl_1 = tf.multiply(alpha, fl_1)\n",
    "        fl = tf.add(fl_0, fl_1)\n",
    "        f1_avg = tf.reduce_mean(fl)\n",
    "        \n",
    "        if rescale:\n",
    "            # rescale f1 to keep the quantity as ce\n",
    "            ce = tf.add(ce_0, ce_1)\n",
    "            ce_avg = tf.reduce_mean(ce)\n",
    "            rescaled_factor = tf.divide(ce_avg, f1_avg + epsilon)\n",
    "            f1_avg = tf.multiply(rescaled_factor, f1_avg)\n",
    "        \n",
    "        return f1_avg\n",
    "    \n",
    "    return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "21074562-23bc-4257-a10d-328a14a01395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_loss(y_true, y_pred):\n",
    "    # y_true:真实标签0或者1；y_pred:为正类的概率\n",
    "    loss = 2 * tf.reduce_sum(y_true * y_pred) / tf.reduce_sum(y_true + y_pred) + K.epsilon()\n",
    "    return -loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "signal-assets",
   "metadata": {
    "id": "signal-assets"
   },
   "outputs": [],
   "source": [
    "def transform_y(y_true, y_pred):\n",
    "    mask_value = tf.constant(-1)\n",
    "    mask_y_true = tf.not_equal(tf.cast(y_true, dtype=tf.int32), tf.cast(mask_value, dtype=tf.int32))\n",
    "#     print(f\"mask_y_true:{mask_y_true}\")\n",
    "#     y_true_ = tf.cond(tf.equal(y_true, mask_value), lambda: 0, lambda: y_true)\n",
    "    y_true_ = tf.cast(y_true, dtype=tf.int32) * tf.cast(mask_y_true, dtype=tf.int32)\n",
    "    y_pred_ = tf.cast(y_pred, dtype=tf.float32) * tf.cast(mask_y_true, dtype=tf.float32)\n",
    "#     print(f\"y_true_:{y_true_}, y_pred_:{y_pred_}\")\n",
    "    \n",
    "    return tf.cast(y_true_, dtype=tf.float32), tf.cast(y_pred_, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "strong-assessment",
   "metadata": {
    "id": "strong-assessment"
   },
   "outputs": [],
   "source": [
    "def my_binary_crossentropy(y_true, y_pred):\n",
    "    # print(f\"y_true:{y_true}, y_pred:{y_pred}\")\n",
    "    \n",
    "    y_true, y_pred = transform_y(y_true, y_pred)\n",
    "    # print(f\"y_true_:{y_true}, y_pred_:{y_pred}\")\n",
    "\n",
    "    # loss = binary_crossentropy(y_true, y_pred)\n",
    "    loss = variant_focal_loss()(y_true, y_pred)\n",
    "#     loss = f1_loss(y_true, y_pred)\n",
    "\n",
    "    # y_true0 = tf.constant([y_true.numpy()[0]])\n",
    "    # y_true1 = tf.constant([y_true.numpy()[1]])\n",
    "    # y_pred0 = tf.constant([y_pred.numpy()[0]])\n",
    "    # y_pred1 = tf.constant([y_pred.numpy()[1]])\n",
    "    # loss0 = variant_focal_loss()(y_true0, y_pred0)\n",
    "    # loss1 = variant_focal_loss()(y_true1, y_pred1)\n",
    "    # print(f\"y_true_0:{y_true0}, y_pred_0:{y_pred0}\")\n",
    "    # print(f\"y_true_1:{y_true1}, y_pred_1:{y_pred1}\")\n",
    "    # print(f\"loss0:{loss0}\")\n",
    "    # print(f\"loss1:{loss1}\")\n",
    "\n",
    "    # print(f\"loss:{loss}\")\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "confused-acoustic",
   "metadata": {
    "id": "confused-acoustic"
   },
   "outputs": [],
   "source": [
    "def tarnsform_metrics(y_true, y_pred):\n",
    "    y_true_, y_pred_ = y_true.numpy(), y_pred.numpy()\n",
    "    for i in range(y_true_.shape[0]):\n",
    "        for j in range(y_true_.shape[1]):\n",
    "            if y_true_[i][j] == -1:\n",
    "                y_true_[i][j] = 0\n",
    "                y_pred_[i][j] = random.choice([0, 1])\n",
    "            if y_pred_[i][j] > 0.5:\n",
    "                y_pred_[i][j] = 1\n",
    "            else:\n",
    "                y_pred_[i][j] = 0\n",
    "    return y_true_, y_pred_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "abroad-monaco",
   "metadata": {
    "id": "abroad-monaco"
   },
   "outputs": [],
   "source": [
    "def my_binary_accuracy(y_true, y_pred):\n",
    "#     print(\"my_binary_accuracy\")\n",
    "#     print(f\"y_true:{y_true}, y_pred:{y_pred}\")\n",
    "    \n",
    "    y_true_, y_pred_ = tarnsform_metrics(y_true, y_pred)\n",
    "#     print(f\"y_true_:{y_true_}, y_pred_:{y_pred_}\")\n",
    "\n",
    "    accuracy = binary_accuracy(y_true_, y_pred_)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "radical-chuck",
   "metadata": {
    "id": "radical-chuck"
   },
   "outputs": [],
   "source": [
    "def my_f1_score(y_true, y_pred):\n",
    "#     print(\"my_f1_score\")\n",
    "#     print(f\"y_true:{y_true}, y_pred:{y_pred}\")\n",
    "    \n",
    "    y_true_, y_pred_ = tarnsform_metrics(y_true, y_pred)\n",
    "#     print(f\"y_true_:{y_true_}, y_pred_:{y_pred_}\")\n",
    "\n",
    "    return f1_score(y_true_, y_pred_, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "79bb58d5-872e-4dcc-b815-35211fe2bd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_model = TFBertForPreTraining.from_pretrained(bert_path, from_pt=True)\n",
    "# data = next(it)[0]\n",
    "# bert_output_texta = bert_model({'input_ids':data[0], 'token_type_ids':data[1], 'attention_mask':data[2]}, return_dict=False, training=True)\n",
    "# bert_output_texta[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "standard-tomorrow",
   "metadata": {
    "id": "standard-tomorrow"
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    K.clear_session()\n",
    "    \n",
    "    bert_model = TFBertModel.from_pretrained(bert_path, from_pt=True, trainable=True)\n",
    "    for l in bert_model.layers:\n",
    "        l.trainable = True\n",
    " \n",
    "    input_ids_texta = Input(shape=(None,), dtype='int32')\n",
    "    input_token_type_ids_texta = Input(shape=(None,), dtype='int32')\n",
    "    input_attention_mask_texta = Input(shape=(None,), dtype='int32')\n",
    "    input_ids_textb = Input(shape=(None,), dtype='int32')\n",
    "    input_token_type_ids_textb = Input(shape=(None,), dtype='int32')\n",
    "    input_attention_mask_textb = Input(shape=(None,), dtype='int32')\n",
    "    input_token_type_ids_textb = Input(shape=(None,), dtype='int32')\n",
    "    input_cat_texta = Input(shape=(1), dtype='float32')\n",
    "    input_cat_textb = Input(shape=(1), dtype='float32')\n",
    " \n",
    "    bert_output_texta = bert_model({'input_ids':input_ids_texta, 'token_type_ids':input_token_type_ids_texta, 'attention_mask':input_attention_mask_texta}, return_dict=False, training=True)\n",
    "    projection_logits_texta = bert_output_texta[0]\n",
    "    bert_cls_texta = Lambda(lambda x: x[:, 0])(projection_logits_texta) # 取出[CLS]对应的向量用来做分类\n",
    "\n",
    "    bert_output_textb = bert_model({'input_ids':input_ids_textb, 'token_type_ids':input_token_type_ids_textb, 'attention_mask':input_attention_mask_textb}, return_dict=False, training=True)\n",
    "    projection_logits_textb = bert_output_textb[0]\n",
    "    bert_cls_textb = Lambda(lambda x: x[:, 0])(projection_logits_textb) # 取出[CLS]对应的向量用来做分类\n",
    "\n",
    "    bert_cls = concatenate([bert_cls_texta, bert_cls_textb, input_cat_texta, input_cat_textb], axis=-1)\n",
    "    \n",
    "    dropout_A = Dropout(0.5)(bert_cls)\n",
    "    output_A = Dense(1, activation='sigmoid')(dropout_A)\n",
    "    \n",
    "    dropout_B = Dropout(0.5)(bert_cls)\n",
    "    output_B = Dense(1, activation='sigmoid')(dropout_B)\n",
    "\n",
    "    dropout_C = Dropout(0.5)(bert_cls)\n",
    "    output_C = Dense(1, activation='sigmoid')(dropout_C)\n",
    " \n",
    "    model = Model([input_ids_texta, input_token_type_ids_texta, input_attention_mask_texta, input_ids_textb, input_token_type_ids_textb, input_attention_mask_textb, input_cat_texta, input_cat_textb], [output_A, output_B, output_C])\n",
    "    model.compile(\n",
    "                  loss=[my_binary_crossentropy, my_binary_crossentropy, 'binary_crossentropy'],\n",
    "#                   loss='binary_crossentropy',\n",
    "#                   loss=binary_crossentropy,\n",
    "                  optimizer=Adam(1e-5),    #用足够小的学习率\n",
    "                  metrics=[my_binary_accuracy, my_f1_score]\n",
    "#                   metrics='accuracy'\n",
    "                 )\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "foreign-tribe",
   "metadata": {
    "id": "foreign-tribe"
   },
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='loss', patience=3)   #早停法，防止过拟合\n",
    "plateau = ReduceLROnPlateau(monitor=\"loss\", verbose=1, mode='max', factor=0.5, patience=2) #当评价指标不在提升时，减少学习率\n",
    "checkpoint = ModelCheckpoint('trained_model/multi_keras_bert_sohu.weights', monitor='loss', verbose=2, save_best_only=True, save_weights_only=True) #保存最好的模型\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./logs\", update_freq=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-journey",
   "metadata": {
    "id": "rotary-journey"
   },
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "deb8cd1e-18c9-4851-9249-ead48e716bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 2\n",
    "\n",
    "# train_dataset_iterator = batch_iter(data_path, [\"train.txt\", \"valid.txt\", \"round2.txt\"], tokenizer, batch_size)\n",
    "# train_step = get_step(sample_count, batch_size)\n",
    "\n",
    "# model = get_model()\n",
    "\n",
    "# model.fit(\n",
    "#   train_dataset_iterator,\n",
    "#   # steps_per_epoch=10,\n",
    "#   steps_per_epoch=train_step,\n",
    "#   epochs=5,\n",
    "# #       validation_data=dev_dataset_iterator,\n",
    "#   # validation_steps=2,\n",
    "# #       validation_steps=dev_step,\n",
    "# #   callbacks=[early_stopping, plateau, checkpoint, tensorboard_callback],\n",
    "#   verbose=1\n",
    "# )\n",
    "\n",
    "# model.save_weights(f\"trained_model/multi_keras_bert_sohu_final.weights\")\n",
    "\n",
    "# save_test_result(model, f\"trained_model/multi_keras_bert_sohu_test_result_final.csv\", f\"trained_model/multi_keras_bert_sohu_test_result_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "nvzxFiaqmqMX",
   "metadata": {
    "id": "nvzxFiaqmqMX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForPreTraining.\n",
      "\n",
      "All the weights of TFBertForPreTraining were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForPreTraining for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_for_pre_training (TFBer TFBertForPreTraining 102882442   input_3[0][0]                    \n",
      "                                                                 input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "                                                                 input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 21128)        0           tf_bert_for_pre_training[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 21128)        0           tf_bert_for_pre_training[1][0]   \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 42258)        0           lambda[0][0]                     \n",
      "                                                                 lambda_1[0][0]                   \n",
      "                                                                 input_8[0][0]                    \n",
      "                                                                 input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 42258)        0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 42258)        0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)            (None, 42258)        0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            42259       dropout_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            42259       dropout_38[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            42259       dropout_39[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 103,009,219\n",
      "Trainable params: 103,009,219\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7faf34560f50>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.load_weights('trained_model/multi_keras_bert_sohu.weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "XpnsY1ZZmr_h",
   "metadata": {
    "id": "XpnsY1ZZmr_h"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/data1/wangchenyue/zsd/code/sohu2021/conda_env/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:3504: UserWarning: Even though the tf.config.experimental_run_functions_eagerly option is set, this option does not apply to tf.data functions. tf.data functions are still traced and executed as graphs.\n",
      "  \"Even though the tf.config.experimental_run_functions_eagerly \"\n",
      "29822it [5:50:52,  1.42it/s]\n"
     ]
    }
   ],
   "source": [
    "save_test_result(model, \n",
    "                 f\"trained_model/multi_keras_bert_sohu_test_result_.csv\",\n",
    "                 f\"trained_model/multi_keras_bert_sohu_test_result_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "x4wZR3ZpNf5w",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x4wZR3ZpNf5w",
    "outputId": "64d8ae7a-791e-4e5c-a0e8-12689d353ec9"
   },
   "outputs": [],
   "source": [
    "# epoch = 1\n",
    "# per_train_step = 5000\n",
    "# per_epoch_step = (sample_count // batch_size + 1) // per_train_step + 1\n",
    "\n",
    "# # start_i = (3+8+1+2) % per_epoch_step\n",
    "# start_i = (0+1) % per_epoch_step\n",
    "# for i in range(start_i):\n",
    "#   for j in range(per_train_step):\n",
    "#     next(train_dataset_iterator)\n",
    "\n",
    "\n",
    "# #模型训练\n",
    "# for epoch_ in range(epoch):\n",
    "#   for i in range(start_i, per_epoch_step):\n",
    "#     model.fit(\n",
    "#         train_dataset_iterator,\n",
    "#         steps_per_epoch=per_train_step,\n",
    "#         epochs=1,\n",
    "#         # class_weight=[labelA_class_weights, labelB_class_weights, labelC_class_weights],\n",
    "#         verbose=1\n",
    "#     )\n",
    "\n",
    "#     model.save_weights(f\"/content/drive/MyDrive/sohu_match/multi_keras_bert_sohu_step.weights\")\n",
    "#     # model.save(f\"/content/drive/MyDrive/sohu_match/multi_keras_bert_sohu_step.model\")\n",
    "\n",
    "#   save_test_result(model, f\"/content/drive/MyDrive/sohu_match/multi_keras_bert_sohu_test_result_epoch.csv\", f\"trained_model/multi_keras_bert_sohu_test_result_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ordinary-nashville",
   "metadata": {
    "id": "ordinary-nashville"
   },
   "outputs": [],
   "source": [
    "# dev_dataset_iterator = batch_iter(data_path, \"valid.txt\", tokenizer, batch_size=1)\n",
    "# data = next(dev_dataset_iterator)\n",
    "# model.predict(data[0]), data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "rx1AsBA3fS7p",
   "metadata": {
    "id": "rx1AsBA3fS7p"
   },
   "outputs": [],
   "source": [
    "# data = next(dev_dataset_iterator)\n",
    "# model.predict(data[0]), data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "introductory-telephone",
   "metadata": {
    "id": "introductory-telephone"
   },
   "outputs": [],
   "source": [
    "# model.save_weights(\"/content/drive/MyDrive/multi_keras_bert_sohu_final.weights\")\n",
    "# model.save(\"/content/drive/MyDrive/multi_keras_bert_sohu_final.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hiM3cmiokFMQ",
   "metadata": {
    "id": "hiM3cmiokFMQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virgin-quarterly",
   "metadata": {
    "id": "virgin-quarterly"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tribal-banking",
   "metadata": {
    "id": "tribal-banking"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "likely-celebration",
   "metadata": {
    "id": "likely-celebration"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "latest-biology",
   "metadata": {
    "id": "latest-biology"
   },
   "source": [
    "# 模型加载及测试"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reserved-nightmare",
   "metadata": {
    "id": "reserved-nightmare"
   },
   "source": [
    "## load_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sufficient-shadow",
   "metadata": {
    "id": "sufficient-shadow"
   },
   "source": [
    "## load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-adventure",
   "metadata": {
    "id": "white-adventure"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structural-shark",
   "metadata": {
    "id": "structural-shark"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "o7jaIjHXf1Pk",
   "metadata": {
    "id": "o7jaIjHXf1Pk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9jQK0PLYf1TC",
   "metadata": {
    "id": "9jQK0PLYf1TC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Me0AC8uru-dN",
   "metadata": {
    "id": "Me0AC8uru-dN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CJbObtBvxoH2",
   "metadata": {
    "id": "CJbObtBvxoH2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "reserved-nightmare"
   ],
   "name": "keras_bert_transformers_two_text_input.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
