{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "precious-column",
   "metadata": {
    "id": "precious-column"
   },
   "source": [
    "# 导包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ivu9Z3WNt4BV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "ivu9Z3WNt4BV",
    "outputId": "a421751c-af12-46f9-a9bd-076e0ee60ef3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.7.10 | packaged by conda-forge | (default, Feb 19 2021, 16:07:37) \\n[GCC 9.3.0]'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys \n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "VNZFCpWIfJ9t",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VNZFCpWIfJ9t",
    "outputId": "404b5dfd-71db-4323-e5a7-e00bf07d5bf9"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "F65Y9KEAiZzD",
   "metadata": {
    "id": "F65Y9KEAiZzD"
   },
   "outputs": [],
   "source": [
    "# !unzip sohu2021_open_data_clean.zip\n",
    "# !unzip chinese_L-12_H-768_A-12.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ZBvllWcDWLZN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZBvllWcDWLZN",
    "outputId": "70daddf4-29ef-48f1-9612-96f639a4c615"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "subject-motorcycle",
   "metadata": {
    "id": "subject-motorcycle"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "import sys\n",
    "import re\n",
    "from collections import Counter\n",
    "import random\n",
    "import json\n",
    "from joblib import dump, load\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "# import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "tf.config.run_functions_eagerly(True)\n",
    "tf.get_logger().setLevel(tf.compat.v1.logging.ERROR)\n",
    "from keras.metrics import top_k_categorical_accuracy, binary_accuracy\n",
    "from keras.layers import *\n",
    "from keras.callbacks import *\n",
    "from keras.models import Model, load_model, model_from_json\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from keras.losses import SparseCategoricalCrossentropy, binary_crossentropy\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    TFBertForPreTraining,\n",
    "    TFBertModel,\n",
    ")\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.utils import class_weight\n",
    "import torch\n",
    "from pyhanlp import *\n",
    "import jieba\n",
    "\n",
    "from my_utils import calculate_bm25_similarity, calculate_tf_cosine_similarity, calculate_tfidf_cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "patent-winner",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "patent-winner",
    "outputId": "79dceb89-d54d-4a37-dc00-ade7e64c3987"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "067b1c15-860a-477f-953a-f371ca788697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 7415377022590500059,\n",
       " name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 10770692224\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 5767317252031276943\n",
       " physical_device_desc: \"device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:09:00.0, compute capability: 6.1\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rolled-process",
   "metadata": {
    "id": "rolled-process"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cubic-statistics",
   "metadata": {
    "id": "cubic-statistics"
   },
   "outputs": [],
   "source": [
    "data_path = \"sohu2021_open_data_clean/\"\n",
    "text_max_length = 512\n",
    "bert_path = r\"chinese_L-12_H-768_A-12\"\n",
    "\n",
    "check_point_path = 'trained_model_substract_1/multi_keras_bert_sohu.weights'\n",
    "weights_path = \"trained_model_substract_1/multi_keras_bert_sohu_final.weights\"\n",
    "config_path = \"trained_model_substract_1/multi_keras_bert_sohu_final.model_config.json\"\n",
    "result_path = \"trained_model_substract_1/multi_keras_bert_sohu_test_result_final.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "collect-portal",
   "metadata": {
    "id": "collect-portal"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.summarization.bm25.BM25 at 0x7f54f418c990>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25Model = load(\"bm25.bin\")\n",
    "bm25Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3932e5-4e86-44a6-ba49-54acf6568517",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norman-track",
   "metadata": {
    "id": "norman-track"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-pillow",
   "metadata": {
    "id": "cordless-pillow"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absolute-perception",
   "metadata": {
    "id": "absolute-perception"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "forced-heritage",
   "metadata": {
    "id": "forced-heritage"
   },
   "outputs": [],
   "source": [
    "# 转换bert模型，到pytorch的pd格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "necessary-photograph",
   "metadata": {
    "id": "necessary-photograph"
   },
   "outputs": [],
   "source": [
    "# !transformers-cli convert --model_type bert \\\n",
    "#   --tf_checkpoint chinese_L-12_H-768_A-12/bert_model.ckpt \\\n",
    "#   --config chinese_L-12_H-768_A-12/bert_config.json \\\n",
    "#   --pytorch_dump_output chinese_L-12_H-768_A-12/pytorch_model.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brave-purse",
   "metadata": {
    "id": "brave-purse"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-substance",
   "metadata": {
    "id": "polish-substance"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-strike",
   "metadata": {
    "id": "institutional-strike"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "skilled-possibility",
   "metadata": {
    "id": "skilled-possibility"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "voluntary-pittsburgh",
   "metadata": {
    "id": "voluntary-pittsburgh"
   },
   "source": [
    "# 多任务分支模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protective-script",
   "metadata": {
    "id": "protective-script"
   },
   "source": [
    "## 构建数据迭代器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "minimal-sierra",
   "metadata": {
    "id": "minimal-sierra"
   },
   "outputs": [],
   "source": [
    "label_type_to_id = {'labelA':0, 'labelB':1}\n",
    "label_to_id = {'0':0, '1':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "hungry-principal",
   "metadata": {
    "id": "hungry-principal"
   },
   "outputs": [],
   "source": [
    "def get_text_iterator(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            yield line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22be15ad-aa15-423f-861a-892add822381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _transform_text(text):\n",
    "   text = text.strip().replace('\\n', '。').replace('\\t', '').replace('\\u3000', '')\n",
    "   return re.sub(r'。+', '。', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "741f573d-a1ea-4455-bb79-e717964b643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary(text, senc_num=20):\n",
    "    a = HanLP.extractSummary(text, 20)\n",
    "    a_ = str(a)\n",
    "    return a_[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "pregnant-force",
   "metadata": {
    "id": "pregnant-force"
   },
   "outputs": [],
   "source": [
    "def get_data_iterator(data_path, file_names):\n",
    "    # TODO: 随机取\n",
    "    file_iters = []\n",
    "    for file_name in file_names:\n",
    "      for category in os.listdir(data_path):\n",
    "          category_path = os.path.join(data_path, category)\n",
    "          if not os.path.isdir(category_path):\n",
    "              continue\n",
    "              \n",
    "          file_path = os.path.join(category_path, file_name)\n",
    "          if not os.path.isfile(file_path):\n",
    "              continue\n",
    "              \n",
    "          \n",
    "          file_iter = get_text_iterator(file_path)\n",
    "          cat_source = 0\n",
    "          if category[0] == '长':\n",
    "            cat_source = 1\n",
    "          cat_target = 0\n",
    "          if category[1] == '长':\n",
    "            cat_target = 1\n",
    "          file_iters.append((file_iter, cat_source, cat_target))\n",
    "        \n",
    "    while len(file_iters) > 0:\n",
    "        i = random.randrange(len(file_iters))\n",
    "        line = next(file_iters[i][0], None)\n",
    "        cat_source = file_iters[i][1]\n",
    "        cat_target = file_iters[i][2]\n",
    "        if line is None:\n",
    "            del file_iters[i]\n",
    "            continue\n",
    "            \n",
    "        data = json.loads(line)\n",
    "\n",
    "        data['source'] = _transform_text(data['source'])\n",
    "        if len(data['source']) == 0:\n",
    "            print('source:', line, data)\n",
    "            break\n",
    "#                     continue\n",
    "\n",
    "        data['target'] = _transform_text(data['target'])\n",
    "        if len(data['target']) == 0:\n",
    "            print('target:', line, data)\n",
    "            break\n",
    "#                     continue\n",
    "\n",
    "        label_name_list = list(key for key in data.keys() if key[:5]=='label')\n",
    "        if len(label_name_list) != 1:\n",
    "            print('label_name_list:', line, data)\n",
    "            break\n",
    "#                     continue\n",
    "        label_name = label_name_list[0]\n",
    "        if data[label_name] not in label_to_id.keys():\n",
    "            print('label_name:', line, data, label_name)\n",
    "            break\n",
    "#                     continue\n",
    "        \n",
    "        label_dict = {key: -1 for key in label_type_to_id.keys()}\n",
    "        label_dict[label_name] = label_to_id[data[label_name]]\n",
    "        if label_dict['labelA'] == 0:\n",
    "            label_dict['labelB'] = 0\n",
    "        if label_dict['labelB'] == 1:\n",
    "            label_dict['labelA'] = 1\n",
    "\n",
    "        yield data['source'], data['target'], cat_source, cat_target, label_dict['labelA'], label_dict['labelB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "advised-breathing",
   "metadata": {
    "id": "advised-breathing"
   },
   "outputs": [],
   "source": [
    "it = get_data_iterator(data_path, [\"train.txt\", \"valid.txt\", \"round2.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fossil-recording",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fossil-recording",
    "outputId": "0a9d7206-30af-4b7b-b33a-f62f9674c52c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('小艺的故事让爱回家2021年2月16日大年初五19：30带上你最亲爱的人与团团君相约《小艺的故事》直播间！',\n",
       " '香港代购了不起啊，宋点卷竟然在直播间“炫富”起来',\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da257b78-2765-4f85-b0b4-de3694adec52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_num(data_path, file_names):\n",
    "    count = 0\n",
    "    it = get_data_iterator(data_path, file_names)\n",
    "    for data in tqdm(it):\n",
    "        count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "pediatric-bookmark",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pediatric-bookmark",
    "outputId": "d0df3034-3e43-4cc6-98ac-ecb131b7dee2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "119168it [00:06, 19617.84it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "119168"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_count = get_sample_num(data_path, [\"train.txt\", \"valid.txt\", \"round2.txt\"])\n",
    "sample_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2zHQy8QwageB",
   "metadata": {
    "id": "2zHQy8QwageB",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_sample_y(data_path, file_names):\n",
    "    labelA_list = []\n",
    "    labelB_list = []\n",
    "    it = get_data_iterator(data_path, file_names)\n",
    "    for source, target, cat_source, cat_target, labelA, labelB in tqdm(it):\n",
    "        if labelA != -1:\n",
    "          labelA_list.append(labelA)\n",
    "        if labelB != -1:\n",
    "          labelB_list.append(labelB)\n",
    "    return labelA_list, labelB_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f619d0d2-a487-4dc8-a01d-ca10b942660d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.unique(labelA_list), labelA_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "whljl4m-cn3l",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "whljl4m-cn3l",
    "outputId": "e2ada33c-7ae4-437e-b984-b555d060d7ab"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "69578it [00:03, 20956.15it/s]\n",
      "/data1/wangchenyue/zsd/code/sohu2021/conda_env/lib/python3.7/site-packages/sklearn/utils/validation.py:72: FutureWarning: Pass classes=[0 1], y=[0 0 1 ... 0 1 0] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n",
      "/data1/wangchenyue/zsd/code/sohu2021/conda_env/lib/python3.7/site-packages/sklearn/utils/validation.py:72: FutureWarning: Pass classes=[0 1], y=[0 0 0 ... 0 0 0] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.9232596 , 1.09065406]), array([0.55924623, 4.71967812]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelA_list, labelB_list = get_sample_y(data_path, [\"train.txt\", \"valid.txt\"])\n",
    "labelA_class_weights = class_weight.compute_class_weight('balanced', np.unique(labelA_list), np.array(labelA_list))\n",
    "labelB_class_weights = class_weight.compute_class_weight('balanced', np.unique(labelB_list), np.array(labelB_list))\n",
    "labelA_class_weights, labelB_class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "47840218-69ab-4b48-ba48-4ef81899b830",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(bert_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "streaming-amsterdam",
   "metadata": {
    "id": "streaming-amsterdam"
   },
   "outputs": [],
   "source": [
    "def _get_indices(text, text_pair=None):\n",
    "    return tokenizer.encode_plus(text=text,\n",
    "                            text_pair=text_pair,\n",
    "                            max_length=text_max_length, \n",
    "                            add_special_tokens=True, \n",
    "                            padding='max_length', \n",
    "#                             truncation_strategy='longest_first', \n",
    "                            truncation=True,\n",
    "#                                          return_tensors='tf',\n",
    "                            return_token_type_ids=True\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "paperback-dynamics",
   "metadata": {
    "id": "paperback-dynamics"
   },
   "outputs": [],
   "source": [
    "def get_keras_bert_iterator(data_path, file_names, tokenizer):\n",
    "    while True:\n",
    "        data_it = get_data_iterator(data_path, file_names)\n",
    "        for source, target, cat_source, cat_target, labelA, labelB in data_it:\n",
    "            data_source = _get_indices(text=source)\n",
    "            data_target = _get_indices(text=target)\n",
    "#             print(indices, type(indices), len(indices))\n",
    "            seg_source = jieba.lcut(source)\n",
    "            seg_target = jieba.lcut(target)\n",
    "            bm25 = calculate_bm25_similarity(bm25Model, seg_source, seg_target)\n",
    "            tf_cosine = calculate_tf_cosine_similarity(seg_source, seg_target)\n",
    "            tfidf_cosine = calculate_tfidf_cosine_similarity(seg_source, seg_target, bm25Model.idf)\n",
    "            yield data_source['input_ids'], data_source['token_type_ids'], data_source['attention_mask'], \\\n",
    "                  data_target['input_ids'], data_target['token_type_ids'], data_target['attention_mask'], \\\n",
    "                  bm25, tf_cosine, tfidf_cosine, \\\n",
    "                  cat_source, cat_target, \\\n",
    "                  labelA, labelB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "chinese-persian",
   "metadata": {
    "id": "chinese-persian"
   },
   "outputs": [],
   "source": [
    "it = get_keras_bert_iterator(data_path, [\"train.txt\", \"valid.txt\", \"round2.txt\"], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "complete-explanation",
   "metadata": {
    "id": "complete-explanation"
   },
   "outputs": [],
   "source": [
    "# next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "suburban-priority",
   "metadata": {
    "id": "suburban-priority"
   },
   "outputs": [],
   "source": [
    "def batch_iter(data_path, file_names, tokenizer, batch_size=64, shuffle=True):\n",
    "    \"\"\"生成批次数据\"\"\"\n",
    "    keras_bert_iter = get_keras_bert_iterator(data_path, file_names, tokenizer)\n",
    "    while True:\n",
    "        data_list = []\n",
    "        for _ in range(batch_size):\n",
    "            data = next(keras_bert_iter)\n",
    "#             print(data)\n",
    "            data_list.append(data)\n",
    "        if shuffle:\n",
    "            random.shuffle(data_list)\n",
    "#         print(data_list)\n",
    "        \n",
    "        input_ids_texta_list = []\n",
    "        token_type_ids_texta_list = []\n",
    "        attention_mask_texta_list = []\n",
    "        input_ids_textb_list = []\n",
    "        token_type_ids_textb_list = []\n",
    "        attention_mask_textb_list = []\n",
    "        bm25_list = []\n",
    "        tf_cosine_list = []\n",
    "        tfidf_cosine_list = []\n",
    "        cat_texta_list = []\n",
    "        cat_textb_list = []\n",
    "        labelA_list = []\n",
    "        labelB_list = []\n",
    "        for data in data_list:\n",
    "            input_ids_texta, token_type_ids_texta, attention_mask_texta, \\\n",
    "            input_ids_textb, token_type_ids_textb, attention_mask_textb, \\\n",
    "            bm25, tf_cosine, tfidf_cosine, \\\n",
    "            cat_texta, cat_textb, \\\n",
    "            labelA, labelB = data\n",
    "#             print(indices, type(indices))\n",
    "            input_ids_texta_list.append(input_ids_texta)\n",
    "            token_type_ids_texta_list.append(token_type_ids_texta)\n",
    "            attention_mask_texta_list.append(attention_mask_texta)\n",
    "            input_ids_textb_list.append(input_ids_textb)\n",
    "            token_type_ids_textb_list.append(token_type_ids_textb)\n",
    "            attention_mask_textb_list.append(attention_mask_textb)\n",
    "            bm25_list.append(bm25)\n",
    "            tf_cosine_list.append(tf_cosine)\n",
    "            tfidf_cosine_list.append(tfidf_cosine)\n",
    "            cat_texta_list.append(cat_texta)\n",
    "            cat_textb_list.append(cat_textb)\n",
    "            labelA_list.append(labelA)\n",
    "            labelB_list.append(labelB)\n",
    "\n",
    "        yield [np.array(input_ids_texta_list), np.array(token_type_ids_texta_list), np.array(attention_mask_texta_list), \n",
    "               np.array(input_ids_textb_list), np.array(token_type_ids_textb_list), np.array(attention_mask_textb_list), \n",
    "               np.array(bm25_list), np.array(tf_cosine_list), np.array(tfidf_cosine_list),\n",
    "               np.array(cat_texta_list), np.array(cat_textb_list)], \\\n",
    "            [np.array(labelA_list, dtype=np.int32), np.array(labelB_list, dtype=np.int32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "artificial-truck",
   "metadata": {
    "id": "artificial-truck"
   },
   "outputs": [],
   "source": [
    "it = batch_iter(data_path, [\"train.txt\", \"valid.txt\", \"round2.txt\"], tokenizer, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "broken-guinea",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "broken-guinea",
    "outputId": "d308a85d-fe68-4f14-dc2a-138e1e387f22"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.713 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([array([[ 101,  686, 4518, ...,    0,    0,    0],\n",
       "         [ 101,  707, 2128, ...,    0,    0,    0]]),\n",
       "  array([[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]]),\n",
       "  array([[1, 1, 1, ..., 0, 0, 0],\n",
       "         [1, 1, 1, ..., 0, 0, 0]]),\n",
       "  array([[ 101, 1059, 3152, ..., 2990, 4433,  102],\n",
       "         [ 101, 7390, 4708, ..., 3198, 7313,  102]]),\n",
       "  array([[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]]),\n",
       "  array([[1, 1, 1, ..., 1, 1, 1],\n",
       "         [1, 1, 1, ..., 1, 1, 1]]),\n",
       "  array([0.07674449, 0.29054894]),\n",
       "  array([0.20902253, 0.03947062]),\n",
       "  array([0.00205734, 0.01755584]),\n",
       "  array([0, 0]),\n",
       "  array([1, 1])],\n",
       " [array([ 0, -1], dtype=int32), array([0, 0], dtype=int32)])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZcPnAGELrtyP",
   "metadata": {
    "id": "ZcPnAGELrtyP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oOc8KEL8rt7q",
   "metadata": {
    "id": "oOc8KEL8rt7q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "o1Yyn--vrt_z",
   "metadata": {
    "id": "o1Yyn--vrt_z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZFA6-3RYruEk",
   "metadata": {
    "id": "ZFA6-3RYruEk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "qhRn-MB4ruH6",
   "metadata": {
    "id": "qhRn-MB4ruH6"
   },
   "outputs": [],
   "source": [
    "def get_test_data_iterator(data_path, file_name):\n",
    "  # print(data_path)\n",
    "  for category in os.listdir(data_path):\n",
    "    category_path = os.path.join(data_path, category)\n",
    "    # print(category_path)\n",
    "    if not os.path.isdir(category_path):\n",
    "      # print(f\"{category_path} not dir\")\n",
    "      continue\n",
    "        \n",
    "    file_path = os.path.join(category_path, file_name)\n",
    "    # print(file_path)\n",
    "    if not os.path.isfile(file_path):\n",
    "      # print(f\"{file_path} not file\")\n",
    "      continue\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "      for line in f:\n",
    "        # print(line)\n",
    "        data = json.loads(line)\n",
    "        \n",
    "        data['source'] = _transform_text(data['source'])\n",
    "        if len(data['source']) == 0:\n",
    "          print('source:', line, data)\n",
    "          break\n",
    "            \n",
    "        data['target'] = _transform_text(data['target'])\n",
    "        if len(data['target']) == 0:\n",
    "          print('target:', line, data)\n",
    "          break\n",
    "\n",
    "        cat_source = 0\n",
    "        if category[0] == '长':\n",
    "          cat_source = 1\n",
    "        cat_target = 0\n",
    "        if category[1] == '长':\n",
    "          cat_target = 1\n",
    "            \n",
    "        yield data['source'], data['target'], cat_source, cat_target, data['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8jsAuwC8ruSJ",
   "metadata": {
    "id": "8jsAuwC8ruSJ"
   },
   "outputs": [],
   "source": [
    "def get_test_keras_bert_iterator(data_path, file_name):\n",
    "  it = get_test_data_iterator(data_path, file_name)\n",
    "  for source, target, cat_source, cat_target, id in it:\n",
    "    data_source = _get_indices(text=source)\n",
    "    data_target = _get_indices(text=target)\n",
    "    \n",
    "    seg_source = jieba.lcut(source)\n",
    "    seg_target = jieba.lcut(target)\n",
    "    bm25 = calculate_bm25_similarity(bm25Model, seg_source, seg_target)\n",
    "    tf_cosine = calculate_tf_cosine_similarity(seg_source, seg_target)\n",
    "    tfidf_cosine = calculate_tfidf_cosine_similarity(seg_source, seg_target, bm25Model.idf)\n",
    "    \n",
    "    yield data_source['input_ids'], data_source['token_type_ids'], data_source['attention_mask'], \\\n",
    "          data_target['input_ids'], data_target['token_type_ids'], data_target['attention_mask'], \\\n",
    "          bm25, tf_cosine, tfidf_cosine, \\\n",
    "          cat_source, cat_target, \\\n",
    "          id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "A3aaKzFtrubI",
   "metadata": {
    "id": "A3aaKzFtrubI"
   },
   "outputs": [],
   "source": [
    "def get_test_iterator(data_path, file_name):\n",
    "  it = get_test_keras_bert_iterator(data_path, file_name)\n",
    "  for input_ids_texta, token_type_ids_texta, attention_mask_texta, \\\n",
    "      input_ids_textb, token_type_ids_textb, attention_mask_textb, \\\n",
    "      bm25, tf_cosine, tfidf_cosine, \\\n",
    "      cat_source, cat_target, \\\n",
    "      id in it:\n",
    "    yield [np.array([input_ids_texta]), np.array([token_type_ids_texta]), np.array([attention_mask_texta]), \n",
    "           np.array([input_ids_textb]), np.array([token_type_ids_textb]), np.array([attention_mask_textb]), \n",
    "           np.array([bm25]), np.array([tf_cosine]), np.array([tfidf_cosine]),\n",
    "           np.array([cat_source]), np.array([cat_target])], id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "NTmqSZRDv065",
   "metadata": {
    "id": "NTmqSZRDv065"
   },
   "outputs": [],
   "source": [
    "it = get_test_iterator(data_path, \"test_with_id.txt\")\n",
    "# next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "iH42Aoz9r8Sz",
   "metadata": {
    "id": "iH42Aoz9r8Sz"
   },
   "outputs": [],
   "source": [
    "def save_test_result(model, result_file):\n",
    "  it = get_test_iterator(data_path, \"test_with_id.txt\")\n",
    "#   print(\"      \", end=\"\")\n",
    "#   count = 0\n",
    "  with open(result_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(f\"id,label\\n\")\n",
    "    for data, id in tqdm(it):\n",
    "      predict = model.predict(data)\n",
    "      if id[-1] == 'a':\n",
    "        predict_cls = 1 if predict[0][0][0] > 0.5 else 0\n",
    "      elif id[-1] == 'b':\n",
    "        predict_cls = 1 if predict[1][0][0] > 0.5 else 0\n",
    "      else:\n",
    "        print(id)\n",
    "        continue\n",
    "      f.write(f\"{id},{predict_cls}\\n\")\n",
    "#       count += 1\n",
    "#       print(f\"\\b\\b\\b\\b\\b\\b{count}\", end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6O6gbcVr8ZC",
   "metadata": {
    "id": "e6O6gbcVr8ZC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jSB9mibcr8cP",
   "metadata": {
    "id": "jSB9mibcr8cP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zHTMitEQr8gT",
   "metadata": {
    "id": "zHTMitEQr8gT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oIln0I3Aru0R",
   "metadata": {
    "id": "oIln0I3Aru0R"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "recent-yahoo",
   "metadata": {
    "id": "recent-yahoo"
   },
   "source": [
    "## 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "kOcnb7QxgxZX",
   "metadata": {
    "id": "kOcnb7QxgxZX"
   },
   "outputs": [],
   "source": [
    "def variant_focal_loss(gamma=2., alpha=0.5, rescale = False):\n",
    "\n",
    "    gamma = float(gamma)\n",
    "    alpha = float(alpha)\n",
    "\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        # print(y_true)\n",
    "        \"\"\"\n",
    "        Focal loss for bianry-classification\n",
    "        FL(p_t)=-rescaled_factor*alpha_t*(1-p_t)^{gamma}log(p_t)\n",
    "        \n",
    "        Notice: \n",
    "        y_pred is probability after sigmoid\n",
    "\n",
    "        Arguments:\n",
    "            y_true {tensor} -- groud truth label, shape of [batch_size, 1]\n",
    "            y_pred {tensor} -- predicted label, shape of [batch_size, 1]\n",
    "\n",
    "        Keyword Arguments:\n",
    "            gamma {float} -- (default: {2.0})  \n",
    "            alpha {float} -- (default: {0.5})\n",
    "\n",
    "        Returns:\n",
    "            [tensor] -- loss.\n",
    "        \"\"\"\n",
    "        epsilon = 1.e-9  \n",
    "        y_true = tf.convert_to_tensor(y_true, tf.float32)\n",
    "        y_pred = tf.convert_to_tensor(y_pred, tf.float32)\n",
    "        model_out = tf.clip_by_value(y_pred, epsilon, 1.-epsilon)  # to advoid numeric underflow\n",
    "        \n",
    "        # compute cross entropy ce = ce_0 + ce_1 = - (1-y)*log(1-y_hat) - y*log(y_hat)\n",
    "        ce_0 = tf.multiply(tf.subtract(1., y_true), -tf.math.log(tf.subtract(1., model_out)))\n",
    "        ce_1 = tf.multiply(y_true, -tf.math.log(model_out))\n",
    "\n",
    "        # compute focal loss fl = fl_0 + fl_1\n",
    "        # obviously fl < ce because of the down-weighting, we can fix it by rescaling\n",
    "        # fl_0 = -(1-y_true)*(1-alpha)*((y_hat)^gamma)*log(1-y_hat) = (1-alpha)*((y_hat)^gamma)*ce_0\n",
    "        fl_0 = tf.multiply(tf.pow(model_out, gamma), ce_0)\n",
    "        fl_0 = tf.multiply(1.-alpha, fl_0)\n",
    "        # fl_1= -y_true*alpha*((1-y_hat)^gamma)*log(y_hat) = alpha*((1-y_hat)^gamma*ce_1\n",
    "        fl_1 = tf.multiply(tf.pow(tf.subtract(1., model_out), gamma), ce_1)\n",
    "        fl_1 = tf.multiply(alpha, fl_1)\n",
    "        fl = tf.add(fl_0, fl_1)\n",
    "        f1_avg = tf.reduce_mean(fl)\n",
    "        \n",
    "        if rescale:\n",
    "            # rescale f1 to keep the quantity as ce\n",
    "            ce = tf.add(ce_0, ce_1)\n",
    "            ce_avg = tf.reduce_mean(ce)\n",
    "            rescaled_factor = tf.divide(ce_avg, f1_avg + epsilon)\n",
    "            f1_avg = tf.multiply(rescaled_factor, f1_avg)\n",
    "        \n",
    "        return f1_avg\n",
    "    \n",
    "    return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "21074562-23bc-4257-a10d-328a14a01395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_loss(y_true, y_pred):\n",
    "    # y_true:真实标签0或者1；y_pred:为正类的概率\n",
    "    loss = 2 * tf.reduce_sum(y_true * y_pred) / tf.reduce_sum(y_true + y_pred) + K.epsilon()\n",
    "    return -loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "signal-assets",
   "metadata": {
    "id": "signal-assets"
   },
   "outputs": [],
   "source": [
    "def transform_y(y_true, y_pred):\n",
    "    mask_value = tf.constant(-1)\n",
    "    mask_y_true = tf.not_equal(tf.cast(y_true, dtype=tf.int32), tf.cast(mask_value, dtype=tf.int32))\n",
    "#     print(f\"mask_y_true:{mask_y_true}\")\n",
    "#     y_true_ = tf.cond(tf.equal(y_true, mask_value), lambda: 0, lambda: y_true)\n",
    "    y_true_ = tf.cast(y_true, dtype=tf.int32) * tf.cast(mask_y_true, dtype=tf.int32)\n",
    "    y_pred_ = tf.cast(y_pred, dtype=tf.float32) * tf.cast(mask_y_true, dtype=tf.float32)\n",
    "#     print(f\"y_true_:{y_true_}, y_pred_:{y_pred_}\")\n",
    "    \n",
    "    return tf.cast(y_true_, dtype=tf.float32), tf.cast(y_pred_, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "strong-assessment",
   "metadata": {
    "id": "strong-assessment"
   },
   "outputs": [],
   "source": [
    "def my_binary_crossentropy(y_true, y_pred):\n",
    "    # print(f\"y_true:{y_true}, y_pred:{y_pred}\")\n",
    "    \n",
    "    y_true, y_pred = transform_y(y_true, y_pred)\n",
    "    # print(f\"y_true_:{y_true}, y_pred_:{y_pred}\")\n",
    "\n",
    "    loss = binary_crossentropy(y_true, y_pred)\n",
    "#     loss = variant_focal_loss()(y_true, y_pred)\n",
    "#     loss = f1_loss(y_true, y_pred)\n",
    "\n",
    "    # y_true0 = tf.constant([y_true.numpy()[0]])\n",
    "    # y_true1 = tf.constant([y_true.numpy()[1]])\n",
    "    # y_pred0 = tf.constant([y_pred.numpy()[0]])\n",
    "    # y_pred1 = tf.constant([y_pred.numpy()[1]])\n",
    "    # loss0 = variant_focal_loss()(y_true0, y_pred0)\n",
    "    # loss1 = variant_focal_loss()(y_true1, y_pred1)\n",
    "    # print(f\"y_true_0:{y_true0}, y_pred_0:{y_pred0}\")\n",
    "    # print(f\"y_true_1:{y_true1}, y_pred_1:{y_pred1}\")\n",
    "    # print(f\"loss0:{loss0}\")\n",
    "    # print(f\"loss1:{loss1}\")\n",
    "\n",
    "    # print(f\"loss:{loss}\")\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "confused-acoustic",
   "metadata": {
    "id": "confused-acoustic"
   },
   "outputs": [],
   "source": [
    "def tarnsform_metrics(y_true, y_pred):\n",
    "    y_true_, y_pred_ = y_true.numpy(), y_pred.numpy()\n",
    "    for i in range(y_true_.shape[0]):\n",
    "        for j in range(y_true_.shape[1]):\n",
    "            if y_true_[i][j] == -1:\n",
    "                y_true_[i][j] = 0\n",
    "                y_pred_[i][j] = random.choice([0, 1])\n",
    "            if y_pred_[i][j] > 0.5:\n",
    "                y_pred_[i][j] = 1\n",
    "            else:\n",
    "                y_pred_[i][j] = 0\n",
    "    return y_true_, y_pred_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "abroad-monaco",
   "metadata": {
    "id": "abroad-monaco"
   },
   "outputs": [],
   "source": [
    "def my_binary_accuracy(y_true, y_pred):\n",
    "#     print(\"my_binary_accuracy\")\n",
    "#     print(f\"y_true:{y_true}, y_pred:{y_pred}\")\n",
    "    \n",
    "    y_true_, y_pred_ = tarnsform_metrics(y_true, y_pred)\n",
    "#     print(f\"y_true_:{y_true_}, y_pred_:{y_pred_}\")\n",
    "\n",
    "    accuracy = binary_accuracy(y_true_, y_pred_)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "radical-chuck",
   "metadata": {
    "id": "radical-chuck"
   },
   "outputs": [],
   "source": [
    "def my_f1_score(y_true, y_pred):\n",
    "#     print(\"my_f1_score\")\n",
    "#     print(f\"y_true:{y_true}, y_pred:{y_pred}\")\n",
    "    \n",
    "    y_true_, y_pred_ = tarnsform_metrics(y_true, y_pred)\n",
    "#     print(f\"y_true_:{y_true_}, y_pred_:{y_pred_}\")\n",
    "\n",
    "    return f1_score(y_true_, y_pred_, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "standard-tomorrow",
   "metadata": {
    "id": "standard-tomorrow"
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    K.clear_session()\n",
    "    \n",
    "    bert_model = TFBertModel.from_pretrained(bert_path, from_pt=True, trainable=True)\n",
    "    for l in bert_model.layers:\n",
    "        l.trainable = True\n",
    " \n",
    "    input_ids_texta = Input(shape=(None,), dtype='int32', name='input_ids_texta')\n",
    "    input_token_type_ids_texta = Input(shape=(None,), dtype='int32', name='input_token_type_ids_texta')\n",
    "    input_attention_mask_texta = Input(shape=(None,), dtype='int32', name='input_attention_mask_texta')\n",
    "    input_ids_textb = Input(shape=(None,), dtype='int32', name='input_ids_textb')\n",
    "    input_token_type_ids_textb = Input(shape=(None,), dtype='int32', name='input_token_type_ids_textb')\n",
    "    input_attention_mask_textb = Input(shape=(None,), dtype='int32', name='input_attention_mask_textb')\n",
    "    input_token_type_ids_textb = Input(shape=(None,), dtype='int32', name='input_token_type_ids_textb')\n",
    "    input_bm25 = Input(shape=(1), dtype='float32', name='input_bm25')\n",
    "    input_tf_cosine = Input(shape=(1), dtype='float32', name='input_tf_cosine')\n",
    "    input_tfidf_cosine = Input(shape=(1), dtype='float32', name='input_tfidf_cosine')\n",
    "    input_cat_texta = Input(shape=(1), dtype='float32', name='input_cat_texta')\n",
    "    input_cat_textb = Input(shape=(1), dtype='float32', name='input_cat_textb')\n",
    " \n",
    "    bert_output_texta = bert_model({'input_ids':input_ids_texta, 'token_type_ids':input_token_type_ids_texta, 'attention_mask':input_attention_mask_texta}, return_dict=False, training=True)\n",
    "    projection_logits_texta = bert_output_texta[0]\n",
    "    bert_cls_texta = Lambda(lambda x: x[:, 0])(projection_logits_texta) # 取出[CLS]对应的向量用来做分类\n",
    "\n",
    "    bert_output_textb = bert_model({'input_ids':input_ids_textb, 'token_type_ids':input_token_type_ids_textb, 'attention_mask':input_attention_mask_textb}, return_dict=False, training=True)\n",
    "    projection_logits_textb = bert_output_textb[0]\n",
    "    bert_cls_textb = Lambda(lambda x: x[:, 0])(projection_logits_textb) # 取出[CLS]对应的向量用来做分类\n",
    "    \n",
    "    subtracted = Subtract()([bert_cls_texta, bert_cls_textb])\n",
    "    cos = Dot(axes=1, normalize=True)([bert_cls_texta, bert_cls_textb]) # dot=1按行点积，normalize=True输出余弦相似度\n",
    "\n",
    "    bert_cls = concatenate([bert_cls_texta, bert_cls_textb, subtracted, cos, input_bm25, input_tf_cosine, input_tfidf_cosine, input_cat_texta, input_cat_textb], axis=-1)\n",
    "    \n",
    "    dense_A_0 = Dense(256, activation='relu')(bert_cls)\n",
    "    dropout_A_0 = Dropout(0.2)(dense_A_0)\n",
    "    dense_A_1 = Dense(32, activation='relu')(dropout_A_0)\n",
    "    dropout_A_1 = Dropout(0.2)(dense_A_1)\n",
    "    output_A = Dense(1, activation='sigmoid', name='output_A')(dropout_A_1)\n",
    "    \n",
    "    dense_B_0 = Dense(256, activation='relu')(bert_cls)\n",
    "    dropout_B_0 = Dropout(0.2)(dense_B_0)\n",
    "    dense_B_1 = Dense(32, activation='relu')(dropout_B_0)\n",
    "    dropout_B_1 = Dropout(0.2)(dense_B_1)\n",
    "    output_B = Dense(1, activation='sigmoid', name='output_B')(dropout_B_1)\n",
    " \n",
    "    model = Model([input_ids_texta, input_token_type_ids_texta, input_attention_mask_texta, \\\n",
    "                   input_ids_textb, input_token_type_ids_textb, input_attention_mask_textb, \\\n",
    "                   input_bm25, input_tf_cosine, input_tfidf_cosine, input_cat_texta, input_cat_textb], \n",
    "                  [output_A, output_B])\n",
    "    \n",
    "    loss={\n",
    "          'output_A':my_binary_crossentropy(),\n",
    "          'output_B':my_binary_crossentropy,\n",
    "      }\n",
    "    \n",
    "    bert_var_name_list = [x.name for x in bert_model.trainable_variables]\n",
    "    model_var_list = [x for x in model.trainable_variables if x.name not in bert_var_name_list]\n",
    "    bert_optimizer = Adam(1e-5)\n",
    "    bert_optimizer_ = bert_optimizer.minimize(loss, var_list=bert_model.trainable_variables)\n",
    "    model_optimizer = Adam(1e-3)\n",
    "    model_optimizer_ = model_optimizer.minimize(loss, var_list=model_var_list)\n",
    "    \n",
    "    model.compile(\n",
    "#                   loss=my_binary_crossentropy,\n",
    "#                   loss={\n",
    "#                       'output_A':my_binary_crossentropy,\n",
    "#                       'output_B':my_binary_crossentropy,\n",
    "#                   },\n",
    "#                   loss='binary_crossentropy',\n",
    "#                   loss=binary_crossentropy,\n",
    "#                   optimizer=Adam(1e-5),    #用足够小的学习率\n",
    "                  optimizer=[bert_optimizer_, model_optimizer_],\n",
    "                  metrics=[my_binary_accuracy, my_f1_score]\n",
    "#                   metrics='accuracy'\n",
    "                 )\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "foreign-tribe",
   "metadata": {
    "id": "foreign-tribe"
   },
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='loss', patience=3)   #早停法，防止过拟合\n",
    "plateau = ReduceLROnPlateau(monitor=\"loss\", verbose=1, factor=0.5, patience=2) #当评价指标不在提升时，减少学习率\n",
    "checkpoint = ModelCheckpoint(check_point_path, monitor='loss', verbose=2, save_best_only=True, save_weights_only=True) #保存最好的模型\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./logs\", update_freq=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-journey",
   "metadata": {
    "id": "rotary-journey"
   },
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "095a98e8-0f84-4e47-977d-9e2aca5a3f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_step(sample_count, batch_size):\n",
    "    step = sample_count // batch_size\n",
    "    if sample_count % batch_size != 0:\n",
    "        step += 1\n",
    "    return step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "569dc75c-fd99-4991-a083-09f4de146168",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['bert.embeddings.position_ids', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`tape` is required when a `Tensor` loss is passed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-154615619aa0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-64-cd433e01a02c>\u001b[0m in \u001b[0;36mget_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mmodel_var_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbert_var_name_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mbert_optimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mbert_optimizer_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbert_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0mmodel_optimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mmodel_optimizer_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_var_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/zsd/code/sohu2021/conda_env/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, var_list, grad_loss, name, tape)\u001b[0m\n\u001b[1;32m    495\u001b[0m     \"\"\"\n\u001b[1;32m    496\u001b[0m     grads_and_vars = self._compute_gradients(\n\u001b[0;32m--> 497\u001b[0;31m         loss, var_list=var_list, grad_loss=grad_loss, tape=tape)\n\u001b[0m\u001b[1;32m    498\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/zsd/code/sohu2021/conda_env/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_compute_gradients\u001b[0;34m(self, loss, var_list, grad_loss, tape)\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# TODO(josh11b): Test that we handle weight decay in a reasonable way.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"`tape` is required when a `Tensor` loss is passed.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m     \u001b[0mtape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mbackprop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: `tape` is required when a `Tensor` loss is passed."
     ]
    }
   ],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "deb8cd1e-18c9-4851-9249-ead48e716bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 2\n",
    "# epochs = 10\n",
    "\n",
    "# train_dataset_iterator = batch_iter(data_path, [\"train.txt\", \"valid.txt\", \"round2.txt\"], tokenizer, batch_size)\n",
    "# train_step = get_step(sample_count, batch_size)\n",
    "\n",
    "# model = get_model()\n",
    "# plot_model(model, \"keras_bert_transformers_two_text_input_SubStract_bm25cosine_1.png\", show_shapes=True)\n",
    "\n",
    "# model.fit(\n",
    "#     train_dataset_iterator,\n",
    "#     # steps_per_epoch=10,\n",
    "#     steps_per_epoch=train_step,\n",
    "#     epochs=epochs,\n",
    "# #       validation_data=dev_dataset_iterator,\n",
    "#   # validation_steps=2,\n",
    "# #       validation_steps=dev_step,\n",
    "# #     validation_split=0.2,\n",
    "# #     class_weight={\n",
    "# #         'output_A':labelA_class_weights,\n",
    "# #         'output_B':labelB_class_weights,\n",
    "# #     },\n",
    "#     callbacks=[early_stopping, plateau, checkpoint, tensorboard_callback],\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# model.save_weights(weights_path)\n",
    "# # model_json = model.to_json()\n",
    "# # with open(config_path, 'w', encoding='utf-8') as file:\n",
    "# #     file.write(model_json)\n",
    "\n",
    "# save_test_result(model, result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "hiM3cmiokFMQ",
   "metadata": {
    "id": "hiM3cmiokFMQ"
   },
   "outputs": [],
   "source": [
    "# model = get_model()\n",
    "# # with open(config_path, 'r', encoding='utf-8') as json_file:\n",
    "# #     loaded_model_json = json_file.read()\n",
    "# # model = model_from_json(loaded_model_json)\n",
    "# model.load_weights(check_point_path)\n",
    "# save_test_result(model, \"trained_model_substract_1/multi_keras_bert_sohu_test_result_epoch3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virgin-quarterly",
   "metadata": {
    "id": "virgin-quarterly"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7958336-4d39-4606-b471-1053f1d158fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tribal-banking",
   "metadata": {
    "id": "tribal-banking"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "likely-celebration",
   "metadata": {
    "id": "likely-celebration"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "latest-biology",
   "metadata": {
    "id": "latest-biology"
   },
   "source": [
    "# 模型加载及测试"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reserved-nightmare",
   "metadata": {
    "id": "reserved-nightmare"
   },
   "source": [
    "## load_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sufficient-shadow",
   "metadata": {
    "id": "sufficient-shadow"
   },
   "source": [
    "## load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-adventure",
   "metadata": {
    "id": "white-adventure"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structural-shark",
   "metadata": {
    "id": "structural-shark"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "o7jaIjHXf1Pk",
   "metadata": {
    "id": "o7jaIjHXf1Pk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9jQK0PLYf1TC",
   "metadata": {
    "id": "9jQK0PLYf1TC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Me0AC8uru-dN",
   "metadata": {
    "id": "Me0AC8uru-dN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CJbObtBvxoH2",
   "metadata": {
    "id": "CJbObtBvxoH2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "reserved-nightmare"
   ],
   "name": "keras_bert_transformers_two_text_input.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
