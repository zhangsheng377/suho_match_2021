{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "precious-column",
   "metadata": {
    "id": "precious-column"
   },
   "source": [
    "# 导包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ivu9Z3WNt4BV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "ivu9Z3WNt4BV",
    "outputId": "a421751c-af12-46f9-a9bd-076e0ee60ef3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.7.10 | packaged by conda-forge | (default, Feb 19 2021, 16:07:37) \\n[GCC 9.3.0]'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys \n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "VNZFCpWIfJ9t",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VNZFCpWIfJ9t",
    "outputId": "404b5dfd-71db-4323-e5a7-e00bf07d5bf9"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "F65Y9KEAiZzD",
   "metadata": {
    "id": "F65Y9KEAiZzD"
   },
   "outputs": [],
   "source": [
    "# !unzip sohu2021_open_data_clean.zip\n",
    "# !unzip chinese_L-12_H-768_A-12.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ZBvllWcDWLZN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZBvllWcDWLZN",
    "outputId": "70daddf4-29ef-48f1-9612-96f639a4c615"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "subject-motorcycle",
   "metadata": {
    "id": "subject-motorcycle"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "import sys\n",
    "import re\n",
    "from collections import Counter\n",
    "import random\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "tf.config.run_functions_eagerly(True)\n",
    "tf.get_logger().setLevel(tf.compat.v1.logging.ERROR)\n",
    "from keras.metrics import top_k_categorical_accuracy, binary_accuracy\n",
    "from keras.layers import *\n",
    "from keras.callbacks import *\n",
    "from keras.models import Model, load_model\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.losses import SparseCategoricalCrossentropy, binary_crossentropy\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    TFBertForPreTraining,\n",
    "    TFBertModel,\n",
    ")\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.utils import class_weight\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "patent-winner",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "patent-winner",
    "outputId": "79dceb89-d54d-4a37-dc00-ade7e64c3987"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "067b1c15-860a-477f-953a-f371ca788697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 16911046687258274317,\n",
       " name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 10770692224\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 6013277038116388007\n",
       " physical_device_desc: \"device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:09:00.0, compute capability: 6.1\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cdebb3b-13f0-4474-9e85-0d8533b5cc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras import backend as K\n",
    "# K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rolled-process",
   "metadata": {
    "id": "rolled-process"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cubic-statistics",
   "metadata": {
    "id": "cubic-statistics"
   },
   "outputs": [],
   "source": [
    "data_path = \"sohu2021_open_data_clean/\"\n",
    "text_max_length = 512\n",
    "bert_path = r\"chinese_L-12_H-768_A-12\"\n",
    "\n",
    "check_point_path = 'trained_model/multi_keras_bert_sohu.weights'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-portal",
   "metadata": {
    "id": "collect-portal"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norman-track",
   "metadata": {
    "id": "norman-track"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-pillow",
   "metadata": {
    "id": "cordless-pillow"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absolute-perception",
   "metadata": {
    "id": "absolute-perception"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "forced-heritage",
   "metadata": {
    "id": "forced-heritage"
   },
   "outputs": [],
   "source": [
    "# 转换bert模型，到pytorch的pd格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "necessary-photograph",
   "metadata": {
    "id": "necessary-photograph"
   },
   "outputs": [],
   "source": [
    "# !transformers-cli convert --model_type bert \\\n",
    "#   --tf_checkpoint chinese_L-12_H-768_A-12/bert_model.ckpt \\\n",
    "#   --config chinese_L-12_H-768_A-12/bert_config.json \\\n",
    "#   --pytorch_dump_output chinese_L-12_H-768_A-12/pytorch_model.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brave-purse",
   "metadata": {
    "id": "brave-purse"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-substance",
   "metadata": {
    "id": "polish-substance"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-strike",
   "metadata": {
    "id": "institutional-strike"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "skilled-possibility",
   "metadata": {
    "id": "skilled-possibility"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "voluntary-pittsburgh",
   "metadata": {
    "id": "voluntary-pittsburgh"
   },
   "source": [
    "# 多任务分支模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protective-script",
   "metadata": {
    "id": "protective-script"
   },
   "source": [
    "## 构建数据迭代器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "minimal-sierra",
   "metadata": {
    "id": "minimal-sierra"
   },
   "outputs": [],
   "source": [
    "label_type_to_id = {'labelA':0, 'labelB':1}\n",
    "label_to_id = {'0':0, '1':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "hungry-principal",
   "metadata": {
    "id": "hungry-principal"
   },
   "outputs": [],
   "source": [
    "def get_text_iterator(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            yield line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22be15ad-aa15-423f-861a-892add822381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _transform_text(text):\n",
    "   text = text.strip().replace('\\n', '。').replace('\\t', '').replace('\\u3000', '')\n",
    "   return re.sub(r'。+', '。', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "pregnant-force",
   "metadata": {
    "id": "pregnant-force"
   },
   "outputs": [],
   "source": [
    "def get_data_iterator(data_path, file_names):\n",
    "    # TODO: 随机取\n",
    "    file_iters = []\n",
    "    for file_name in file_names:\n",
    "      for category in os.listdir(data_path):\n",
    "          category_path = os.path.join(data_path, category)\n",
    "          if not os.path.isdir(category_path):\n",
    "              continue\n",
    "              \n",
    "          file_path = os.path.join(category_path, file_name)\n",
    "          if not os.path.isfile(file_path):\n",
    "              continue\n",
    "              \n",
    "          \n",
    "          file_iter = get_text_iterator(file_path)\n",
    "          cat_source = 0\n",
    "          if category[0] == '长':\n",
    "            cat_source = 1\n",
    "          cat_target = 0\n",
    "          if category[1] == '长':\n",
    "            cat_target = 1\n",
    "          file_iters.append((file_iter, cat_source, cat_target))\n",
    "        \n",
    "    while len(file_iters) > 0:\n",
    "        i = random.randrange(len(file_iters))\n",
    "        line = next(file_iters[i][0], None)\n",
    "        cat_source = file_iters[i][1]\n",
    "        cat_target = file_iters[i][2]\n",
    "        if line is None:\n",
    "            del file_iters[i]\n",
    "            continue\n",
    "            \n",
    "        data = json.loads(line)\n",
    "\n",
    "        data['source'] = _transform_text(data['source'])\n",
    "        if len(data['source']) == 0:\n",
    "            print('source:', line, data)\n",
    "            break\n",
    "#                     continue\n",
    "\n",
    "        data['target'] = _transform_text(data['target'])\n",
    "        if len(data['target']) == 0:\n",
    "            print('target:', line, data)\n",
    "            break\n",
    "#                     continue\n",
    "\n",
    "        label_name_list = list(key for key in data.keys() if key[:5]=='label')\n",
    "        if len(label_name_list) != 1:\n",
    "            print('label_name_list:', line, data)\n",
    "            break\n",
    "#                     continue\n",
    "        label_name = label_name_list[0]\n",
    "        if data[label_name] not in label_to_id.keys():\n",
    "            print('label_name:', line, data, label_name)\n",
    "            break\n",
    "#                     continue\n",
    "        \n",
    "        label_dict = {key: -1 for key in label_type_to_id.keys()}\n",
    "        label_dict[label_name] = label_to_id[data[label_name]]\n",
    "        if label_dict['labelA'] == 0:\n",
    "            label_dict['labelB'] = 0\n",
    "        if label_dict['labelB'] == 1:\n",
    "            label_dict['labelA'] = 1\n",
    "\n",
    "        yield data['source'], data['target'], cat_source, cat_target, label_dict['labelA'], label_dict['labelB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "advised-breathing",
   "metadata": {
    "id": "advised-breathing"
   },
   "outputs": [],
   "source": [
    "it = get_data_iterator(data_path, [\"train.txt\", \"valid.txt\", \"round2.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fossil-recording",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fossil-recording",
    "outputId": "0a9d7206-30af-4b7b-b33a-f62f9674c52c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('谁能打破科比81分纪录？奥尼尔给出5个候选人，补充利拉德比尔！', 'NBA现役能入名人堂的球星很多，但是能被立铜像只有2人', 0, 0, 0, 0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da257b78-2765-4f85-b0b4-de3694adec52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_num(data_path, file_names):\n",
    "    count = 0\n",
    "    it = get_data_iterator(data_path, file_names)\n",
    "    for data in tqdm(it):\n",
    "        count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "pediatric-bookmark",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pediatric-bookmark",
    "outputId": "d0df3034-3e43-4cc6-98ac-ecb131b7dee2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "119168it [00:05, 20830.81it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "119168"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_count = get_sample_num(data_path, [\"train.txt\", \"valid.txt\", \"round2.txt\"])\n",
    "sample_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2zHQy8QwageB",
   "metadata": {
    "id": "2zHQy8QwageB",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_sample_y(data_path, file_names):\n",
    "    labelA_list = []\n",
    "    labelB_list = []\n",
    "    it = get_data_iterator(data_path, file_names)\n",
    "    for source, target, cat_source, cat_target, labelA, labelB in tqdm(it):\n",
    "        if labelA != -1:\n",
    "          labelA_list.append(labelA)\n",
    "        if labelB != -1:\n",
    "          labelB_list.append(labelB)\n",
    "    return labelA_list, labelB_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "whljl4m-cn3l",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "whljl4m-cn3l",
    "outputId": "e2ada33c-7ae4-437e-b984-b555d060d7ab"
   },
   "outputs": [],
   "source": [
    "# labelA_list, labelB_list, labelC_list = get_sample_y(data_path, [\"train.txt\", \"valid.txt\"])\n",
    "# labelA_class_weights = class_weight.compute_class_weight('balanced', np.unique(labelA_list), labelA_list)\n",
    "# labelB_class_weights = class_weight.compute_class_weight('balanced', np.unique(labelB_list), labelB_list)\n",
    "# labelC_class_weights = class_weight.compute_class_weight('balanced', np.unique(labelC_list), labelC_list)\n",
    "# labelA_class_weights, labelB_class_weights, labelC_class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47840218-69ab-4b48-ba48-4ef81899b830",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(bert_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "streaming-amsterdam",
   "metadata": {
    "id": "streaming-amsterdam"
   },
   "outputs": [],
   "source": [
    "def _get_indices(text, text_pair=None):\n",
    "    return tokenizer.encode_plus(text=text,\n",
    "                            text_pair=text_pair,\n",
    "                            max_length=text_max_length, \n",
    "                            add_special_tokens=True, \n",
    "                            padding='max_length', \n",
    "#                             truncation_strategy='longest_first', \n",
    "                            truncation=True,\n",
    "#                                          return_tensors='tf',\n",
    "                            return_token_type_ids=True\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "paperback-dynamics",
   "metadata": {
    "id": "paperback-dynamics"
   },
   "outputs": [],
   "source": [
    "def get_keras_bert_iterator(data_path, file_names, tokenizer):\n",
    "    while True:\n",
    "        data_it = get_data_iterator(data_path, file_names)\n",
    "        for source, target, cat_source, cat_target, labelA, labelB in data_it:\n",
    "            data_source = _get_indices(text=source)\n",
    "            data_target = _get_indices(text=target)\n",
    "#             print(indices, type(indices), len(indices))\n",
    "            yield data_source['input_ids'], data_source['token_type_ids'], data_source['attention_mask'], data_target['input_ids'], data_target['token_type_ids'], data_target['attention_mask'], cat_source, cat_target, labelA, labelB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "chinese-persian",
   "metadata": {
    "id": "chinese-persian"
   },
   "outputs": [],
   "source": [
    "it = get_keras_bert_iterator(data_path, [\"train.txt\", \"valid.txt\", \"round2.txt\"], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "complete-explanation",
   "metadata": {
    "id": "complete-explanation"
   },
   "outputs": [],
   "source": [
    "# next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "suburban-priority",
   "metadata": {
    "id": "suburban-priority"
   },
   "outputs": [],
   "source": [
    "def batch_iter(data_path, file_names, tokenizer, batch_size=64, shuffle=True):\n",
    "    \"\"\"生成批次数据\"\"\"\n",
    "    keras_bert_iter = get_keras_bert_iterator(data_path, file_names, tokenizer)\n",
    "    while True:\n",
    "        data_list = []\n",
    "        for _ in range(batch_size):\n",
    "            data = next(keras_bert_iter)\n",
    "            data_list.append(data)\n",
    "        if shuffle:\n",
    "            random.shuffle(data_list)\n",
    "        \n",
    "        input_ids_texta_list = []\n",
    "        token_type_ids_texta_list = []\n",
    "        attention_mask_texta_list = []\n",
    "        input_ids_textb_list = []\n",
    "        token_type_ids_textb_list = []\n",
    "        attention_mask_textb_list = []\n",
    "        cat_texta_list = []\n",
    "        cat_textb_list = []\n",
    "        labelA_list = []\n",
    "        labelB_list = []\n",
    "        for data in data_list:\n",
    "            input_ids_texta, token_type_ids_texta, attention_mask_texta, input_ids_textb, token_type_ids_textb, attention_mask_textb, cat_texta, cat_textb, labelA, labelB = data\n",
    "#             print(indices, type(indices))\n",
    "            input_ids_texta_list.append(input_ids_texta)\n",
    "            token_type_ids_texta_list.append(token_type_ids_texta)\n",
    "            attention_mask_texta_list.append(attention_mask_texta)\n",
    "            input_ids_textb_list.append(input_ids_textb)\n",
    "            token_type_ids_textb_list.append(token_type_ids_textb)\n",
    "            attention_mask_textb_list.append(attention_mask_textb)\n",
    "            cat_texta_list.append(cat_texta)\n",
    "            cat_textb_list.append(cat_textb)\n",
    "            labelA_list.append(labelA)\n",
    "            labelB_list.append(labelB)\n",
    "\n",
    "        yield [np.array(input_ids_texta_list), np.array(token_type_ids_texta_list), np.array(attention_mask_texta_list), \n",
    "               np.array(input_ids_textb_list), np.array(token_type_ids_textb_list), np.array(attention_mask_textb_list), \n",
    "               np.array(cat_texta_list), np.array(cat_textb_list)], \\\n",
    "            [np.array(labelA_list, dtype=np.int32), np.array(labelB_list, dtype=np.int32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "artificial-truck",
   "metadata": {
    "id": "artificial-truck"
   },
   "outputs": [],
   "source": [
    "it = batch_iter(data_path, [\"train.txt\", \"valid.txt\", \"round2.txt\"], tokenizer, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "broken-guinea",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "broken-guinea",
    "outputId": "d308a85d-fe68-4f14-dc2a-138e1e387f22"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([[ 101, 1266,  776, ...,    0,    0,    0],\n",
       "         [ 101, 7716, 1305, ...,    0,    0,    0]]),\n",
       "  array([[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]]),\n",
       "  array([[1, 1, 1, ..., 0, 0, 0],\n",
       "         [1, 1, 1, ..., 0, 0, 0]]),\n",
       "  array([[ 101, 3173,  776, ...,  711,  712,  102],\n",
       "         [ 101,  791, 1921, ..., 3209, 8024,  102]]),\n",
       "  array([[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]]),\n",
       "  array([[1, 1, 1, ..., 1, 1, 1],\n",
       "         [1, 1, 1, ..., 1, 1, 1]]),\n",
       "  array([0, 0]),\n",
       "  array([1, 1])],\n",
       " [array([1, 0], dtype=int32), array([-1,  0], dtype=int32)])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZcPnAGELrtyP",
   "metadata": {
    "id": "ZcPnAGELrtyP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oOc8KEL8rt7q",
   "metadata": {
    "id": "oOc8KEL8rt7q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "o1Yyn--vrt_z",
   "metadata": {
    "id": "o1Yyn--vrt_z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZFA6-3RYruEk",
   "metadata": {
    "id": "ZFA6-3RYruEk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "qhRn-MB4ruH6",
   "metadata": {
    "id": "qhRn-MB4ruH6"
   },
   "outputs": [],
   "source": [
    "def get_test_data_iterator(data_path, file_name):\n",
    "  # print(data_path)\n",
    "  for category in os.listdir(data_path):\n",
    "    category_path = os.path.join(data_path, category)\n",
    "    # print(category_path)\n",
    "    if not os.path.isdir(category_path):\n",
    "      # print(f\"{category_path} not dir\")\n",
    "      continue\n",
    "        \n",
    "    file_path = os.path.join(category_path, file_name)\n",
    "    # print(file_path)\n",
    "    if not os.path.isfile(file_path):\n",
    "      # print(f\"{file_path} not file\")\n",
    "      continue\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "      for line in f:\n",
    "        # print(line)\n",
    "        data = json.loads(line)\n",
    "        \n",
    "        data['source'] = _transform_text(data['source'])\n",
    "        if len(data['source']) == 0:\n",
    "          print('source:', line, data)\n",
    "          break\n",
    "            \n",
    "        data['target'] = _transform_text(data['target'])\n",
    "        if len(data['target']) == 0:\n",
    "          print('target:', line, data)\n",
    "          break\n",
    "\n",
    "        cat_source = 0\n",
    "        if category[0] == '长':\n",
    "          cat_source = 1\n",
    "        cat_target = 0\n",
    "        if category[1] == '长':\n",
    "          cat_target = 1\n",
    "            \n",
    "        yield data['source'], data['target'], cat_source, cat_target, data['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8jsAuwC8ruSJ",
   "metadata": {
    "id": "8jsAuwC8ruSJ"
   },
   "outputs": [],
   "source": [
    "def get_test_keras_bert_iterator(data_path, file_name):\n",
    "  it = get_test_data_iterator(data_path, file_name)\n",
    "  for source, target, cat_source, cat_target, id in it:\n",
    "    data_source = _get_indices(text=source)\n",
    "    data_target = _get_indices(text=target)\n",
    "    yield data_source['input_ids'], data_source['token_type_ids'], data_source['attention_mask'], data_target['input_ids'], data_target['token_type_ids'], data_target['attention_mask'], cat_source, cat_target, id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "A3aaKzFtrubI",
   "metadata": {
    "id": "A3aaKzFtrubI"
   },
   "outputs": [],
   "source": [
    "def get_test_iterator(data_path, file_name):\n",
    "  it = get_test_keras_bert_iterator(data_path, file_name)\n",
    "  for input_ids_texta, token_type_ids_texta, attention_mask_texta, input_ids_textb, token_type_ids_textb, attention_mask_textb, cat_source, cat_target, id in it:\n",
    "    yield [np.array([input_ids_texta]), np.array([token_type_ids_texta]), np.array([attention_mask_texta]), np.array([input_ids_textb]), np.array([token_type_ids_textb]), np.array([attention_mask_textb]), np.array([cat_source]), np.array([cat_target])], id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "NTmqSZRDv065",
   "metadata": {
    "id": "NTmqSZRDv065"
   },
   "outputs": [],
   "source": [
    "it = get_test_iterator(data_path, \"test_with_id.txt\")\n",
    "# next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "iH42Aoz9r8Sz",
   "metadata": {
    "id": "iH42Aoz9r8Sz"
   },
   "outputs": [],
   "source": [
    "def save_test_result(model, result_file):\n",
    "  it = get_test_iterator(data_path, \"test_with_id.txt\")\n",
    "#   print(\"      \", end=\"\")\n",
    "#   count = 0\n",
    "  with open(result_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(f\"id,label\\n\")\n",
    "    for data, id in tqdm(it):\n",
    "      predict = model.predict(data)\n",
    "      if id[-1] == 'a':\n",
    "        predict_cls = 1 if predict[0][0][0] > 0.5 else 0\n",
    "      elif id[-1] == 'b':\n",
    "        predict_cls = 1 if predict[1][0][0] > 0.5 else 0\n",
    "      else:\n",
    "        print(id)\n",
    "        continue\n",
    "      f.write(f\"{id},{predict_cls}\\n\")\n",
    "#       count += 1\n",
    "#       print(f\"\\b\\b\\b\\b\\b\\b{count}\", end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6O6gbcVr8ZC",
   "metadata": {
    "id": "e6O6gbcVr8ZC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jSB9mibcr8cP",
   "metadata": {
    "id": "jSB9mibcr8cP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zHTMitEQr8gT",
   "metadata": {
    "id": "zHTMitEQr8gT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oIln0I3Aru0R",
   "metadata": {
    "id": "oIln0I3Aru0R"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "recent-yahoo",
   "metadata": {
    "id": "recent-yahoo"
   },
   "source": [
    "## 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "kOcnb7QxgxZX",
   "metadata": {
    "id": "kOcnb7QxgxZX"
   },
   "outputs": [],
   "source": [
    "def variant_focal_loss(gamma=2., alpha=0.5, rescale = False):\n",
    "\n",
    "    gamma = float(gamma)\n",
    "    alpha = float(alpha)\n",
    "\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        # print(y_true)\n",
    "        \"\"\"\n",
    "        Focal loss for bianry-classification\n",
    "        FL(p_t)=-rescaled_factor*alpha_t*(1-p_t)^{gamma}log(p_t)\n",
    "        \n",
    "        Notice: \n",
    "        y_pred is probability after sigmoid\n",
    "\n",
    "        Arguments:\n",
    "            y_true {tensor} -- groud truth label, shape of [batch_size, 1]\n",
    "            y_pred {tensor} -- predicted label, shape of [batch_size, 1]\n",
    "\n",
    "        Keyword Arguments:\n",
    "            gamma {float} -- (default: {2.0})  \n",
    "            alpha {float} -- (default: {0.5})\n",
    "\n",
    "        Returns:\n",
    "            [tensor] -- loss.\n",
    "        \"\"\"\n",
    "        epsilon = 1.e-9  \n",
    "        y_true = tf.convert_to_tensor(y_true, tf.float32)\n",
    "        y_pred = tf.convert_to_tensor(y_pred, tf.float32)\n",
    "        model_out = tf.clip_by_value(y_pred, epsilon, 1.-epsilon)  # to advoid numeric underflow\n",
    "        \n",
    "        # compute cross entropy ce = ce_0 + ce_1 = - (1-y)*log(1-y_hat) - y*log(y_hat)\n",
    "        ce_0 = tf.multiply(tf.subtract(1., y_true), -tf.math.log(tf.subtract(1., model_out)))\n",
    "        ce_1 = tf.multiply(y_true, -tf.math.log(model_out))\n",
    "\n",
    "        # compute focal loss fl = fl_0 + fl_1\n",
    "        # obviously fl < ce because of the down-weighting, we can fix it by rescaling\n",
    "        # fl_0 = -(1-y_true)*(1-alpha)*((y_hat)^gamma)*log(1-y_hat) = (1-alpha)*((y_hat)^gamma)*ce_0\n",
    "        fl_0 = tf.multiply(tf.pow(model_out, gamma), ce_0)\n",
    "        fl_0 = tf.multiply(1.-alpha, fl_0)\n",
    "        # fl_1= -y_true*alpha*((1-y_hat)^gamma)*log(y_hat) = alpha*((1-y_hat)^gamma*ce_1\n",
    "        fl_1 = tf.multiply(tf.pow(tf.subtract(1., model_out), gamma), ce_1)\n",
    "        fl_1 = tf.multiply(alpha, fl_1)\n",
    "        fl = tf.add(fl_0, fl_1)\n",
    "        f1_avg = tf.reduce_mean(fl)\n",
    "        \n",
    "        if rescale:\n",
    "            # rescale f1 to keep the quantity as ce\n",
    "            ce = tf.add(ce_0, ce_1)\n",
    "            ce_avg = tf.reduce_mean(ce)\n",
    "            rescaled_factor = tf.divide(ce_avg, f1_avg + epsilon)\n",
    "            f1_avg = tf.multiply(rescaled_factor, f1_avg)\n",
    "        \n",
    "        return f1_avg\n",
    "    \n",
    "    return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "21074562-23bc-4257-a10d-328a14a01395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_loss(y_true, y_pred):\n",
    "    # y_true:真实标签0或者1；y_pred:为正类的概率\n",
    "    loss = 2 * tf.reduce_sum(y_true * y_pred) / tf.reduce_sum(y_true + y_pred) + K.epsilon()\n",
    "    return -loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "signal-assets",
   "metadata": {
    "id": "signal-assets"
   },
   "outputs": [],
   "source": [
    "def transform_y(y_true, y_pred):\n",
    "    mask_value = tf.constant(-1)\n",
    "    mask_y_true = tf.not_equal(tf.cast(y_true, dtype=tf.int32), tf.cast(mask_value, dtype=tf.int32))\n",
    "#     print(f\"mask_y_true:{mask_y_true}\")\n",
    "#     y_true_ = tf.cond(tf.equal(y_true, mask_value), lambda: 0, lambda: y_true)\n",
    "    y_true_ = tf.cast(y_true, dtype=tf.int32) * tf.cast(mask_y_true, dtype=tf.int32)\n",
    "    y_pred_ = tf.cast(y_pred, dtype=tf.float32) * tf.cast(mask_y_true, dtype=tf.float32)\n",
    "#     print(f\"y_true_:{y_true_}, y_pred_:{y_pred_}\")\n",
    "    \n",
    "    return tf.cast(y_true_, dtype=tf.float32), tf.cast(y_pred_, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "strong-assessment",
   "metadata": {
    "id": "strong-assessment"
   },
   "outputs": [],
   "source": [
    "def my_binary_crossentropy(y_true, y_pred):\n",
    "    # print(f\"y_true:{y_true}, y_pred:{y_pred}\")\n",
    "    \n",
    "    y_true, y_pred = transform_y(y_true, y_pred)\n",
    "    # print(f\"y_true_:{y_true}, y_pred_:{y_pred}\")\n",
    "\n",
    "    # loss = binary_crossentropy(y_true, y_pred)\n",
    "    loss = variant_focal_loss()(y_true, y_pred)\n",
    "#     loss = f1_loss(y_true, y_pred)\n",
    "\n",
    "    # y_true0 = tf.constant([y_true.numpy()[0]])\n",
    "    # y_true1 = tf.constant([y_true.numpy()[1]])\n",
    "    # y_pred0 = tf.constant([y_pred.numpy()[0]])\n",
    "    # y_pred1 = tf.constant([y_pred.numpy()[1]])\n",
    "    # loss0 = variant_focal_loss()(y_true0, y_pred0)\n",
    "    # loss1 = variant_focal_loss()(y_true1, y_pred1)\n",
    "    # print(f\"y_true_0:{y_true0}, y_pred_0:{y_pred0}\")\n",
    "    # print(f\"y_true_1:{y_true1}, y_pred_1:{y_pred1}\")\n",
    "    # print(f\"loss0:{loss0}\")\n",
    "    # print(f\"loss1:{loss1}\")\n",
    "\n",
    "    # print(f\"loss:{loss}\")\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "confused-acoustic",
   "metadata": {
    "id": "confused-acoustic"
   },
   "outputs": [],
   "source": [
    "def tarnsform_metrics(y_true, y_pred):\n",
    "    y_true_, y_pred_ = y_true.numpy(), y_pred.numpy()\n",
    "    for i in range(y_true_.shape[0]):\n",
    "        for j in range(y_true_.shape[1]):\n",
    "            if y_true_[i][j] == -1:\n",
    "                y_true_[i][j] = 0\n",
    "                y_pred_[i][j] = random.choice([0, 1])\n",
    "            if y_pred_[i][j] > 0.5:\n",
    "                y_pred_[i][j] = 1\n",
    "            else:\n",
    "                y_pred_[i][j] = 0\n",
    "    return y_true_, y_pred_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "abroad-monaco",
   "metadata": {
    "id": "abroad-monaco"
   },
   "outputs": [],
   "source": [
    "def my_binary_accuracy(y_true, y_pred):\n",
    "#     print(\"my_binary_accuracy\")\n",
    "#     print(f\"y_true:{y_true}, y_pred:{y_pred}\")\n",
    "    \n",
    "    y_true_, y_pred_ = tarnsform_metrics(y_true, y_pred)\n",
    "#     print(f\"y_true_:{y_true_}, y_pred_:{y_pred_}\")\n",
    "\n",
    "    accuracy = binary_accuracy(y_true_, y_pred_)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "radical-chuck",
   "metadata": {
    "id": "radical-chuck"
   },
   "outputs": [],
   "source": [
    "def my_f1_score(y_true, y_pred):\n",
    "#     print(\"my_f1_score\")\n",
    "#     print(f\"y_true:{y_true}, y_pred:{y_pred}\")\n",
    "    \n",
    "    y_true_, y_pred_ = tarnsform_metrics(y_true, y_pred)\n",
    "#     print(f\"y_true_:{y_true_}, y_pred_:{y_pred_}\")\n",
    "\n",
    "    return f1_score(y_true_, y_pred_, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "standard-tomorrow",
   "metadata": {
    "id": "standard-tomorrow"
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    K.clear_session()\n",
    "    \n",
    "    bert_model = TFBertModel.from_pretrained(bert_path, from_pt=True, trainable=True)\n",
    "    for l in bert_model.layers:\n",
    "        l.trainable = True\n",
    " \n",
    "    input_ids_texta = Input(shape=(None,), dtype='int32', name='input_ids_texta')\n",
    "    input_token_type_ids_texta = Input(shape=(None,), dtype='int32', name='input_token_type_ids_texta')\n",
    "    input_attention_mask_texta = Input(shape=(None,), dtype='int32', name='input_attention_mask_texta')\n",
    "    input_ids_textb = Input(shape=(None,), dtype='int32', name='input_ids_textb')\n",
    "    input_token_type_ids_textb = Input(shape=(None,), dtype='int32', name='input_token_type_ids_textb')\n",
    "    input_attention_mask_textb = Input(shape=(None,), dtype='int32', name='input_attention_mask_textb')\n",
    "    input_token_type_ids_textb = Input(shape=(None,), dtype='int32', name='input_token_type_ids_textb')\n",
    "    input_cat_texta = Input(shape=(1), dtype='float32', name='input_cat_texta')\n",
    "    input_cat_textb = Input(shape=(1), dtype='float32', name='input_cat_textb')\n",
    " \n",
    "    bert_output_texta = bert_model({'input_ids':input_ids_texta, 'token_type_ids':input_token_type_ids_texta, 'attention_mask':input_attention_mask_texta}, return_dict=False, training=True)\n",
    "    projection_logits_texta = bert_output_texta[0]\n",
    "    bert_cls_texta = Lambda(lambda x: x[:, 0])(projection_logits_texta) # 取出[CLS]对应的向量用来做分类\n",
    "\n",
    "    bert_output_textb = bert_model({'input_ids':input_ids_textb, 'token_type_ids':input_token_type_ids_textb, 'attention_mask':input_attention_mask_textb}, return_dict=False, training=True)\n",
    "    projection_logits_textb = bert_output_textb[0]\n",
    "    bert_cls_textb = Lambda(lambda x: x[:, 0])(projection_logits_textb) # 取出[CLS]对应的向量用来做分类\n",
    "\n",
    "    bert_cls = concatenate([bert_cls_texta, bert_cls_textb, input_cat_texta, input_cat_textb], axis=-1)\n",
    "    \n",
    "    dropout_A = Dropout(0.5)(bert_cls)\n",
    "    output_A = Dense(1, activation='sigmoid', name='output_A')(dropout_A)\n",
    "    \n",
    "    dropout_B = Dropout(0.5)(bert_cls)\n",
    "    output_B = Dense(1, activation='sigmoid', name='output_B')(dropout_B)\n",
    " \n",
    "    model = Model([input_ids_texta, input_token_type_ids_texta, input_attention_mask_texta, input_ids_textb, input_token_type_ids_textb, input_attention_mask_textb, input_cat_texta, input_cat_textb], [output_A, output_B])\n",
    "    model.compile(\n",
    "                  loss=my_binary_crossentropy,\n",
    "#                   loss='binary_crossentropy',\n",
    "#                   loss=binary_crossentropy,\n",
    "                  optimizer=Adam(1e-5),    #用足够小的学习率\n",
    "                  metrics=[my_binary_accuracy, my_f1_score]\n",
    "#                   metrics='accuracy'\n",
    "                 )\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "foreign-tribe",
   "metadata": {
    "id": "foreign-tribe"
   },
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='loss', patience=3)   #早停法，防止过拟合\n",
    "plateau = ReduceLROnPlateau(monitor=\"loss\", verbose=1, factor=0.5, patience=2) #当评价指标不在提升时，减少学习率\n",
    "checkpoint = ModelCheckpoint(check_point_path, monitor='loss', verbose=2, save_best_only=True, save_weights_only=True) #保存最好的模型\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./logs\", update_freq=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-journey",
   "metadata": {
    "id": "rotary-journey"
   },
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "095a98e8-0f84-4e47-977d-9e2aca5a3f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_step(sample_count, batch_size):\n",
    "    step = sample_count // batch_size\n",
    "    if sample_count % batch_size != 0:\n",
    "        step += 1\n",
    "    return step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "deb8cd1e-18c9-4851-9249-ead48e716bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 2\n",
    "# epochs = 10\n",
    "\n",
    "# train_dataset_iterator = batch_iter(data_path, [\"train.txt\", \"valid.txt\", \"round2.txt\"], tokenizer, batch_size)\n",
    "# train_step = get_step(sample_count, batch_size)\n",
    "\n",
    "# model = get_model()\n",
    "\n",
    "# model.fit(\n",
    "#   train_dataset_iterator,\n",
    "#   # steps_per_epoch=10,\n",
    "#   steps_per_epoch=train_step,\n",
    "#   epochs=epochs,\n",
    "# #       validation_data=dev_dataset_iterator,\n",
    "#   # validation_steps=2,\n",
    "# #       validation_steps=dev_step,\n",
    "#   callbacks=[early_stopping, plateau, checkpoint, tensorboard_callback],\n",
    "#   verbose=1\n",
    "# )\n",
    "\n",
    "# model.save_weights(f\"trained_model/multi_keras_bert_sohu_final.weights\")\n",
    "\n",
    "# save_test_result(model, f\"trained_model/multi_keras_bert_sohu_test_result_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "hiM3cmiokFMQ",
   "metadata": {
    "id": "hiM3cmiokFMQ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.embeddings.position_ids', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_attention_mask_texta (Inp [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_ids_texta (InputLayer)    [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_token_type_ids_texta (Inp [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_attention_mask_textb (Inp [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_ids_textb (InputLayer)    [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_token_type_ids_textb (Inp [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     TFBaseModelOutputWit 102267648   input_attention_mask_texta[0][0] \n",
      "                                                                 input_ids_texta[0][0]            \n",
      "                                                                 input_token_type_ids_texta[0][0] \n",
      "                                                                 input_attention_mask_textb[0][0] \n",
      "                                                                 input_ids_textb[0][0]            \n",
      "                                                                 input_token_type_ids_textb[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 768)          0           tf_bert_model[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 768)          0           tf_bert_model[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "input_cat_texta (InputLayer)    [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_cat_textb (InputLayer)    [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1538)         0           lambda[0][0]                     \n",
      "                                                                 lambda_1[0][0]                   \n",
      "                                                                 input_cat_texta[0][0]            \n",
      "                                                                 input_cat_textb[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 1538)         0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 1538)         0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "output_A (Dense)                (None, 1)            1539        dropout_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "output_B (Dense)                (None, 1)            1539        dropout_38[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 102,270,726\n",
      "Trainable params: 102,270,726\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/data1/wangchenyue/zsd/code/sohu2021/conda_env/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:3504: UserWarning: Even though the tf.config.experimental_run_functions_eagerly option is set, this option does not apply to tf.data functions. tf.data functions are still traced and executed as graphs.\n",
      "  \"Even though the tf.config.experimental_run_functions_eagerly \"\n",
      "29822it [3:45:52,  2.20it/s]\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.load_weights(check_point_path)\n",
    "save_test_result(model, \"trained_model/multi_keras_bert_sohu_test_result_epoch2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virgin-quarterly",
   "metadata": {
    "id": "virgin-quarterly"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf31a0f-54ef-4b9d-905c-c087fc9dade2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tribal-banking",
   "metadata": {
    "id": "tribal-banking"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "likely-celebration",
   "metadata": {
    "id": "likely-celebration"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "latest-biology",
   "metadata": {
    "id": "latest-biology"
   },
   "source": [
    "# 模型加载及测试"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reserved-nightmare",
   "metadata": {
    "id": "reserved-nightmare"
   },
   "source": [
    "## load_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sufficient-shadow",
   "metadata": {
    "id": "sufficient-shadow"
   },
   "source": [
    "## load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-adventure",
   "metadata": {
    "id": "white-adventure"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structural-shark",
   "metadata": {
    "id": "structural-shark"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "o7jaIjHXf1Pk",
   "metadata": {
    "id": "o7jaIjHXf1Pk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9jQK0PLYf1TC",
   "metadata": {
    "id": "9jQK0PLYf1TC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Me0AC8uru-dN",
   "metadata": {
    "id": "Me0AC8uru-dN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CJbObtBvxoH2",
   "metadata": {
    "id": "CJbObtBvxoH2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "reserved-nightmare"
   ],
   "name": "keras_bert_transformers_two_text_input.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
