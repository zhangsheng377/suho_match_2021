{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "precious-column",
   "metadata": {
    "id": "precious-column"
   },
   "source": [
    "# 导包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ivu9Z3WNt4BV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "ivu9Z3WNt4BV",
    "outputId": "a421751c-af12-46f9-a9bd-076e0ee60ef3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.7.10 | packaged by conda-forge | (default, Feb 19 2021, 16:07:37) \\n[GCC 9.3.0]'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys \n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "VNZFCpWIfJ9t",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VNZFCpWIfJ9t",
    "outputId": "404b5dfd-71db-4323-e5a7-e00bf07d5bf9"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "F65Y9KEAiZzD",
   "metadata": {
    "id": "F65Y9KEAiZzD"
   },
   "outputs": [],
   "source": [
    "# !unzip sohu2021_open_data_clean.zip\n",
    "# !unzip chinese_L-12_H-768_A-12.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ZBvllWcDWLZN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZBvllWcDWLZN",
    "outputId": "70daddf4-29ef-48f1-9612-96f639a4c615"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subject-motorcycle",
   "metadata": {
    "id": "subject-motorcycle"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import sys\n",
    "import re\n",
    "from collections import Counter\n",
    "import random\n",
    "import json\n",
    "from joblib import dump, load\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "# import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "tf.config.run_functions_eagerly(True)\n",
    "tf.get_logger().setLevel(tf.compat.v1.logging.ERROR)\n",
    "from keras.metrics import top_k_categorical_accuracy, binary_accuracy\n",
    "from keras.layers import *\n",
    "from keras.callbacks import *\n",
    "from keras.models import Model, load_model\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.losses import SparseCategoricalCrossentropy, binary_crossentropy\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    TFBertForPreTraining,\n",
    "    TFBertModel,\n",
    ")\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.utils import class_weight\n",
    "import torch\n",
    "from pyhanlp import *\n",
    "import jieba\n",
    "\n",
    "from my_utils import calculate_bm25_similarity, calculate_tf_cosine_similarity, calculate_tfidf_cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patent-winner",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "patent-winner",
    "outputId": "79dceb89-d54d-4a37-dc00-ade7e64c3987"
   },
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067b1c15-860a-477f-953a-f371ca788697",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rolled-process",
   "metadata": {
    "id": "rolled-process"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cubic-statistics",
   "metadata": {
    "id": "cubic-statistics"
   },
   "outputs": [],
   "source": [
    "data_path = \"sohu2021_open_data_clean/\"\n",
    "text_max_length = 512\n",
    "bert_path = r\"chinese_L-12_H-768_A-12\"\n",
    "\n",
    "check_point_path = 'trained_model_substract/multi_keras_bert_sohu.weights'\n",
    "weights_path = \"trained_model_substract/multi_keras_bert_sohu_final.weights\"\n",
    "result_path = \"trained_model_substract/multi_keras_bert_sohu_test_result_final.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-portal",
   "metadata": {
    "id": "collect-portal"
   },
   "outputs": [],
   "source": [
    "bm25Model = load(\"bm25.bin\")\n",
    "bm25Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3932e5-4e86-44a6-ba49-54acf6568517",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norman-track",
   "metadata": {
    "id": "norman-track"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-pillow",
   "metadata": {
    "id": "cordless-pillow"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absolute-perception",
   "metadata": {
    "id": "absolute-perception"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-heritage",
   "metadata": {
    "id": "forced-heritage"
   },
   "outputs": [],
   "source": [
    "# 转换bert模型，到pytorch的pd格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "necessary-photograph",
   "metadata": {
    "id": "necessary-photograph"
   },
   "outputs": [],
   "source": [
    "# !transformers-cli convert --model_type bert \\\n",
    "#   --tf_checkpoint chinese_L-12_H-768_A-12/bert_model.ckpt \\\n",
    "#   --config chinese_L-12_H-768_A-12/bert_config.json \\\n",
    "#   --pytorch_dump_output chinese_L-12_H-768_A-12/pytorch_model.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brave-purse",
   "metadata": {
    "id": "brave-purse"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-substance",
   "metadata": {
    "id": "polish-substance"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-strike",
   "metadata": {
    "id": "institutional-strike"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "skilled-possibility",
   "metadata": {
    "id": "skilled-possibility"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "voluntary-pittsburgh",
   "metadata": {
    "id": "voluntary-pittsburgh"
   },
   "source": [
    "# 多任务分支模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protective-script",
   "metadata": {
    "id": "protective-script"
   },
   "source": [
    "## 构建数据迭代器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-sierra",
   "metadata": {
    "id": "minimal-sierra"
   },
   "outputs": [],
   "source": [
    "label_type_to_id = {'labelA':0, 'labelB':1}\n",
    "label_to_id = {'0':0, '1':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungry-principal",
   "metadata": {
    "id": "hungry-principal"
   },
   "outputs": [],
   "source": [
    "def get_text_iterator(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            yield line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22be15ad-aa15-423f-861a-892add822381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _transform_text(text):\n",
    "   text = text.strip().replace('\\n', '。').replace('\\t', '').replace('\\u3000', '')\n",
    "   return re.sub(r'。+', '。', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741f573d-a1ea-4455-bb79-e717964b643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary(text, senc_num=20):\n",
    "    a = HanLP.extractSummary(text, 20)\n",
    "    a_ = str(a)\n",
    "    return a_[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-force",
   "metadata": {
    "id": "pregnant-force"
   },
   "outputs": [],
   "source": [
    "def get_data_iterator(data_path, file_names):\n",
    "    # TODO: 随机取\n",
    "    file_iters = []\n",
    "    for file_name in file_names:\n",
    "      for category in os.listdir(data_path):\n",
    "          category_path = os.path.join(data_path, category)\n",
    "          if not os.path.isdir(category_path):\n",
    "              continue\n",
    "              \n",
    "          file_path = os.path.join(category_path, file_name)\n",
    "          if not os.path.isfile(file_path):\n",
    "              continue\n",
    "              \n",
    "          \n",
    "          file_iter = get_text_iterator(file_path)\n",
    "          cat_source = 0\n",
    "          if category[0] == '长':\n",
    "            cat_source = 1\n",
    "          cat_target = 0\n",
    "          if category[1] == '长':\n",
    "            cat_target = 1\n",
    "          file_iters.append((file_iter, cat_source, cat_target))\n",
    "        \n",
    "    while len(file_iters) > 0:\n",
    "        i = random.randrange(len(file_iters))\n",
    "        line = next(file_iters[i][0], None)\n",
    "        cat_source = file_iters[i][1]\n",
    "        cat_target = file_iters[i][2]\n",
    "        if line is None:\n",
    "            del file_iters[i]\n",
    "            continue\n",
    "            \n",
    "        data = json.loads(line)\n",
    "\n",
    "        data['source'] = _transform_text(data['source'])\n",
    "        if len(data['source']) == 0:\n",
    "            print('source:', line, data)\n",
    "            break\n",
    "#                     continue\n",
    "\n",
    "        data['target'] = _transform_text(data['target'])\n",
    "        if len(data['target']) == 0:\n",
    "            print('target:', line, data)\n",
    "            break\n",
    "#                     continue\n",
    "\n",
    "        label_name_list = list(key for key in data.keys() if key[:5]=='label')\n",
    "        if len(label_name_list) != 1:\n",
    "            print('label_name_list:', line, data)\n",
    "            break\n",
    "#                     continue\n",
    "        label_name = label_name_list[0]\n",
    "        if data[label_name] not in label_to_id.keys():\n",
    "            print('label_name:', line, data, label_name)\n",
    "            break\n",
    "#                     continue\n",
    "        \n",
    "        label_dict = {key: -1 for key in label_type_to_id.keys()}\n",
    "        label_dict[label_name] = label_to_id[data[label_name]]\n",
    "        if label_dict['labelA'] == 0:\n",
    "            label_dict['labelB'] = 0\n",
    "        if label_dict['labelB'] == 1:\n",
    "            label_dict['labelA'] = 1\n",
    "\n",
    "        yield data['source'], data['target'], cat_source, cat_target, label_dict['labelA'], label_dict['labelB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advised-breathing",
   "metadata": {
    "id": "advised-breathing"
   },
   "outputs": [],
   "source": [
    "it = get_data_iterator(data_path, [\"train.txt\", \"valid.txt\", \"round2.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-recording",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fossil-recording",
    "outputId": "0a9d7206-30af-4b7b-b33a-f62f9674c52c"
   },
   "outputs": [],
   "source": [
    "next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da257b78-2765-4f85-b0b4-de3694adec52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_num(data_path, file_names):\n",
    "    count = 0\n",
    "    it = get_data_iterator(data_path, file_names)\n",
    "    for data in tqdm(it):\n",
    "        count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pediatric-bookmark",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pediatric-bookmark",
    "outputId": "d0df3034-3e43-4cc6-98ac-ecb131b7dee2"
   },
   "outputs": [],
   "source": [
    "sample_count = get_sample_num(data_path, [\"train.txt\", \"valid.txt\", \"round2.txt\"])\n",
    "sample_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2zHQy8QwageB",
   "metadata": {
    "id": "2zHQy8QwageB",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_sample_y(data_path, file_names):\n",
    "    labelA_list = []\n",
    "    labelB_list = []\n",
    "    it = get_data_iterator(data_path, file_names)\n",
    "    for source, target, cat_source, cat_target, labelA, labelB in tqdm(it):\n",
    "        if labelA != -1:\n",
    "          labelA_list.append(labelA)\n",
    "        if labelB != -1:\n",
    "          labelB_list.append(labelB)\n",
    "    return labelA_list, labelB_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whljl4m-cn3l",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "whljl4m-cn3l",
    "outputId": "e2ada33c-7ae4-437e-b984-b555d060d7ab"
   },
   "outputs": [],
   "source": [
    "# labelA_list, labelB_list, labelC_list = get_sample_y(data_path, [\"train.txt\", \"valid.txt\"])\n",
    "# labelA_class_weights = class_weight.compute_class_weight('balanced', np.unique(labelA_list), labelA_list)\n",
    "# labelB_class_weights = class_weight.compute_class_weight('balanced', np.unique(labelB_list), labelB_list)\n",
    "# labelC_class_weights = class_weight.compute_class_weight('balanced', np.unique(labelC_list), labelC_list)\n",
    "# labelA_class_weights, labelB_class_weights, labelC_class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47840218-69ab-4b48-ba48-4ef81899b830",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(bert_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-amsterdam",
   "metadata": {
    "id": "streaming-amsterdam"
   },
   "outputs": [],
   "source": [
    "def _get_indices(text, text_pair=None):\n",
    "    return tokenizer.encode_plus(text=text,\n",
    "                            text_pair=text_pair,\n",
    "                            max_length=text_max_length, \n",
    "                            add_special_tokens=True, \n",
    "                            padding='max_length', \n",
    "#                             truncation_strategy='longest_first', \n",
    "                            truncation=True,\n",
    "#                                          return_tensors='tf',\n",
    "                            return_token_type_ids=True\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-dynamics",
   "metadata": {
    "id": "paperback-dynamics"
   },
   "outputs": [],
   "source": [
    "def get_keras_bert_iterator(data_path, file_names, tokenizer):\n",
    "    while True:\n",
    "        data_it = get_data_iterator(data_path, file_names)\n",
    "        for source, target, cat_source, cat_target, labelA, labelB in data_it:\n",
    "            data_source = _get_indices(text=source)\n",
    "            data_target = _get_indices(text=target)\n",
    "#             print(indices, type(indices), len(indices))\n",
    "            seg_source = jieba.lcut(source)\n",
    "            seg_target = jieba.lcut(target)\n",
    "            bm25 = calculate_bm25_similarity(bm25Model, seg_source, seg_target)\n",
    "            tf_cosine = calculate_tf_cosine_similarity(seg_source, seg_target)\n",
    "            tfidf_cosine = calculate_tfidf_cosine_similarity(seg_source, seg_target, bm25Model.idf)\n",
    "            yield data_source['input_ids'], data_source['token_type_ids'], data_source['attention_mask'], \\\n",
    "                  data_target['input_ids'], data_target['token_type_ids'], data_target['attention_mask'], \\\n",
    "                  bm25, tf_cosine, tfidf_cosine, \\\n",
    "                  cat_source, cat_target, \\\n",
    "                  labelA, labelB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chinese-persian",
   "metadata": {
    "id": "chinese-persian"
   },
   "outputs": [],
   "source": [
    "it = get_keras_bert_iterator(data_path, [\"train.txt\", \"valid.txt\", \"round2.txt\"], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-explanation",
   "metadata": {
    "id": "complete-explanation"
   },
   "outputs": [],
   "source": [
    "# next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suburban-priority",
   "metadata": {
    "id": "suburban-priority"
   },
   "outputs": [],
   "source": [
    "def batch_iter(data_path, file_names, tokenizer, batch_size=64, shuffle=True):\n",
    "    \"\"\"生成批次数据\"\"\"\n",
    "    keras_bert_iter = get_keras_bert_iterator(data_path, file_names, tokenizer)\n",
    "    while True:\n",
    "        data_list = []\n",
    "        for _ in range(batch_size):\n",
    "            data = next(keras_bert_iter)\n",
    "            print(data)\n",
    "            data_list.append(data)\n",
    "        if shuffle:\n",
    "            random.shuffle(data_list)\n",
    "        print(data_list)\n",
    "        \n",
    "        input_ids_texta_list = []\n",
    "        token_type_ids_texta_list = []\n",
    "        attention_mask_texta_list = []\n",
    "        input_ids_textb_list = []\n",
    "        token_type_ids_textb_list = []\n",
    "        attention_mask_textb_list = []\n",
    "        bm25_list = []\n",
    "        tf_cosine_list = []\n",
    "        tfidf_cosine_list = []\n",
    "        cat_texta_list = []\n",
    "        cat_textb_list = []\n",
    "        labelA_list = []\n",
    "        labelB_list = []\n",
    "        for data in data_list:\n",
    "            input_ids_texta, token_type_ids_texta, attention_mask_texta, \\\n",
    "            input_ids_textb, token_type_ids_textb, attention_mask_textb, \\\n",
    "            bm25, tf_cosine, tfidf_cosine, \\\n",
    "            cat_texta, cat_textb, \\\n",
    "            labelA, labelB = data\n",
    "#             print(indices, type(indices))\n",
    "            input_ids_texta_list.append(input_ids_texta)\n",
    "            token_type_ids_texta_list.append(token_type_ids_texta)\n",
    "            attention_mask_texta_list.append(attention_mask_texta)\n",
    "            input_ids_textb_list.append(input_ids_textb)\n",
    "            token_type_ids_textb_list.append(token_type_ids_textb)\n",
    "            attention_mask_textb_list.append(attention_mask_textb)\n",
    "            bm25_list.append(bm25)\n",
    "            tf_cosine_list.append(tf_cosine)\n",
    "            tfidf_cosine_list.append(tfidf_cosine_list)\n",
    "            cat_texta_list.append(cat_texta)\n",
    "            cat_textb_list.append(cat_textb)\n",
    "            labelA_list.append(labelA)\n",
    "            labelB_list.append(labelB)\n",
    "\n",
    "        yield [np.array(input_ids_texta_list), np.array(token_type_ids_texta_list), np.array(attention_mask_texta_list), \n",
    "               np.array(input_ids_textb_list), np.array(token_type_ids_textb_list), np.array(attention_mask_textb_list), \n",
    "               np.array(bm25_list), np.array(tf_cosine_list), np.array(tfidf_cosine_list),\n",
    "               np.array(cat_texta_list), np.array(cat_textb_list)], \\\n",
    "            [np.array(labelA_list, dtype=np.int32), np.array(labelB_list, dtype=np.int32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artificial-truck",
   "metadata": {
    "id": "artificial-truck"
   },
   "outputs": [],
   "source": [
    "it = batch_iter(data_path, [\"train.txt\", \"valid.txt\", \"round2.txt\"], tokenizer, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broken-guinea",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "broken-guinea",
    "outputId": "d308a85d-fe68-4f14-dc2a-138e1e387f22"
   },
   "outputs": [],
   "source": [
    "next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZcPnAGELrtyP",
   "metadata": {
    "id": "ZcPnAGELrtyP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oOc8KEL8rt7q",
   "metadata": {
    "id": "oOc8KEL8rt7q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "o1Yyn--vrt_z",
   "metadata": {
    "id": "o1Yyn--vrt_z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZFA6-3RYruEk",
   "metadata": {
    "id": "ZFA6-3RYruEk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "qhRn-MB4ruH6",
   "metadata": {
    "id": "qhRn-MB4ruH6"
   },
   "outputs": [],
   "source": [
    "def get_test_data_iterator(data_path, file_name):\n",
    "  # print(data_path)\n",
    "  for category in os.listdir(data_path):\n",
    "    category_path = os.path.join(data_path, category)\n",
    "    # print(category_path)\n",
    "    if not os.path.isdir(category_path):\n",
    "      # print(f\"{category_path} not dir\")\n",
    "      continue\n",
    "        \n",
    "    file_path = os.path.join(category_path, file_name)\n",
    "    # print(file_path)\n",
    "    if not os.path.isfile(file_path):\n",
    "      # print(f\"{file_path} not file\")\n",
    "      continue\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "      for line in f:\n",
    "        # print(line)\n",
    "        data = json.loads(line)\n",
    "        \n",
    "        data['source'] = _transform_text(data['source'])\n",
    "        if len(data['source']) == 0:\n",
    "          print('source:', line, data)\n",
    "          break\n",
    "            \n",
    "        data['target'] = _transform_text(data['target'])\n",
    "        if len(data['target']) == 0:\n",
    "          print('target:', line, data)\n",
    "          break\n",
    "\n",
    "        cat_source = 0\n",
    "        if category[0] == '长':\n",
    "          cat_source = 1\n",
    "        cat_target = 0\n",
    "        if category[1] == '长':\n",
    "          cat_target = 1\n",
    "            \n",
    "        yield data['source'], data['target'], cat_source, cat_target, data['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8jsAuwC8ruSJ",
   "metadata": {
    "id": "8jsAuwC8ruSJ"
   },
   "outputs": [],
   "source": [
    "def get_test_keras_bert_iterator(data_path, file_name):\n",
    "  it = get_test_data_iterator(data_path, file_name)\n",
    "  for source, target, cat_source, cat_target, id in it:\n",
    "    data_source = _get_indices(text=source)\n",
    "    data_target = _get_indices(text=target)\n",
    "    yield data_source['input_ids'], data_source['token_type_ids'], data_source['attention_mask'], data_target['input_ids'], data_target['token_type_ids'], data_target['attention_mask'], cat_source, cat_target, id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "A3aaKzFtrubI",
   "metadata": {
    "id": "A3aaKzFtrubI"
   },
   "outputs": [],
   "source": [
    "def get_test_iterator(data_path, file_name):\n",
    "  it = get_test_keras_bert_iterator(data_path, file_name)\n",
    "  for input_ids_texta, token_type_ids_texta, attention_mask_texta, input_ids_textb, token_type_ids_textb, attention_mask_textb, cat_source, cat_target, id in it:\n",
    "    yield [np.array([input_ids_texta]), np.array([token_type_ids_texta]), np.array([attention_mask_texta]), np.array([input_ids_textb]), np.array([token_type_ids_textb]), np.array([attention_mask_textb]), np.array([cat_source]), np.array([cat_target])], id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "NTmqSZRDv065",
   "metadata": {
    "id": "NTmqSZRDv065"
   },
   "outputs": [],
   "source": [
    "it = get_test_iterator(data_path, \"test_with_id.txt\")\n",
    "# next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "iH42Aoz9r8Sz",
   "metadata": {
    "id": "iH42Aoz9r8Sz"
   },
   "outputs": [],
   "source": [
    "def save_test_result(model, result_file):\n",
    "  it = get_test_iterator(data_path, \"test_with_id.txt\")\n",
    "#   print(\"      \", end=\"\")\n",
    "#   count = 0\n",
    "  with open(result_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(f\"id,label\\n\")\n",
    "    for data, id in tqdm(it):\n",
    "      predict = model.predict(data)\n",
    "      if id[-1] == 'a':\n",
    "        predict_cls = 1 if predict[0][0][0] > 0.5 else 0\n",
    "      elif id[-1] == 'b':\n",
    "        predict_cls = 1 if predict[1][0][0] > 0.5 else 0\n",
    "      else:\n",
    "        print(id)\n",
    "        continue\n",
    "      f.write(f\"{id},{predict_cls}\\n\")\n",
    "#       count += 1\n",
    "#       print(f\"\\b\\b\\b\\b\\b\\b{count}\", end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6O6gbcVr8ZC",
   "metadata": {
    "id": "e6O6gbcVr8ZC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jSB9mibcr8cP",
   "metadata": {
    "id": "jSB9mibcr8cP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zHTMitEQr8gT",
   "metadata": {
    "id": "zHTMitEQr8gT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oIln0I3Aru0R",
   "metadata": {
    "id": "oIln0I3Aru0R"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "recent-yahoo",
   "metadata": {
    "id": "recent-yahoo"
   },
   "source": [
    "## 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "kOcnb7QxgxZX",
   "metadata": {
    "id": "kOcnb7QxgxZX"
   },
   "outputs": [],
   "source": [
    "def variant_focal_loss(gamma=2., alpha=0.5, rescale = False):\n",
    "\n",
    "    gamma = float(gamma)\n",
    "    alpha = float(alpha)\n",
    "\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        # print(y_true)\n",
    "        \"\"\"\n",
    "        Focal loss for bianry-classification\n",
    "        FL(p_t)=-rescaled_factor*alpha_t*(1-p_t)^{gamma}log(p_t)\n",
    "        \n",
    "        Notice: \n",
    "        y_pred is probability after sigmoid\n",
    "\n",
    "        Arguments:\n",
    "            y_true {tensor} -- groud truth label, shape of [batch_size, 1]\n",
    "            y_pred {tensor} -- predicted label, shape of [batch_size, 1]\n",
    "\n",
    "        Keyword Arguments:\n",
    "            gamma {float} -- (default: {2.0})  \n",
    "            alpha {float} -- (default: {0.5})\n",
    "\n",
    "        Returns:\n",
    "            [tensor] -- loss.\n",
    "        \"\"\"\n",
    "        epsilon = 1.e-9  \n",
    "        y_true = tf.convert_to_tensor(y_true, tf.float32)\n",
    "        y_pred = tf.convert_to_tensor(y_pred, tf.float32)\n",
    "        model_out = tf.clip_by_value(y_pred, epsilon, 1.-epsilon)  # to advoid numeric underflow\n",
    "        \n",
    "        # compute cross entropy ce = ce_0 + ce_1 = - (1-y)*log(1-y_hat) - y*log(y_hat)\n",
    "        ce_0 = tf.multiply(tf.subtract(1., y_true), -tf.math.log(tf.subtract(1., model_out)))\n",
    "        ce_1 = tf.multiply(y_true, -tf.math.log(model_out))\n",
    "\n",
    "        # compute focal loss fl = fl_0 + fl_1\n",
    "        # obviously fl < ce because of the down-weighting, we can fix it by rescaling\n",
    "        # fl_0 = -(1-y_true)*(1-alpha)*((y_hat)^gamma)*log(1-y_hat) = (1-alpha)*((y_hat)^gamma)*ce_0\n",
    "        fl_0 = tf.multiply(tf.pow(model_out, gamma), ce_0)\n",
    "        fl_0 = tf.multiply(1.-alpha, fl_0)\n",
    "        # fl_1= -y_true*alpha*((1-y_hat)^gamma)*log(y_hat) = alpha*((1-y_hat)^gamma*ce_1\n",
    "        fl_1 = tf.multiply(tf.pow(tf.subtract(1., model_out), gamma), ce_1)\n",
    "        fl_1 = tf.multiply(alpha, fl_1)\n",
    "        fl = tf.add(fl_0, fl_1)\n",
    "        f1_avg = tf.reduce_mean(fl)\n",
    "        \n",
    "        if rescale:\n",
    "            # rescale f1 to keep the quantity as ce\n",
    "            ce = tf.add(ce_0, ce_1)\n",
    "            ce_avg = tf.reduce_mean(ce)\n",
    "            rescaled_factor = tf.divide(ce_avg, f1_avg + epsilon)\n",
    "            f1_avg = tf.multiply(rescaled_factor, f1_avg)\n",
    "        \n",
    "        return f1_avg\n",
    "    \n",
    "    return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "21074562-23bc-4257-a10d-328a14a01395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_loss(y_true, y_pred):\n",
    "    # y_true:真实标签0或者1；y_pred:为正类的概率\n",
    "    loss = 2 * tf.reduce_sum(y_true * y_pred) / tf.reduce_sum(y_true + y_pred) + K.epsilon()\n",
    "    return -loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "signal-assets",
   "metadata": {
    "id": "signal-assets"
   },
   "outputs": [],
   "source": [
    "def transform_y(y_true, y_pred):\n",
    "    mask_value = tf.constant(-1)\n",
    "    mask_y_true = tf.not_equal(tf.cast(y_true, dtype=tf.int32), tf.cast(mask_value, dtype=tf.int32))\n",
    "#     print(f\"mask_y_true:{mask_y_true}\")\n",
    "#     y_true_ = tf.cond(tf.equal(y_true, mask_value), lambda: 0, lambda: y_true)\n",
    "    y_true_ = tf.cast(y_true, dtype=tf.int32) * tf.cast(mask_y_true, dtype=tf.int32)\n",
    "    y_pred_ = tf.cast(y_pred, dtype=tf.float32) * tf.cast(mask_y_true, dtype=tf.float32)\n",
    "#     print(f\"y_true_:{y_true_}, y_pred_:{y_pred_}\")\n",
    "    \n",
    "    return tf.cast(y_true_, dtype=tf.float32), tf.cast(y_pred_, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "strong-assessment",
   "metadata": {
    "id": "strong-assessment"
   },
   "outputs": [],
   "source": [
    "def my_binary_crossentropy(y_true, y_pred):\n",
    "    # print(f\"y_true:{y_true}, y_pred:{y_pred}\")\n",
    "    \n",
    "    y_true, y_pred = transform_y(y_true, y_pred)\n",
    "    # print(f\"y_true_:{y_true}, y_pred_:{y_pred}\")\n",
    "\n",
    "    loss = binary_crossentropy(y_true, y_pred)\n",
    "#     loss = variant_focal_loss()(y_true, y_pred)\n",
    "#     loss = f1_loss(y_true, y_pred)\n",
    "\n",
    "    # y_true0 = tf.constant([y_true.numpy()[0]])\n",
    "    # y_true1 = tf.constant([y_true.numpy()[1]])\n",
    "    # y_pred0 = tf.constant([y_pred.numpy()[0]])\n",
    "    # y_pred1 = tf.constant([y_pred.numpy()[1]])\n",
    "    # loss0 = variant_focal_loss()(y_true0, y_pred0)\n",
    "    # loss1 = variant_focal_loss()(y_true1, y_pred1)\n",
    "    # print(f\"y_true_0:{y_true0}, y_pred_0:{y_pred0}\")\n",
    "    # print(f\"y_true_1:{y_true1}, y_pred_1:{y_pred1}\")\n",
    "    # print(f\"loss0:{loss0}\")\n",
    "    # print(f\"loss1:{loss1}\")\n",
    "\n",
    "    # print(f\"loss:{loss}\")\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "confused-acoustic",
   "metadata": {
    "id": "confused-acoustic"
   },
   "outputs": [],
   "source": [
    "def tarnsform_metrics(y_true, y_pred):\n",
    "    y_true_, y_pred_ = y_true.numpy(), y_pred.numpy()\n",
    "    for i in range(y_true_.shape[0]):\n",
    "        for j in range(y_true_.shape[1]):\n",
    "            if y_true_[i][j] == -1:\n",
    "                y_true_[i][j] = 0\n",
    "                y_pred_[i][j] = random.choice([0, 1])\n",
    "            if y_pred_[i][j] > 0.5:\n",
    "                y_pred_[i][j] = 1\n",
    "            else:\n",
    "                y_pred_[i][j] = 0\n",
    "    return y_true_, y_pred_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "abroad-monaco",
   "metadata": {
    "id": "abroad-monaco"
   },
   "outputs": [],
   "source": [
    "def my_binary_accuracy(y_true, y_pred):\n",
    "#     print(\"my_binary_accuracy\")\n",
    "#     print(f\"y_true:{y_true}, y_pred:{y_pred}\")\n",
    "    \n",
    "    y_true_, y_pred_ = tarnsform_metrics(y_true, y_pred)\n",
    "#     print(f\"y_true_:{y_true_}, y_pred_:{y_pred_}\")\n",
    "\n",
    "    accuracy = binary_accuracy(y_true_, y_pred_)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "radical-chuck",
   "metadata": {
    "id": "radical-chuck"
   },
   "outputs": [],
   "source": [
    "def my_f1_score(y_true, y_pred):\n",
    "#     print(\"my_f1_score\")\n",
    "#     print(f\"y_true:{y_true}, y_pred:{y_pred}\")\n",
    "    \n",
    "    y_true_, y_pred_ = tarnsform_metrics(y_true, y_pred)\n",
    "#     print(f\"y_true_:{y_true_}, y_pred_:{y_pred_}\")\n",
    "\n",
    "    return f1_score(y_true_, y_pred_, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "standard-tomorrow",
   "metadata": {
    "id": "standard-tomorrow"
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    K.clear_session()\n",
    "    \n",
    "    bert_model = TFBertModel.from_pretrained(bert_path, from_pt=True, trainable=True)\n",
    "    for l in bert_model.layers:\n",
    "        l.trainable = True\n",
    " \n",
    "    input_ids_texta = Input(shape=(None,), dtype='int32', name='input_ids_texta')\n",
    "    input_token_type_ids_texta = Input(shape=(None,), dtype='int32', name='input_token_type_ids_texta')\n",
    "    input_attention_mask_texta = Input(shape=(None,), dtype='int32', name='input_attention_mask_texta')\n",
    "    input_ids_textb = Input(shape=(None,), dtype='int32', name='input_ids_textb')\n",
    "    input_token_type_ids_textb = Input(shape=(None,), dtype='int32', name='input_token_type_ids_textb')\n",
    "    input_attention_mask_textb = Input(shape=(None,), dtype='int32', name='input_attention_mask_textb')\n",
    "    input_token_type_ids_textb = Input(shape=(None,), dtype='int32', name='input_token_type_ids_textb')\n",
    "    input_cat_texta = Input(shape=(1), dtype='float32', name='input_cat_texta')\n",
    "    input_cat_textb = Input(shape=(1), dtype='float32', name='input_cat_textb')\n",
    " \n",
    "    bert_output_texta = bert_model({'input_ids':input_ids_texta, 'token_type_ids':input_token_type_ids_texta, 'attention_mask':input_attention_mask_texta}, return_dict=False, training=True)\n",
    "    projection_logits_texta = bert_output_texta[0]\n",
    "    bert_cls_texta = Lambda(lambda x: x[:, 0])(projection_logits_texta) # 取出[CLS]对应的向量用来做分类\n",
    "\n",
    "    bert_output_textb = bert_model({'input_ids':input_ids_textb, 'token_type_ids':input_token_type_ids_textb, 'attention_mask':input_attention_mask_textb}, return_dict=False, training=True)\n",
    "    projection_logits_textb = bert_output_textb[0]\n",
    "    bert_cls_textb = Lambda(lambda x: x[:, 0])(projection_logits_textb) # 取出[CLS]对应的向量用来做分类\n",
    "    \n",
    "    subtracted = Subtract()([bert_cls_texta, bert_cls_textb])\n",
    "    cos = Dot(axes=1, normalize=True)([bert_cls_texta, bert_cls_textb]) # dot=1按行点积，normalize=True输出余弦相似度\n",
    "\n",
    "    bert_cls = concatenate([bert_cls_texta, bert_cls_textb, subtracted, cos, input_cat_texta, input_cat_textb], axis=-1)\n",
    "    \n",
    "    dropout_A = Dropout(0.5)(bert_cls)\n",
    "    output_A = Dense(1, activation='sigmoid', name='output_A')(dropout_A)\n",
    "    \n",
    "    dropout_B = Dropout(0.5)(bert_cls)\n",
    "    output_B = Dense(1, activation='sigmoid', name='output_B')(dropout_B)\n",
    " \n",
    "    model = Model([input_ids_texta, input_token_type_ids_texta, input_attention_mask_texta, input_ids_textb, input_token_type_ids_textb, input_attention_mask_textb, input_cat_texta, input_cat_textb], [output_A, output_B])\n",
    "    model.compile(\n",
    "                  loss=my_binary_crossentropy,\n",
    "#                   loss='binary_crossentropy',\n",
    "#                   loss=binary_crossentropy,\n",
    "                  optimizer=Adam(1e-5),    #用足够小的学习率\n",
    "                  metrics=[my_binary_accuracy, my_f1_score]\n",
    "#                   metrics='accuracy'\n",
    "                 )\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "foreign-tribe",
   "metadata": {
    "id": "foreign-tribe"
   },
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='loss', patience=3)   #早停法，防止过拟合\n",
    "plateau = ReduceLROnPlateau(monitor=\"loss\", verbose=1, factor=0.5, patience=2) #当评价指标不在提升时，减少学习率\n",
    "checkpoint = ModelCheckpoint(check_point_path, monitor='loss', verbose=2, save_best_only=True, save_weights_only=True) #保存最好的模型\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./logs\", update_freq=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-journey",
   "metadata": {
    "id": "rotary-journey"
   },
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "095a98e8-0f84-4e47-977d-9e2aca5a3f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_step(sample_count, batch_size):\n",
    "    step = sample_count // batch_size\n",
    "    if sample_count % batch_size != 0:\n",
    "        step += 1\n",
    "    return step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "deb8cd1e-18c9-4851-9249-ead48e716bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'bert.embeddings.position_ids', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_attention_mask_texta (Inp [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_ids_texta (InputLayer)    [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_token_type_ids_texta (Inp [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_attention_mask_textb (Inp [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_ids_textb (InputLayer)    [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_token_type_ids_textb (Inp [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     TFBaseModelOutputWit 102267648   input_attention_mask_texta[0][0] \n",
      "                                                                 input_ids_texta[0][0]            \n",
      "                                                                 input_token_type_ids_texta[0][0] \n",
      "                                                                 input_attention_mask_textb[0][0] \n",
      "                                                                 input_ids_textb[0][0]            \n",
      "                                                                 input_token_type_ids_textb[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 768)          0           tf_bert_model[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 768)          0           tf_bert_model[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "subtract (Subtract)             (None, 768)          0           lambda[0][0]                     \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot (Dot)                       (None, 1)            0           lambda[0][0]                     \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_cat_texta (InputLayer)    [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_cat_textb (InputLayer)    [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 2307)         0           lambda[0][0]                     \n",
      "                                                                 lambda_1[0][0]                   \n",
      "                                                                 subtract[0][0]                   \n",
      "                                                                 dot[0][0]                        \n",
      "                                                                 input_cat_texta[0][0]            \n",
      "                                                                 input_cat_textb[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 2307)         0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 2307)         0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "output_A (Dense)                (None, 1)            2308        dropout_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "output_B (Dense)                (None, 1)            2308        dropout_38[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 102,272,264\n",
      "Trainable params: 102,272,264\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/wangchenyue/zsd/code/sohu2021/conda_env/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:3504: UserWarning: Even though the tf.config.experimental_run_functions_eagerly option is set, this option does not apply to tf.data functions. tf.data functions are still traced and executed as graphs.\n",
      "  \"Even though the tf.config.experimental_run_functions_eagerly \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  111/59584 [..............................] - ETA: 12:08:21 - loss: 1.0086 - output_A_loss: 0.4480 - output_B_loss: 0.5606 - output_A_my_binary_accuracy: 0.5187 - output_A_my_f1_score: 0.4127 - output_B_my_binary_accuracy: 0.7304 - output_B_my_f1_score: 0.7097"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-b28a12a54add>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#       validation_steps=dev_step,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplateau\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensorboard_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m   \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/zsd/code/sohu2021/conda_env/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/zsd/code/sohu2021/conda_env/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    803\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0;34m\"\"\"Runs a training execution with one step.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 805\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mstep_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/zsd/code/sohu2021/conda_env/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mstep_function\u001b[0;34m(model, iterator)\u001b[0m\n\u001b[1;32m    793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m       \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    796\u001b[0m       outputs = reduce_per_replica(\n\u001b[1;32m    797\u001b[0m           outputs, self.distribute_strategy, reduction='first')\n",
      "\u001b[0;32m~/zsd/code/sohu2021/conda_env/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1257\u001b[0m       fn = autograph.tf_convert(\n\u001b[1;32m   1258\u001b[0m           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[0;32m-> 1259\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extended\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/zsd/code/sohu2021/conda_env/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mcall_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   2728\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2729\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2730\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2732\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/zsd/code/sohu2021/conda_env/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   3415\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3416\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mReplicaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplica_id_in_sync_group\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3417\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3419\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_reduce_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestinations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/zsd/code/sohu2021/conda_env/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    570\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUNSPECIFIED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mismethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/zsd/code/sohu2021/conda_env/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mrun_step\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mrun_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m         \u001b[0;31m# Ensure counter is updated only if `train_step` succeeds.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_minimum_control_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/zsd/code/sohu2021/conda_env/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mbackprop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m       \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m       loss = self.compiled_loss(\n\u001b[1;32m    756\u001b[0m           y, y_pred, sample_weight, regularization_losses=self.losses)\n",
      "\u001b[0;32m~/zsd/code/sohu2021/conda_env/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1011\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1012\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/zsd/code/sohu2021/conda_env/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    423\u001b[0m     \"\"\"\n\u001b[1;32m    424\u001b[0m     return self._run_internal_graph(\n\u001b[0;32m--> 425\u001b[0;31m         inputs, training=training, mask=mask)\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/zsd/code/sohu2021/conda_env/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/zsd/code/sohu2021/conda_env/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1011\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1012\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/zsd/code/sohu2021/conda_env/lib/python3.7/site-packages/transformers/models/bert/modeling_tf_bert.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, training, **kwargs)\u001b[0m\n\u001b[1;32m    895\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"output_hidden_states\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"return_dict\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 897\u001b[0;31m             \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"training\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m         )\n\u001b[1;32m    899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/zsd/code/sohu2021/conda_env/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1011\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1012\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/zsd/code/sohu2021/conda_env/lib/python3.7/site-packages/transformers/models/bert/modeling_tf_bert.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, training, **kwargs)\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"output_hidden_states\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"return_dict\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"training\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m         )\n\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/zsd/code/sohu2021/conda_env/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1011\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1012\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/zsd/code/sohu2021/conda_env/lib/python3.7/site-packages/transformers/models/bert/modeling_tf_bert.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m    436\u001b[0m                 \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m                 \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m                 \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m             )\n\u001b[1;32m    440\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/zsd/code/sohu2021/conda_env/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1011\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1012\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/zsd/code/sohu2021/conda_env/lib/python3.7/site-packages/transformers/models/bert/modeling_tf_bert.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, hidden_states, attention_mask, head_mask, output_attentions, training)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         layer_output = self.bert_output(\n\u001b[0;32m--> 403\u001b[0;31m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m         )\n\u001b[1;32m    405\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mattention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# add attentions if we output them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/zsd/code/sohu2021/conda_env/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1011\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1012\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/zsd/code/sohu2021/conda_env/lib/python3.7/site-packages/transformers/models/bert/modeling_tf_bert.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, hidden_states, input_tensor, training)\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/zsd/code/sohu2021/conda_env/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1011\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1012\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/zsd/code/sohu2021/conda_env/lib/python3.7/site-packages/tensorflow/python/keras/layers/normalization.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1259\u001b[0m           \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1260\u001b[0m           \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1261\u001b[0;31m           variance_epsilon=self.epsilon)\n\u001b[0m\u001b[1;32m   1262\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/zsd/code/sohu2021/conda_env/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/zsd/code/sohu2021/conda_env/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py\u001b[0m in \u001b[0;36mbatch_normalization\u001b[0;34m(x, mean, variance, offset, scale, variance_epsilon, name)\u001b[0m\n\u001b[1;32m   1562\u001b[0m     \u001b[0;31m# the precise order of ops that are generated by the expression below.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1563\u001b[0m     return x * math_ops.cast(inv, x.dtype) + math_ops.cast(\n\u001b[0;32m-> 1564\u001b[0;31m         offset - mean * inv if offset is not None else -mean * inv, x.dtype)\n\u001b[0m\u001b[1;32m   1565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/zsd/code/sohu2021/conda_env/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m_run_op\u001b[0;34m(a, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1070\u001b[0m     \u001b[0mtensor_oper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mtensor_oper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "epochs = 10\n",
    "\n",
    "train_dataset_iterator = batch_iter(data_path, [\"train.txt\", \"valid.txt\", \"round2.txt\"], tokenizer, batch_size)\n",
    "train_step = get_step(sample_count, batch_size)\n",
    "\n",
    "model = get_model()\n",
    "\n",
    "model.fit(\n",
    "  train_dataset_iterator,\n",
    "  # steps_per_epoch=10,\n",
    "  steps_per_epoch=train_step,\n",
    "  epochs=epochs,\n",
    "#       validation_data=dev_dataset_iterator,\n",
    "  # validation_steps=2,\n",
    "#       validation_steps=dev_step,\n",
    "  callbacks=[early_stopping, plateau, checkpoint, tensorboard_callback],\n",
    "  verbose=1\n",
    ")\n",
    "\n",
    "model.save_weights(weights_path)\n",
    "\n",
    "save_test_result(model, result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hiM3cmiokFMQ",
   "metadata": {
    "id": "hiM3cmiokFMQ"
   },
   "outputs": [],
   "source": [
    "# model = get_model()\n",
    "# model.load_weights(check_point_path)\n",
    "# save_test_result(model, \"trained_model_summary/multi_keras_bert_sohu_test_result_epoch2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virgin-quarterly",
   "metadata": {
    "id": "virgin-quarterly"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7958336-4d39-4606-b471-1053f1d158fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tribal-banking",
   "metadata": {
    "id": "tribal-banking"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "likely-celebration",
   "metadata": {
    "id": "likely-celebration"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "latest-biology",
   "metadata": {
    "id": "latest-biology"
   },
   "source": [
    "# 模型加载及测试"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reserved-nightmare",
   "metadata": {
    "id": "reserved-nightmare"
   },
   "source": [
    "## load_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sufficient-shadow",
   "metadata": {
    "id": "sufficient-shadow"
   },
   "source": [
    "## load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-adventure",
   "metadata": {
    "id": "white-adventure"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structural-shark",
   "metadata": {
    "id": "structural-shark"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "o7jaIjHXf1Pk",
   "metadata": {
    "id": "o7jaIjHXf1Pk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9jQK0PLYf1TC",
   "metadata": {
    "id": "9jQK0PLYf1TC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Me0AC8uru-dN",
   "metadata": {
    "id": "Me0AC8uru-dN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CJbObtBvxoH2",
   "metadata": {
    "id": "CJbObtBvxoH2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "reserved-nightmare"
   ],
   "name": "keras_bert_transformers_two_text_input.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
