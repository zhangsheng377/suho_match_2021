{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f027699",
   "metadata": {
    "id": "precious-column"
   },
   "source": [
    "# 导包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f2d7421",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "ivu9Z3WNt4BV",
    "outputId": "a421751c-af12-46f9-a9bd-076e0ee60ef3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.7.10 | packaged by conda-forge | (default, Feb 19 2021, 16:07:37) \\n[GCC 9.3.0]'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys \n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d68024fb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VNZFCpWIfJ9t",
    "outputId": "404b5dfd-71db-4323-e5a7-e00bf07d5bf9"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6337d460",
   "metadata": {
    "id": "F65Y9KEAiZzD"
   },
   "outputs": [],
   "source": [
    "# !unzip sohu2021_open_data_clean.zip\n",
    "# !unzip chinese_L-12_H-768_A-12.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7022a024",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZBvllWcDWLZN",
    "outputId": "70daddf4-29ef-48f1-9612-96f639a4c615"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e863158",
   "metadata": {
    "id": "subject-motorcycle"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"\n",
    "import sys\n",
    "import re\n",
    "from collections import Counter\n",
    "import random\n",
    "import json\n",
    "from joblib import dump, load\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "import multiprocessing\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "# tf.config.run_functions_eagerly(True)\n",
    "tf.get_logger().setLevel(tf.compat.v1.logging.ERROR)\n",
    "# import keras\n",
    "from tensorflow.keras.metrics import top_k_categorical_accuracy, binary_accuracy\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras.models import Model, load_model, model_from_json\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy, binary_crossentropy\n",
    "# from keras.utils import multi_gpu_model\n",
    "# from keras.utils.training_utils import multi_gpu_model\n",
    "# from tensorflow.keras.utils import multi_gpu_model\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    TFBertForPreTraining,\n",
    "    TFBertModel,\n",
    ")\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.utils import class_weight\n",
    "import torch\n",
    "from pyhanlp import *\n",
    "import jieba\n",
    "\n",
    "from my_utils import calculate_bm25_similarity, calculate_tf_cosine_similarity, calculate_tfidf_cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c2c73e3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "patent-winner",
    "outputId": "79dceb89-d54d-4a37-dc00-ade7e64c3987"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce736f44-0485-477c-8be8-6b391fd6dc74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91c2ba6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 6479530518984668181,\n",
       " name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 10770692224\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "     link {\n",
       "       device_id: 1\n",
       "       type: \"StreamExecutor\"\n",
       "       strength: 1\n",
       "     }\n",
       "   }\n",
       " }\n",
       " incarnation: 11269919452055794458\n",
       " physical_device_desc: \"device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:05:00.0, compute capability: 6.1\",\n",
       " name: \"/device:GPU:1\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 10770692224\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "     link {\n",
       "       type: \"StreamExecutor\"\n",
       "       strength: 1\n",
       "     }\n",
       "   }\n",
       " }\n",
       " incarnation: 11529704731331391573\n",
       " physical_device_desc: \"device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:08:00.0, compute capability: 6.1\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4048bc2f-e7e4-41b2-bbaf-e4c6b295425c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices: 2\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05ec181",
   "metadata": {
    "id": "rolled-process"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c517425",
   "metadata": {
    "id": "cubic-statistics"
   },
   "outputs": [],
   "source": [
    "data_path = \"sohu2021_open_data_clean/\"\n",
    "# train_file_names = [\"train.txt\", \"valid.txt\", \"round2.txt\", \"round3.txt\"]\n",
    "train_file_name = \"data/shuffle_total_file.json\"\n",
    "text_max_length = 512\n",
    "bert_path = r\"chinese_L-12_H-768_A-12\"\n",
    "\n",
    "check_point_path = 'trained_model_substract_1/multi_keras_bert_sohu.weights'\n",
    "weights_path = \"trained_model_substract_1/multi_keras_bert_sohu_final.weights\"\n",
    "config_path = \"trained_model_substract_1/multi_keras_bert_sohu_final.model_config.json\"\n",
    "result_path = \"trained_model_substract_1/multi_keras_bert_sohu_test_result_final.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "166dec1b",
   "metadata": {
    "id": "collect-portal"
   },
   "outputs": [],
   "source": [
    "# bm25Model = load(\"bm25.bin\")\n",
    "# bm25Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9f94c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba1511a",
   "metadata": {
    "id": "norman-track"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dd18e8",
   "metadata": {
    "id": "cordless-pillow"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1626dd7",
   "metadata": {
    "id": "absolute-perception"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b7481ea",
   "metadata": {
    "id": "forced-heritage"
   },
   "outputs": [],
   "source": [
    "# 转换bert模型，到pytorch的pd格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6631ee6",
   "metadata": {
    "id": "necessary-photograph"
   },
   "outputs": [],
   "source": [
    "# !transformers-cli convert --model_type bert \\\n",
    "#   --tf_checkpoint chinese_L-12_H-768_A-12/bert_model.ckpt \\\n",
    "#   --config chinese_L-12_H-768_A-12/bert_config.json \\\n",
    "#   --pytorch_dump_output chinese_L-12_H-768_A-12/pytorch_model.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b10230",
   "metadata": {
    "id": "brave-purse"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a2ab09",
   "metadata": {
    "id": "polish-substance"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb58565",
   "metadata": {
    "id": "institutional-strike"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6117413",
   "metadata": {
    "id": "skilled-possibility"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0786ea75",
   "metadata": {
    "id": "voluntary-pittsburgh"
   },
   "source": [
    "# 多任务分支模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d971136",
   "metadata": {
    "id": "protective-script"
   },
   "source": [
    "## 构建数据迭代器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88eff7b1",
   "metadata": {
    "id": "minimal-sierra"
   },
   "outputs": [],
   "source": [
    "label_type_to_id = {'labelA':0, 'labelB':1}\n",
    "label_to_id = {'0':0, '1':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1adda0c",
   "metadata": {
    "id": "hungry-principal"
   },
   "outputs": [],
   "source": [
    "# def get_text_iterator(file_path):\n",
    "#     with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#         for line in f:\n",
    "#             yield line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1144b266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _transform_text(text):\n",
    "   text = text.strip().replace('\\n', '。').replace('\\t', '').replace('\\u3000', '')\n",
    "   return re.sub(r'。+', '。', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ad80235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_summary(text, senc_num=20):\n",
    "#     a = HanLP.extractSummary(text, 20)\n",
    "#     a_ = str(a)\n",
    "#     return a_[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36f1931e",
   "metadata": {
    "id": "pregnant-force"
   },
   "outputs": [],
   "source": [
    "# def get_data_iterator(data_path, file_names):\n",
    "#     # TODO: 随机取\n",
    "#     file_iters = []\n",
    "#     for file_name in file_names:\n",
    "#       for category in os.listdir(data_path):\n",
    "#           category_path = os.path.join(data_path, category)\n",
    "#           if not os.path.isdir(category_path):\n",
    "#               continue\n",
    "              \n",
    "#           file_path = os.path.join(category_path, file_name)\n",
    "#           if not os.path.isfile(file_path):\n",
    "#               continue\n",
    "              \n",
    "          \n",
    "#           file_iter = get_text_iterator(file_path)\n",
    "#           cat_source = 0\n",
    "#           if category[0] == '长':\n",
    "#             cat_source = 1\n",
    "#           cat_target = 0\n",
    "#           if category[1] == '长':\n",
    "#             cat_target = 1\n",
    "#           file_iters.append((file_iter, cat_source, cat_target))\n",
    "        \n",
    "#     while len(file_iters) > 0:\n",
    "#         i = random.randrange(len(file_iters))\n",
    "#         line = next(file_iters[i][0], None)\n",
    "#         cat_source = file_iters[i][1]\n",
    "#         cat_target = file_iters[i][2]\n",
    "#         if line is None:\n",
    "#             del file_iters[i]\n",
    "#             continue\n",
    "            \n",
    "#         data = json.loads(line)\n",
    "\n",
    "#         data['source'] = _transform_text(data['source'])\n",
    "#         if len(data['source']) == 0:\n",
    "#             print('source:', line, data)\n",
    "#             break\n",
    "# #                     continue\n",
    "\n",
    "#         data['target'] = _transform_text(data['target'])\n",
    "#         if len(data['target']) == 0:\n",
    "#             print('target:', line, data)\n",
    "#             break\n",
    "# #                     continue\n",
    "\n",
    "#         label_name_list = list(key for key in data.keys() if key[:5]=='label')\n",
    "#         if len(label_name_list) != 1:\n",
    "#             print('label_name_list:', line, data)\n",
    "#             break\n",
    "# #                     continue\n",
    "#         label_name = label_name_list[0]\n",
    "#         if data[label_name] not in label_to_id.keys():\n",
    "#             print('label_name:', line, data, label_name)\n",
    "#             break\n",
    "# #                     continue\n",
    "        \n",
    "#         label_dict = {key: -1 for key in label_type_to_id.keys()}\n",
    "#         label_dict[label_name] = label_to_id[data[label_name]]\n",
    "#         if label_dict['labelA'] == 0:\n",
    "#             label_dict['labelB'] = 0\n",
    "#         if label_dict['labelB'] == 1:\n",
    "#             label_dict['labelA'] = 1\n",
    "\n",
    "#         yield data['source'], data['target'], cat_source, cat_target, label_dict['labelA'], label_dict['labelB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f3eda7d",
   "metadata": {
    "id": "advised-breathing"
   },
   "outputs": [],
   "source": [
    "# it = get_data_iterator(data_path, train_file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ecb1fb58",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fossil-recording",
    "outputId": "0a9d7206-30af-4b7b-b33a-f62f9674c52c"
   },
   "outputs": [],
   "source": [
    "# next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c99d37f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_num(data_path, file_names):\n",
    "    count = 0\n",
    "    it = get_data_iterator(data_path, file_names)\n",
    "    for data in tqdm(it):\n",
    "        count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ee80423",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pediatric-bookmark",
    "outputId": "d0df3034-3e43-4cc6-98ac-ecb131b7dee2"
   },
   "outputs": [],
   "source": [
    "# sample_count = get_sample_num(data_path, train_file_names)\n",
    "# sample_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32253315-9c3d-4469-8eae-1c30f03ee468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_shuffle_total_file(data_path, file_names, output_file_path):\n",
    "#     data_list = []\n",
    "#     it = get_data_iterator(data_path, file_names)\n",
    "#     for source, target, cat_source, cat_target, labelA, labelB in tqdm(it):\n",
    "#         json_data = {\n",
    "#             'source':source,\n",
    "#             'target':target,\n",
    "#             'cat_source':cat_source,\n",
    "#             'cat_target':cat_target,\n",
    "#             'labelA':labelA,\n",
    "#             'labelB':labelB\n",
    "#         }\n",
    "#         data_list.append(json_data)\n",
    "#     random.shuffle(data_list)\n",
    "    \n",
    "#     with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "#         for json_data in data_list:\n",
    "#             output_file.write(f\"{json.dumps(json_data)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d76b32de-4c27-4692-a04e-95c304e34cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_shuffle_total_file(data_path, train_file_names, train_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a6b70e1-0d75-4e12-9ff1-4e453bf3bbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_iterator(data_path, file_name):\n",
    "    with open(file_name, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            json_data = json.loads(line)\n",
    "            yield json_data['source'], json_data['target'], json_data['cat_source'], json_data['cat_target'], json_data['labelA'], json_data['labelB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "afbf0f46-01c7-4291-9e9e-63556f7e6d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "it = get_data_iterator(data_path, train_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "755a0b31-cbd3-4deb-bb00-349a4f627222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('湖人120-102火箭 一觉醒来，湖人队又是一场大胜。  毫不夸张的说，这两支球队可能都是全NBA球迷最多队伍之一，两队一交手，其收视率，堪比篮圈春晚。 既然是春晚，那就不能少了节目，今儿个双方为大家伙儿准备了以下几个节目： 其一，洛杉矶小伍德，对阵休斯顿真伍德， 其二，洛杉矶詹姆斯，对阵休斯顿詹姆斯， 前三，洛杉矶塔克，对阵休斯顿塔克， 其四，理智考辛斯，对阵暴走莫里斯。  第一节末端，考神与莫里斯率先整活儿，引爆全场气氛。 话说当时詹老汉正持球背打，莫里斯使出一套坦克突进，直接将火箭球员杰肖恩-泰特老哥撵出球场。 考辛斯一看泰特倒地，大喝一声：你敢欺负我队友，这还得了？ 上前一套马保国式接化发，将莫里斯推出二米开外，并回头打算拉起倒地的泰特。 事实证明，人在生气的时候最会被激发出无限的潜能，莫里斯从被推倒地上到站起来推开考辛斯，仅仅用时不到两秒， 这么冷静就走开了！？ 这还是我们认识的考辛斯吗？？ 那速度，简直比韦德、艾弗森、TJ-福特上篮速度更快，詹老汉可能也没想到莫里斯还有这爆发力，本想拉一把，却愣是没追上。 恕技巧君直言，要是莫里斯老哥打球也能有这速度，那么顶掉小黑，出任湖人队首发控卫简直绰绰有余。  判罚结果是，双方各吃一T，大莫被驱逐。 当然，考神在场上也没待多久。 第二节刚开打，老詹突破内线，考神下手切球，一巴掌不小心切到老詹头上。 从老詹的表情大家伙儿可以看到这记如来神掌是有多痛。 尽管考神及时弥补，上前试图拉起老詹，并努力告诉裁判：“那可是我詹大爷，我怎么可能恶犯他？”，依然没逃出被驱逐的命运。  好在浓眉的发挥，又把大家伙儿的目光吸引了回来。 平日里浓眉划划水，一旦遇到自己的模板，那叫一个放开抡绝不含糊，光凭一个上半场，小伍德已经8投8中，入账21分3盖帽。 戴维斯：听说你是我的模板？来个8中8致敬一下。 另一边真伍德整个人都是崩溃的：谁特么说他是小伍德？这不是给我找麻烦么！碰别人就划水碰我就认真，这以后可咋整啊？！ 毫不夸张的说，今儿这场大胜，洛杉矶小伍德得占首功，一人直接打崩火箭队内线。  至于两个詹姆斯，反倒是手牵手好朋友，你划一桨我划一桨，洛杉矶gigi怪入账18+7+7，休斯顿大胡子拿到20+6+9，顺便带走7个失误。 泰特梦幻脚步戏耍老詹 区别在于，湖人赢球了，詹姆斯数据怎样都无所谓，火箭队输球，场边观众直呼：“哈登！醒醒！”  最后一组对位，塔克之间的对决。 火箭老塔克本以为欺负小年轻，不过是手到擒来洒洒水，谁料还是大意了。这家伙居然上场就暴走，出战短短20分钟，便8投7中有17分5篮板3助攻4抢断入账。 反观老塔克，上场30分钟，4投1中4分入账。在老塔克的衬托之下，小塔克的形象显的无比高大。  湖人队在丰田中心，带走第一场比赛的胜利，重新登顶西部第一。',\n",
       " '湖人作为上赛季的卫冕冠军以及本赛季最有希望夺冠的球队，他们的每场比赛都会被球迷关注，联盟也对湖人的比赛格外重视。可能是为了让湖人的比赛变得更加公正，执法湖人比赛的裁判也格外严厉。之前在湖人和火箭的比赛中，半场比赛没结束双方就被吹罚了五次技术犯规，本以为这已经算是告一段落了，没想到联盟还进一步发布了有关考辛斯和大莫里斯的追加处罚。  平时有关注NBA的朋友应该都清楚NBA现在的规则，一般情况下只有球员在场外发表了一些不当言论才会被罚款，一个赛季或许都不会出现一次因为犯规而被罚款的情况。然而火箭和湖人这一场比赛就诞生了两个因为犯规而被罚款的情况，合起来一共被罚款4.5万美元，这足够他们在场外被处罚两次了。可为什么大莫里斯的罚款会被考辛斯多那么多呢？一个3.5万美金，另一个只被罚了1万美金。  在当时那场比赛中，考辛斯和大莫里斯都是因为一次技术犯规以及一次恶意犯规而被罚出场外，可他们的恶意犯规也存在区别。大莫里斯当时被吹罚了一级恶意犯规，而考辛斯对詹姆斯头部击打的行为只是被吹罚了二级恶意犯规。明明大莫里斯和考辛斯的行为都差不多，而且大莫里斯只是将对手推倒，可考辛斯是直接击打对手头部，那为什么大莫里斯还会比考辛斯被罚得更多呢？  这个恶意犯规一级和二级的区别其实不能只看球员犯规的严重性，这还需要看球员犯规的意图，很明显大莫里斯就是故意将对手推倒在地，而考辛斯很可能是无意中打到詹姆斯的头部。如果考辛斯之前没有和大莫里斯发生冲突，或许这一次击打詹姆斯的头部只会被判罚一个违体犯规。当然考辛斯本来就是联盟中恶意犯规的专业户，裁判或许早就对他有意见了，吹一个二级恶意犯规也正常。  现在的各种体育联盟似乎都有点太尊重裁判了，球员们无法用自己的方式去影响比赛，大莫里斯和考辛斯这种吵架放在上世纪八十年代可能都不会被吹罚犯规，因为那时候乔丹和坏孩子军团的对碰要可怕很多。不仅是NBA存在这种情况，CBA其实也有很多这些问题。裁判可以直接影响比赛，只要裁判愿意的话，那么他就可以操控任意一场比赛的结果。  湖人和火箭很可能会因为这一次比赛而产生恩怨，一直都以总冠军为目标的火箭肯定想要击败湖人，只要击败了湖人，那火箭就有信心和机会冲击总冠军了。不过考辛斯和大莫里斯只是队内的角色球员，他们的矛盾很难影响到两支球队的胜负结果。当然比赛还是要存在这些火药味的，不然的话比赛就毫无趣味了。  不知道大家对于这件事情有没有什么别的看法和意见呢？欢迎在下面评论交流一下。',\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "db1dfb39-e619-4c5d-9860-5f59eb7aac66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_count = get_sample_num(data_path, train_file_name)\n",
    "# sample_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ea8ccf2",
   "metadata": {
    "id": "2zHQy8QwageB",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_sample_y(data_path, file_names):\n",
    "    labelA_list = []\n",
    "    labelB_list = []\n",
    "    it = get_data_iterator(data_path, file_names)\n",
    "    for source, target, cat_source, cat_target, labelA, labelB in tqdm(it):\n",
    "        if labelA != -1:\n",
    "          labelA_list.append(labelA)\n",
    "        if labelB != -1:\n",
    "          labelB_list.append(labelB)\n",
    "    return labelA_list, labelB_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7985a92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.unique(labelA_list), labelA_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a44e2e7f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "whljl4m-cn3l",
    "outputId": "e2ada33c-7ae4-437e-b984-b555d060d7ab"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "168714it [00:31, 5377.32it/s]\n",
      "/data1/wangchenyue/zsd/code/sohu2021/conda_env/lib/python3.7/site-packages/sklearn/utils/validation.py:72: FutureWarning: Pass classes=[0 1], y=[0 0 1 ... 1 0 1] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n",
      "/data1/wangchenyue/zsd/code/sohu2021/conda_env/lib/python3.7/site-packages/sklearn/utils/validation.py:72: FutureWarning: Pass classes=[0 1], y=[0 0 1 ... 1 0 0] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.94664122, 1.05973338]), array([0.56295201, 4.47127937]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelA_list, labelB_list = get_sample_y(data_path, train_file_name)\n",
    "labelA_class_weights = class_weight.compute_class_weight('balanced', np.unique(labelA_list), np.array(labelA_list))\n",
    "labelB_class_weights = class_weight.compute_class_weight('balanced', np.unique(labelB_list), np.array(labelB_list))\n",
    "labelA_class_weights, labelB_class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "28d13b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(bert_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "76ca366c",
   "metadata": {
    "id": "streaming-amsterdam"
   },
   "outputs": [],
   "source": [
    "def _get_indices(text, text_pair=None):\n",
    "    return tokenizer.encode_plus(text=text,\n",
    "                            text_pair=text_pair,\n",
    "                            max_length=text_max_length, \n",
    "                            add_special_tokens=True, \n",
    "                            padding='max_length', \n",
    "#                             truncation_strategy='longest_first', \n",
    "                            truncation=True,\n",
    "#                                          return_tensors='tf',\n",
    "                            return_token_type_ids=True\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f97bc79e-e158-4d05-af77-3dfa6d49573a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keras_bert_iterator_notwhile(data_path, file_names, tokenizer):\n",
    "    data_it = get_data_iterator(data_path, file_names)\n",
    "    for source, target, cat_source, cat_target, labelA, labelB in data_it:\n",
    "        data_source = _get_indices(text=source)\n",
    "        data_target = _get_indices(text=target)\n",
    "#             print(indices, type(indices), len(indices))\n",
    "        seg_source = jieba.lcut(source)\n",
    "        seg_target = jieba.lcut(target)\n",
    "        bm25 = calculate_bm25_similarity(bm25Model, seg_source, seg_target)\n",
    "        tf_cosine = calculate_tf_cosine_similarity(seg_source, seg_target)\n",
    "        tfidf_cosine = calculate_tfidf_cosine_similarity(seg_source, seg_target, bm25Model.idf)\n",
    "        id = \"\"\n",
    "        yield data_source['input_ids'], data_source['token_type_ids'], data_source['attention_mask'], \\\n",
    "              data_target['input_ids'], data_target['token_type_ids'], data_target['attention_mask'], \\\n",
    "              bm25, tf_cosine, tfidf_cosine, \\\n",
    "              cat_source, cat_target, \\\n",
    "              labelA, labelB, id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5b760494-d910-4e5d-9514-d78b70b3e7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "it = get_keras_bert_iterator_notwhile(data_path, train_file_name, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a25ff812-c029-4af5-977c-99bc170fe20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b2aa58d1-7166-457c-9951-583b720ab535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tfrecord(it, output_path):\n",
    "#     it = get_keras_bert_iterator_notwhile(data_path, train_file_name, tokenizer)\n",
    "    with tf.io.TFRecordWriter(output_path) as tfrecord_writer:\n",
    "        for ids_texta, token_type_ids_texta, attention_mask_texta, \\\n",
    "            ids_textb, token_type_ids_textb, attention_mask_textb, \\\n",
    "            bm25, tf_cosine, tfidf_cosine, \\\n",
    "            cat_texta, cat_textb, \\\n",
    "            labelA, labelB, id in tqdm(it):\n",
    "\n",
    "            \"\"\" 2. 定义features \"\"\"\n",
    "            example = tf.train.Example(\n",
    "                features = tf.train.Features(\n",
    "                    feature = {\n",
    "                        'ids_texta': tf.train.Feature(\n",
    "                            int64_list=tf.train.Int64List(value=ids_texta)),\n",
    "                        'token_type_ids_texta': tf.train.Feature(\n",
    "                            int64_list=tf.train.Int64List(value=token_type_ids_texta)),\n",
    "                        'attention_mask_texta': tf.train.Feature(\n",
    "                            int64_list=tf.train.Int64List(value=attention_mask_texta)),\n",
    "                        'ids_textb': tf.train.Feature(\n",
    "                            int64_list=tf.train.Int64List(value=ids_textb)),\n",
    "                        'token_type_ids_textb': tf.train.Feature(\n",
    "                            int64_list=tf.train.Int64List(value=token_type_ids_textb)),\n",
    "                        'attention_mask_textb': tf.train.Feature(\n",
    "                            int64_list=tf.train.Int64List(value=attention_mask_textb)),\n",
    "                        'bm25': tf.train.Feature(\n",
    "                            float_list=tf.train.FloatList(value=[bm25])),\n",
    "                        'tf_cosine': tf.train.Feature(\n",
    "                            float_list=tf.train.FloatList(value=[tf_cosine])),\n",
    "                        'tfidf_cosine': tf.train.Feature(\n",
    "                            float_list=tf.train.FloatList(value=[tfidf_cosine])),\n",
    "                        'cat_texta': tf.train.Feature(\n",
    "                            int64_list=tf.train.Int64List(value=[cat_texta])),\n",
    "                        'cat_textb': tf.train.Feature(\n",
    "                            int64_list=tf.train.Int64List(value=[cat_textb])),\n",
    "                        'labelA': tf.train.Feature(\n",
    "                            int64_list=tf.train.Int64List(value=[labelA])),\n",
    "                        'labelB': tf.train.Feature(\n",
    "                            int64_list=tf.train.Int64List(value=[labelB])),\n",
    "                        'id': tf.train.Feature(\n",
    "                            bytes_list=tf.train.BytesList(value=[id.encode('utf-8')])),\n",
    "                    }))\n",
    "\n",
    "            \"\"\" 3. 序列化,写入\"\"\"\n",
    "            serialized = example.SerializeToString()\n",
    "            tfrecord_writer.write(serialized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b886760e-fe58-4534-a596-fb16e9879507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it = get_keras_bert_iterator_notwhile(data_path, train_file_name, tokenizer)\n",
    "# to_tfrecord(it, \"data/shuffle_total_file.tfrecord\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "26dc0ad7-e9e3-44e7-bf81-eaba16252f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it = get_test_keras_bert_iterator(data_path, \"test_with_id.txt\")\n",
    "# to_tfrecord(it, \"data/test_file.tfrecord\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2a122dbd",
   "metadata": {
    "id": "e6O6gbcVr8ZC"
   },
   "outputs": [],
   "source": [
    "def parse_from_single_example(example_proto, need_id):\n",
    "    \"\"\" 从example message反序列化得到当初写入的内容 \"\"\"\n",
    "    # 描述features\n",
    "    desc = {\n",
    "        'ids_texta': tf.io.FixedLenFeature([512], dtype=tf.int64),\n",
    "        'token_type_ids_texta': tf.io.FixedLenFeature([512], dtype=tf.int64),\n",
    "        'attention_mask_texta': tf.io.FixedLenFeature([512], dtype=tf.int64),\n",
    "        'ids_textb': tf.io.FixedLenFeature([512], dtype=tf.int64),\n",
    "        'token_type_ids_textb': tf.io.FixedLenFeature([512], dtype=tf.int64),\n",
    "        'attention_mask_textb': tf.io.FixedLenFeature([512], dtype=tf.int64),\n",
    "        'bm25': tf.io.FixedLenFeature([1], dtype=tf.float32),\n",
    "        'tf_cosine': tf.io.FixedLenFeature([1], dtype=tf.float32),\n",
    "        'tfidf_cosine': tf.io.FixedLenFeature([1], dtype=tf.float32),\n",
    "        'cat_texta': tf.io.FixedLenFeature([1], dtype=tf.int64),\n",
    "        'cat_textb': tf.io.FixedLenFeature([1], dtype=tf.int64),\n",
    "        'labelA': tf.io.FixedLenFeature([1], dtype=tf.int64),\n",
    "        'labelB': tf.io.FixedLenFeature([1], dtype=tf.int64),\n",
    "        'id': tf.io.FixedLenFeature([], dtype=tf.string),\n",
    "    }\n",
    "    # 使用tf.io.parse_single_example反序列化\n",
    "    example = tf.io.parse_single_example(example_proto, desc)\n",
    "    \n",
    "    data = {\n",
    "        'ids_texta': example['ids_texta'],\n",
    "        'token_type_ids_texta': example['token_type_ids_texta'],\n",
    "        'attention_mask_texta': example['attention_mask_texta'],\n",
    "        'ids_textb': example['ids_textb'],\n",
    "        'token_type_ids_textb': example['token_type_ids_textb'],\n",
    "        'attention_mask_textb': example['attention_mask_textb'],\n",
    "        'bm25': example['bm25'],\n",
    "        'tf_cosine': example['tf_cosine'],\n",
    "        'tfidf_cosine': example['tfidf_cosine'],\n",
    "        'cat_texta': example['cat_texta'],\n",
    "        'cat_textb': example['cat_textb'],\n",
    "    }\n",
    "    label = {\n",
    "        'labelA': example['labelA'],\n",
    "        'labelB': example['labelB'],\n",
    "    }\n",
    "    if not need_id:\n",
    "        return data, label\n",
    "    return data, label, example['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4ddd2aeb-b112-4db6-b71d-8acff53ca98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.TFRecordDataset([\"data/test_file.tfrecord\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d1d67201-58c0-4a72-afb2-1532ec601d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = iter(dataset)\n",
    "first_example = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b105b63f-d39c-451a-b0ec-c3ccef2e5794",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = parse_from_single_example(first_example, need_id=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e8c3ada1-1cea-4643-8efd-187c9bf5b2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['ids_texta'].numpy(), data['ids_texta'].numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a47c8798-148b-4505-a5f1-9a1fc3efbedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(file_list, batch_size, epochs=None, need_id=False, options=None):\n",
    "    dataset = tf.data.TFRecordDataset(file_list)\n",
    "    if options:\n",
    "        dataset = dataset.with_options(options)\n",
    "    dataset = dataset.map(partial(parse_from_single_example, need_id=need_id), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    if epochs:\n",
    "        dataset = dataset.repeat(epochs)\n",
    "    else:\n",
    "        dataset = dataset.repeat()\n",
    "    return dataset.shuffle(buffer_size=batch_size, reshuffle_each_iteration=True) \\\n",
    "                  .batch(batch_size) \\\n",
    "                  .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c094a6a4-4b35-40dc-9f33-22397f3fe050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ({ids_texta: (None, 512), token_type_ids_texta: (None, 512), attention_mask_texta: (None, 512), ids_textb: (None, 512), token_type_ids_textb: (None, 512), attention_mask_textb: (None, 512), bm25: (None, 1), tf_cosine: (None, 1), tfidf_cosine: (None, 1), cat_texta: (None, 1), cat_textb: (None, 1)}, {labelA: (None, 1), labelB: (None, 1)}), types: ({ids_texta: tf.int64, token_type_ids_texta: tf.int64, attention_mask_texta: tf.int64, ids_textb: tf.int64, token_type_ids_textb: tf.int64, attention_mask_textb: tf.int64, bm25: tf.float32, tf_cosine: tf.float32, tfidf_cosine: tf.float32, cat_texta: tf.int64, cat_textb: tf.int64}, {labelA: tf.int64, labelB: tf.int64})>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = get_dataset(\"data/shuffle_total_file.tfrecord\", epochs=1, batch_size=1)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7faaec73-e9c6-4f8e-97e6-c9297581ff6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a07f7dc8-fe3a-4130-9f72-84375f452f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_length(file_list):\n",
    "    dataset = get_dataset(file_list, epochs=1, batch_size=1)\n",
    "    count = 0\n",
    "    for _ in tqdm(iter(dataset)):\n",
    "        count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8ad992d3-9bef-4bff-b3cd-f938d9d073e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "168714it [01:09, 2435.33it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "168714"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_len = get_dataset_length(\"data/shuffle_total_file.tfrecord\")\n",
    "train_dataset_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4bf15d-6426-4fac-b6f3-1e70efa61603",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654009ff-69b7-489d-83a0-ece03cc97794",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2d52b287-ac44-4e06-877d-21c325022007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_test_result(weights_path, result_file):\n",
    "    options = tf.data.Options()\n",
    "    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\n",
    "    it = get_dataset(\"data/test_file.tfrecord\", epochs=1, batch_size=1, need_id=True, options=options)\n",
    "    \n",
    "    model = get_model()\n",
    "    model.load_weights(weights_path)\n",
    "    \n",
    "    with open(result_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"id,label\\n\")\n",
    "        for data, _, id in tqdm(it):\n",
    "#             print(id.numpy(), id.numpy()[0], str(id.numpy()[0], encoding=\"utf-8\"), id.numpy()[0][-1], str(id.numpy()[0], encoding=\"utf-8\")[-1])\n",
    "            id_ = str(id.numpy()[0], encoding=\"utf-8\")\n",
    "            last_char = id_[-1]\n",
    "            predict = model.predict(data)\n",
    "            if last_char == 'a':\n",
    "                predict_cls = 1 if predict['labelA'][0][0] > 0.5 else 0\n",
    "            elif last_char == 'b':\n",
    "                predict_cls = 1 if predict['labelB'][0][0] > 0.5 else 0\n",
    "            else:\n",
    "                print(id)\n",
    "                continue\n",
    "            f.write(f\"{id_},{predict_cls}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a8656d-3386-4e43-9451-25c966f9f403",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3712a5c2-8e95-4785-9ec1-bae7de910f8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179fc94b",
   "metadata": {
    "id": "zHTMitEQr8gT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6517af8a",
   "metadata": {
    "id": "oIln0I3Aru0R"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47df24ef",
   "metadata": {
    "id": "recent-yahoo"
   },
   "source": [
    "## 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "34262f19",
   "metadata": {
    "id": "kOcnb7QxgxZX"
   },
   "outputs": [],
   "source": [
    "def variant_focal_loss(gamma=2., alpha=0.5, rescale = False):\n",
    "\n",
    "    gamma = float(gamma)\n",
    "    alpha = float(alpha)\n",
    "\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        # print(y_true)\n",
    "        \"\"\"\n",
    "        Focal loss for bianry-classification\n",
    "        FL(p_t)=-rescaled_factor*alpha_t*(1-p_t)^{gamma}log(p_t)\n",
    "        \n",
    "        Notice: \n",
    "        y_pred is probability after sigmoid\n",
    "\n",
    "        Arguments:\n",
    "            y_true {tensor} -- groud truth label, shape of [batch_size, 1]\n",
    "            y_pred {tensor} -- predicted label, shape of [batch_size, 1]\n",
    "\n",
    "        Keyword Arguments:\n",
    "            gamma {float} -- (default: {2.0})  \n",
    "            alpha {float} -- (default: {0.5})\n",
    "\n",
    "        Returns:\n",
    "            [tensor] -- loss.\n",
    "        \"\"\"\n",
    "        epsilon = 1.e-9  \n",
    "        y_true = tf.convert_to_tensor(y_true, tf.float32)\n",
    "        y_pred = tf.convert_to_tensor(y_pred, tf.float32)\n",
    "        model_out = tf.clip_by_value(y_pred, epsilon, 1.-epsilon)  # to advoid numeric underflow\n",
    "        \n",
    "        # compute cross entropy ce = ce_0 + ce_1 = - (1-y)*log(1-y_hat) - y*log(y_hat)\n",
    "        ce_0 = tf.multiply(tf.subtract(1., y_true), -tf.math.log(tf.subtract(1., model_out)))\n",
    "        ce_1 = tf.multiply(y_true, -tf.math.log(model_out))\n",
    "\n",
    "        # compute focal loss fl = fl_0 + fl_1\n",
    "        # obviously fl < ce because of the down-weighting, we can fix it by rescaling\n",
    "        # fl_0 = -(1-y_true)*(1-alpha)*((y_hat)^gamma)*log(1-y_hat) = (1-alpha)*((y_hat)^gamma)*ce_0\n",
    "        fl_0 = tf.multiply(tf.pow(model_out, gamma), ce_0)\n",
    "        fl_0 = tf.multiply(1.-alpha, fl_0)\n",
    "        # fl_1= -y_true*alpha*((1-y_hat)^gamma)*log(y_hat) = alpha*((1-y_hat)^gamma*ce_1\n",
    "        fl_1 = tf.multiply(tf.pow(tf.subtract(1., model_out), gamma), ce_1)\n",
    "        fl_1 = tf.multiply(alpha, fl_1)\n",
    "        fl = tf.add(fl_0, fl_1)\n",
    "        f1_avg = tf.reduce_mean(fl)\n",
    "        \n",
    "        if rescale:\n",
    "            # rescale f1 to keep the quantity as ce\n",
    "            ce = tf.add(ce_0, ce_1)\n",
    "            ce_avg = tf.reduce_mean(ce)\n",
    "            rescaled_factor = tf.divide(ce_avg, f1_avg + epsilon)\n",
    "            f1_avg = tf.multiply(rescaled_factor, f1_avg)\n",
    "        \n",
    "        return f1_avg\n",
    "    \n",
    "    return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3a0dc7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_loss(y_true, y_pred):\n",
    "    # y_true:真实标签0或者1；y_pred:为正类的概率\n",
    "    loss = 2 * tf.reduce_sum(y_true * y_pred) / tf.reduce_sum(y_true + y_pred) + K.epsilon()\n",
    "    return -loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "189fb6d2",
   "metadata": {
    "id": "signal-assets"
   },
   "outputs": [],
   "source": [
    "def transform_y(y_true, y_pred):\n",
    "    mask_value = tf.constant(-1)\n",
    "    mask_y_true = tf.not_equal(tf.cast(y_true, dtype=tf.int32), tf.cast(mask_value, dtype=tf.int32))\n",
    "#     print(f\"mask_y_true:{mask_y_true}\")\n",
    "#     y_true_ = tf.cond(tf.equal(y_true, mask_value), lambda: 0, lambda: y_true)\n",
    "    y_true_ = tf.cast(y_true, dtype=tf.int32) * tf.cast(mask_y_true, dtype=tf.int32)\n",
    "    y_pred_ = tf.cast(y_pred, dtype=tf.float32) * tf.cast(mask_y_true, dtype=tf.float32)\n",
    "#     print(f\"y_true_:{y_true_}, y_pred_:{y_pred_}\")\n",
    "    \n",
    "    return tf.cast(y_true_, dtype=tf.float32), tf.cast(y_pred_, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "204a37af",
   "metadata": {
    "id": "strong-assessment"
   },
   "outputs": [],
   "source": [
    "def my_binary_crossentropy(y_true, y_pred, class_weight_0, class_weight_1):\n",
    "#     print(f\"y_true: {y_true}\")\n",
    "    mask_value = tf.constant(-1)\n",
    "#     mask_y_true = tf.not_equal(tf.cast(y_true, dtype=tf.int32), tf.cast(mask_value, dtype=tf.int32))\n",
    "    \n",
    "#     mask = tf.zeros(shape=y_true.shape)\n",
    "    zero_value = tf.constant(0)\n",
    "#     print(f\"cond0: {tf.equal(y_true, mask_value)}\")\n",
    "#     print(f\"cond1: {tf.equal(y_true, zero_value)}\")\n",
    "#     weight = [tf.cond(tf.equal(x, mask_value), lambda: 0, tf.cond(tf.equal(x, zero_value), lambda: class_weights[0], lambda: class_weights[1])) for x in y_true]\n",
    "#     weight = [0 if x[0]==-1 else class_weights[x[0]] for x in y_true]\n",
    "    y_true_0 = tf.equal(tf.cast(y_true, dtype=tf.int32), tf.cast(tf.constant(0), dtype=tf.int32))\n",
    "    weight_0 = tf.cast(y_true_0, dtype=tf.float32) * tf.cast(tf.constant(class_weight_0), dtype=tf.float32)\n",
    "    y_true_1 = tf.equal(tf.cast(y_true, dtype=tf.int32), tf.cast(tf.constant(1), dtype=tf.int32))\n",
    "    weight_1 = tf.cast(y_true_1, dtype=tf.float32) * tf.cast(tf.constant(class_weight_1), dtype=tf.float32)\n",
    "    weight = weight_0 + weight_1\n",
    "#     print(f\"weight: {weight}\")\n",
    "    \n",
    "    bin_loss = binary_crossentropy(y_true, y_pred)\n",
    "#     print(f\"bin_loss: {bin_loss}\")\n",
    "#     f1_loss = f1_loss(y_true, y_pred)\n",
    "#     loss = bin_loss + f1_loss\n",
    "    \n",
    "    loss_ = tf.cast(bin_loss, dtype=tf.float32) * tf.cast(weight, dtype=tf.float32)\n",
    "#     print(f\"loss_: {loss_}\")\n",
    "    loss_abs = tf.abs(loss_)\n",
    "#     print(f\"loss_abs: {loss_abs}\")\n",
    "\n",
    "    return loss_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "00a76847-d7f0-4581-b134-ac608c69fc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_binary_crossentropy_A(y_true, y_pred):\n",
    "    return my_binary_crossentropy(y_true, y_pred, labelA_class_weights[0], labelA_class_weights[1])\n",
    "\n",
    "def my_binary_crossentropy_B(y_true, y_pred):\n",
    "    return my_binary_crossentropy(y_true, y_pred, labelB_class_weights[0], labelB_class_weights[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0c54df05",
   "metadata": {
    "id": "confused-acoustic"
   },
   "outputs": [],
   "source": [
    "def tarnsform_metrics(y_true, y_pred):\n",
    "    y_true_, y_pred_ = y_true.numpy(), y_pred.numpy()\n",
    "    for i in range(y_true_.shape[0]):\n",
    "        for j in range(y_true_.shape[1]):\n",
    "            if y_true_[i][j] == -1:\n",
    "                y_true_[i][j] = 0\n",
    "                y_pred_[i][j] = random.choice([0, 1])\n",
    "            if y_pred_[i][j] > 0.5:\n",
    "                y_pred_[i][j] = 1\n",
    "            else:\n",
    "                y_pred_[i][j] = 0\n",
    "    return y_true_, y_pred_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ab1bd6ee",
   "metadata": {
    "id": "abroad-monaco"
   },
   "outputs": [],
   "source": [
    "def my_binary_accuracy(y_true, y_pred):\n",
    "#     print(\"my_binary_accuracy\")\n",
    "#     print(f\"y_true:{y_true}, y_pred:{y_pred}\")\n",
    "    \n",
    "    y_true_, y_pred_ = tarnsform_metrics(y_true, y_pred)\n",
    "#     print(f\"y_true_:{y_true_}, y_pred_:{y_pred_}\")\n",
    "\n",
    "    accuracy = binary_accuracy(y_true_, y_pred_)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b0166607",
   "metadata": {
    "id": "radical-chuck"
   },
   "outputs": [],
   "source": [
    "def my_f1_score(y_true, y_pred):\n",
    "#     print(\"my_f1_score\")\n",
    "#     print(f\"y_true:{y_true}, y_pred:{y_pred}\")\n",
    "    \n",
    "    y_true_, y_pred_ = tarnsform_metrics(y_true, y_pred)\n",
    "#     print(f\"y_true_:{y_true_}, y_pred_:{y_pred_}\")\n",
    "\n",
    "    return f1_score(y_true_, y_pred_, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "085a062a",
   "metadata": {
    "id": "standard-tomorrow"
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "#     K.clear_session()\n",
    "\n",
    "    bert_model = TFBertModel.from_pretrained(bert_path, from_pt=True, trainable=True)\n",
    "    for l in bert_model.layers:\n",
    "        l.trainable = True\n",
    "\n",
    "    input_ids_texta = Input(shape=(None,), dtype='int32', name='input_ids_texta')\n",
    "    input_token_type_ids_texta = Input(shape=(None,), dtype='int32', name='input_token_type_ids_texta')\n",
    "    input_attention_mask_texta = Input(shape=(None,), dtype='int32', name='input_attention_mask_texta')\n",
    "    input_ids_textb = Input(shape=(None,), dtype='int32', name='input_ids_textb')\n",
    "    input_token_type_ids_textb = Input(shape=(None,), dtype='int32', name='input_token_type_ids_textb')\n",
    "    input_attention_mask_textb = Input(shape=(None,), dtype='int32', name='input_attention_mask_textb')\n",
    "    input_bm25 = Input(shape=(1), dtype='float32', name='input_bm25')\n",
    "    input_tf_cosine = Input(shape=(1), dtype='float32', name='input_tf_cosine')\n",
    "    input_tfidf_cosine = Input(shape=(1), dtype='float32', name='input_tfidf_cosine')\n",
    "    input_cat_texta = Input(shape=(1), dtype='float32', name='input_cat_texta')\n",
    "    input_cat_textb = Input(shape=(1), dtype='float32', name='input_cat_textb')\n",
    "\n",
    "    bert_output_texta = bert_model({'input_ids':input_ids_texta, 'token_type_ids':input_token_type_ids_texta, 'attention_mask':input_attention_mask_texta}, return_dict=False, training=True)\n",
    "    projection_logits_texta = bert_output_texta[0]\n",
    "    bert_cls_texta = Lambda(lambda x: x[:, 0])(projection_logits_texta) # 取出[CLS]对应的向量用来做分类\n",
    "\n",
    "    bert_output_textb = bert_model({'input_ids':input_ids_textb, 'token_type_ids':input_token_type_ids_textb, 'attention_mask':input_attention_mask_textb}, return_dict=False, training=True)\n",
    "    projection_logits_textb = bert_output_textb[0]\n",
    "    bert_cls_textb = Lambda(lambda x: x[:, 0])(projection_logits_textb) # 取出[CLS]对应的向量用来做分类\n",
    "\n",
    "    subtracted = Subtract()([bert_cls_texta, bert_cls_textb])\n",
    "    cos = Dot(axes=1, normalize=True)([bert_cls_texta, bert_cls_textb]) # dot=1按行点积，normalize=True输出余弦相似度\n",
    "\n",
    "    bert_cls = concatenate([bert_cls_texta, bert_cls_textb, subtracted, cos, input_bm25, input_tf_cosine, input_tfidf_cosine, input_cat_texta, input_cat_textb], axis=-1)\n",
    "\n",
    "    dense_A_0 = Dense(256, activation='relu')(bert_cls)\n",
    "    dropout_A_0 = Dropout(0.2)(dense_A_0)\n",
    "    dense_A_1 = Dense(32, activation='relu')(dropout_A_0)\n",
    "    dropout_A_1 = Dropout(0.2)(dense_A_1)\n",
    "    output_A = Dense(1, activation='sigmoid', name='output_A')(dropout_A_1)\n",
    "\n",
    "    dense_B_0 = Dense(256, activation='relu')(bert_cls)\n",
    "    dropout_B_0 = Dropout(0.2)(dense_B_0)\n",
    "    dense_B_1 = Dense(32, activation='relu')(dropout_B_0)\n",
    "    dropout_B_1 = Dropout(0.2)(dense_B_1)\n",
    "    output_B = Dense(1, activation='sigmoid', name='output_B')(dropout_B_1)\n",
    "\n",
    "    input_data = {\n",
    "        'ids_texta':input_ids_texta,\n",
    "        'token_type_ids_texta':input_token_type_ids_texta,\n",
    "        'attention_mask_texta':input_attention_mask_texta,\n",
    "        'ids_textb':input_ids_textb,\n",
    "        'token_type_ids_textb':input_token_type_ids_textb,\n",
    "        'attention_mask_textb':input_attention_mask_textb,\n",
    "        'bm25':input_bm25,\n",
    "        'tf_cosine':input_tf_cosine,\n",
    "        'tfidf_cosine':input_tfidf_cosine,\n",
    "        'cat_texta':input_cat_texta,\n",
    "        'cat_textb':input_cat_textb,\n",
    "    }\n",
    "    output_data = {\n",
    "        'labelA':output_A,\n",
    "        'labelB':output_B,\n",
    "    }\n",
    "    model = Model(input_data, output_data)\n",
    "    model.compile(\n",
    "#                   loss=my_binary_crossentropy,\n",
    "#                   loss={\n",
    "#                       'output_A':my_binary_crossentropy_A,\n",
    "#                       'output_B':my_binary_crossentropy_B,\n",
    "#                   },\n",
    "                  loss={\n",
    "                      'labelA':my_binary_crossentropy_A,\n",
    "                      'labelB':my_binary_crossentropy_B,\n",
    "                  },\n",
    "#                   loss='binary_crossentropy',\n",
    "#                   loss=binary_crossentropy,\n",
    "                  optimizer=Adam(1e-5),    #用足够小的学习率\n",
    "#                   metrics=[my_binary_accuracy, my_f1_score]\n",
    "                  metrics='accuracy'\n",
    "                 )\n",
    "#     print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9167033b",
   "metadata": {
    "id": "foreign-tribe"
   },
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='loss', patience=3)   #早停法，防止过拟合\n",
    "plateau = ReduceLROnPlateau(monitor=\"loss\", verbose=1, factor=0.5, patience=2) #当评价指标不在提升时，减少学习率\n",
    "checkpoint = ModelCheckpoint(check_point_path, monitor='loss', verbose=2, save_best_only=True, save_weights_only=True) #保存最好的模型\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=f\"./logs/{datetime.now().strftime('%Y%m%d-%H%M%S')}\", update_freq=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124f8d5b",
   "metadata": {
    "id": "rotary-journey"
   },
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6ccc1a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_step(sample_count, batch_size):\n",
    "    step = sample_count // batch_size\n",
    "    if sample_count % batch_size != 0:\n",
    "        step += 1\n",
    "    return step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "06687d3c-f30f-42f9-b36e-6b731d31f719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = get_model()\n",
    "# plot_model(model, \"keras_bert_transformers_two_text_input_SubStract_bm25cosine_1.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8b445980-5d25-4e9e-83bf-0267ea355fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights(check_point_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5ec8fc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 2\n",
    "# epochs = 10\n",
    "\n",
    "# train_dataset_iterator = batch_iter(data_path, train_file_name, tokenizer, batch_size)\n",
    "# train_step = get_step(sample_count, batch_size)\n",
    "\n",
    "# model.fit(\n",
    "#     train_dataset_iterator,\n",
    "#     # steps_per_epoch=10,\n",
    "#     steps_per_epoch=train_step,\n",
    "#     epochs=epochs,\n",
    "# #       validation_data=dev_dataset_iterator,\n",
    "#   # validation_steps=2,\n",
    "# #       validation_steps=dev_step,\n",
    "# #     validation_split=0.2,\n",
    "# #     class_weight={\n",
    "# #         'output_A':labelA_class_weights,\n",
    "# #         'output_B':labelB_class_weights,\n",
    "# #     },\n",
    "#     callbacks=[early_stopping, plateau, checkpoint, tensorboard_callback],\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# model.save_weights(weights_path)\n",
    "# # model_json = model.to_json()\n",
    "# # with open(config_path, 'w', encoding='utf-8') as file:\n",
    "# #     file.write(model_json)\n",
    "\n",
    "# save_test_result(model, result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "acc707ba",
   "metadata": {
    "id": "hiM3cmiokFMQ"
   },
   "outputs": [],
   "source": [
    "# model = get_model()\n",
    "# # with open(config_path, 'r', encoding='utf-8') as json_file:\n",
    "# #     loaded_model_json = json_file.read()\n",
    "# # model = model_from_json(loaded_model_json)\n",
    "# model.load_weights(check_point_path)\n",
    "# save_test_result(model, \"trained_model_substract_1/multi_keras_bert_sohu_test_result_epoch6.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4a2dfd6d",
   "metadata": {
    "id": "virgin-quarterly"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'bert.embeddings.position_ids', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_attention_mask_texta (Inp [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_ids_texta (InputLayer)    [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_token_type_ids_texta (Inp [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_attention_mask_textb (Inp [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_ids_textb (InputLayer)    [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_token_type_ids_textb (Inp [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     TFBaseModelOutputWit 102267648   input_attention_mask_texta[0][0] \n",
      "                                                                 input_ids_texta[0][0]            \n",
      "                                                                 input_token_type_ids_texta[0][0] \n",
      "                                                                 input_attention_mask_textb[0][0] \n",
      "                                                                 input_ids_textb[0][0]            \n",
      "                                                                 input_token_type_ids_textb[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 768)          0           tf_bert_model[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 768)          0           tf_bert_model[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "subtract (Subtract)             (None, 768)          0           lambda[0][0]                     \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot (Dot)                       (None, 1)            0           lambda[0][0]                     \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_bm25 (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_tf_cosine (InputLayer)    [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_tfidf_cosine (InputLayer) [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_cat_texta (InputLayer)    [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_cat_textb (InputLayer)    [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 2310)         0           lambda[0][0]                     \n",
      "                                                                 lambda_1[0][0]                   \n",
      "                                                                 subtract[0][0]                   \n",
      "                                                                 dot[0][0]                        \n",
      "                                                                 input_bm25[0][0]                 \n",
      "                                                                 input_tf_cosine[0][0]            \n",
      "                                                                 input_tfidf_cosine[0][0]         \n",
      "                                                                 input_cat_texta[0][0]            \n",
      "                                                                 input_cat_textb[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          591616      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          591616      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 256)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)            (None, 256)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 32)           8224        dropout_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 32)           8224        dropout_39[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 32)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)            (None, 32)           0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output_A (Dense)                (None, 1)            33          dropout_38[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "output_B (Dense)                (None, 1)            33          dropout_40[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 103,467,394\n",
      "Trainable params: 103,467,394\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "   20/42179 [..............................] - ETA: 8:16:49 - loss: 0.6808 - output_A_loss: 0.2094 - output_B_loss: 0.4714 - output_A_accuracy: 0.5358 - output_B_accuracy: 0.7528"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-ad5873d7116c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplateau\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensorboard_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     )\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/zsd/code/sohu2021/conda_env/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/zsd/code/sohu2021/conda_env/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/zsd/code/sohu2021/conda_env/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/zsd/code/sohu2021/conda_env/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/zsd/code/sohu2021/conda_env/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/zsd/code/sohu2021/conda_env/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/zsd/code/sohu2021/conda_env/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 2 * strategy.num_replicas_in_sync\n",
    "n_epochs = 10\n",
    "start_epoch = 1\n",
    "\n",
    "options = tf.data.Options()\n",
    "options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\n",
    "train_dataset_iterator = get_dataset(\"data/shuffle_total_file.tfrecord\", batch_size=batch_size, options=options)\n",
    "train_step = get_step(train_dataset_len, batch_size)\n",
    "\n",
    "with strategy.scope():\n",
    "    model = get_model()\n",
    "    print(model.summary())\n",
    "    plot_model(model, \"keras_bert_transformers_two_text_input_SubStract_bm25cosine_1.png\", show_shapes=True)\n",
    "    \n",
    "    model.load_weights(check_point_path)\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch + n_epochs):\n",
    "    model.fit(\n",
    "        train_dataset_iterator,\n",
    "        steps_per_epoch=train_step,\n",
    "        epochs=1,\n",
    "        callbacks=[early_stopping, plateau, checkpoint, tensorboard_callback],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    model.save_weights(weights_path)\n",
    "\n",
    "    save_test_result(weights_path, f\"{result_path}.epoch_{epoch}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6c7081-6116-4123-abc2-7a7f3a94fc56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6287ab7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f4d343",
   "metadata": {
    "id": "tribal-banking"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b9d489",
   "metadata": {
    "id": "likely-celebration"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06708aa7",
   "metadata": {
    "id": "latest-biology"
   },
   "source": [
    "# 模型加载及测试"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7177b6",
   "metadata": {
    "id": "reserved-nightmare"
   },
   "source": [
    "## load_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6520f679",
   "metadata": {
    "id": "sufficient-shadow"
   },
   "source": [
    "## load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00163e3",
   "metadata": {
    "id": "white-adventure"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e635340d",
   "metadata": {
    "id": "structural-shark"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e88531",
   "metadata": {
    "id": "o7jaIjHXf1Pk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ffc019",
   "metadata": {
    "id": "9jQK0PLYf1TC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd15ed44",
   "metadata": {
    "id": "Me0AC8uru-dN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f8e7aa",
   "metadata": {
    "id": "CJbObtBvxoH2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "reserved-nightmare"
   ],
   "name": "keras_bert_transformers_two_text_input.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
