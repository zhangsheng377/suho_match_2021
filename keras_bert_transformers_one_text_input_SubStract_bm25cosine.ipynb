{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f027699",
   "metadata": {
    "id": "precious-column"
   },
   "source": [
    "# 导包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f2d7421",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "ivu9Z3WNt4BV",
    "outputId": "a421751c-af12-46f9-a9bd-076e0ee60ef3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.7.10 | packaged by conda-forge | (default, Feb 19 2021, 16:07:37) \\n[GCC 9.3.0]'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys \n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d68024fb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VNZFCpWIfJ9t",
    "outputId": "404b5dfd-71db-4323-e5a7-e00bf07d5bf9"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6337d460",
   "metadata": {
    "id": "F65Y9KEAiZzD"
   },
   "outputs": [],
   "source": [
    "# !unzip sohu2021_open_data_clean.zip\n",
    "# !unzip chinese_L-12_H-768_A-12.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7022a024",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZBvllWcDWLZN",
    "outputId": "70daddf4-29ef-48f1-9612-96f639a4c615"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e863158",
   "metadata": {
    "id": "subject-motorcycle"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import sys\n",
    "import re\n",
    "from collections import Counter\n",
    "import random\n",
    "import json\n",
    "from joblib import dump, load\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "# import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "tf.config.run_functions_eagerly(True)\n",
    "tf.get_logger().setLevel(tf.compat.v1.logging.ERROR)\n",
    "from keras.metrics import top_k_categorical_accuracy, binary_accuracy\n",
    "from keras.layers import *\n",
    "from keras.callbacks import *\n",
    "from keras.models import Model, load_model, model_from_json\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from keras.losses import SparseCategoricalCrossentropy, binary_crossentropy\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    TFBertForPreTraining,\n",
    "    TFBertModel,\n",
    ")\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.utils import class_weight\n",
    "import torch\n",
    "from pyhanlp import *\n",
    "import jieba\n",
    "\n",
    "from my_utils import calculate_bm25_similarity, calculate_tf_cosine_similarity, calculate_tfidf_cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c2c73e3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "patent-winner",
    "outputId": "79dceb89-d54d-4a37-dc00-ade7e64c3987"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91c2ba6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 8754601229644329807,\n",
       " name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 10770692224\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 13921717382704766844\n",
       " physical_device_desc: \"device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:04:00.0, compute capability: 6.1\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05ec181",
   "metadata": {
    "id": "rolled-process"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d95533e-a64d-46b7-8977-0fe60e382357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff938c13-f794-4d0d-bc37-579e8dbcce70",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_path = r\"chinese_L-12_H-768_A-12\"\n",
    "\n",
    "oriSource_count = 5574\n",
    "category_count = 30\n",
    "sorted_label_list = range(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "351c0fa6-3421-45be-91ee-a5c405e01d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    K.clear_session()\n",
    "    \n",
    "    bert_model = TFBertModel.from_pretrained(bert_path, from_pt=True, trainable=True)\n",
    "    for l in bert_model.layers:\n",
    "        l.trainable = True\n",
    "    \n",
    "    input_bert_indices = Input(shape=(None,), dtype=tf.int32)\n",
    "    input_bert_segments = Input(shape=(None,), dtype=tf.int32)\n",
    "    bert_output = bert_model([input_bert_indices, input_bert_segments])[0]\n",
    "    bert_cls = Lambda(lambda x: x[:, 0])(bert_output)\n",
    "    \n",
    "    input_num_feature = Input(shape=(5,))\n",
    "    \n",
    "    input_oriSource_index = Input(shape=(1,))\n",
    "    oriSource_embedding = Embedding(oriSource_count, 256)(input_oriSource_index)\n",
    "    oriSource_emb = Flatten()(oriSource_embedding)\n",
    "    print(oriSource_emb)\n",
    "    \n",
    "    input_category_index = Input(shape=(1,))\n",
    "    category_embedding = Embedding(category_count, 32)(input_category_index)\n",
    "    category_emb = Flatten()(category_embedding)\n",
    "    print(category_emb)\n",
    "    \n",
    "    emb = concatenate([bert_cls, input_num_feature, oriSource_emb, category_emb])\n",
    "    print(emb)\n",
    "    \n",
    "    d0 = Dense(1024, activation='relu')(emb)\n",
    "    d0_ = Dropout(0.2)(d0)\n",
    "    \n",
    "    d1 = Dense(256, activation='relu')(d0_)\n",
    "    d1_ = Dropout(0.2)(d1)\n",
    "    \n",
    "    output = Dense(len(sorted_label_list), activation='softmax')(d1_)\n",
    "    \n",
    "    model = Model([input_bert_indices, input_bert_segments, input_num_feature, input_oriSource_index, input_category_index], output)\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer=Adam(1e-5),\n",
    "                  metrics=['accuracy']\n",
    "                 )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbb6d978-98d7-497c-bb7d-a1b6bd2d46c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'bert.embeddings.position_ids', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 256), dtype=tf.float32, name=None), name='flatten/Reshape:0', description=\"created by layer 'flatten'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 32), dtype=tf.float32, name=None), name='flatten_1/Reshape:0', description=\"created by layer 'flatten_1'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1061), dtype=tf.float32, name=None), name='concatenate/concat:0', description=\"created by layer 'concatenate'\")\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     TFBaseModelOutputWit 102267648   input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 1, 256)       1426944     input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 32)        960         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 768)          0           tf_bert_model[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 5)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 256)          0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 32)           0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1061)         0           lambda[0][0]                     \n",
      "                                                                 input_3[0][0]                    \n",
      "                                                                 flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1024)         1087488     concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 1024)         0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          262400      dropout_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 30)           7710        dropout_38[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 105,053,150\n",
      "Trainable params: 105,053,150\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc48a06-dae6-4dc6-bf45-15cf4d56610f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd6127a-af0f-41c2-aa8a-43f8e9812d71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ce93d2-8727-4cda-a124-0ae6327c9254",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fd0c84-0fa6-4f1d-94a5-d012d90dcd99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1068874-ca22-43b6-9902-cceeb87bf72e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ff04d4-3e34-4661-8841-f7677c8760cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c517425",
   "metadata": {
    "id": "cubic-statistics"
   },
   "outputs": [],
   "source": [
    "data_path = \"sohu2021_open_data_clean/\"\n",
    "text_max_length = 512\n",
    "bert_path = r\"chinese_L-12_H-768_A-12\"\n",
    "\n",
    "check_point_path = 'trained_model_substract_1/multi_keras_bert_sohu.weights'\n",
    "weights_path = \"trained_model_substract_1/multi_keras_bert_sohu_final.weights\"\n",
    "config_path = \"trained_model_substract_1/multi_keras_bert_sohu_final.model_config.json\"\n",
    "result_path = \"trained_model_substract_1/multi_keras_bert_sohu_test_result_final.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "166dec1b",
   "metadata": {
    "id": "collect-portal"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.summarization.bm25.BM25 at 0x7f5428c18050>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25Model = load(\"bm25.bin\")\n",
    "bm25Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9f94c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba1511a",
   "metadata": {
    "id": "norman-track"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dd18e8",
   "metadata": {
    "id": "cordless-pillow"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1626dd7",
   "metadata": {
    "id": "absolute-perception"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b7481ea",
   "metadata": {
    "id": "forced-heritage"
   },
   "outputs": [],
   "source": [
    "# 转换bert模型，到pytorch的pd格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6631ee6",
   "metadata": {
    "id": "necessary-photograph"
   },
   "outputs": [],
   "source": [
    "# !transformers-cli convert --model_type bert \\\n",
    "#   --tf_checkpoint chinese_L-12_H-768_A-12/bert_model.ckpt \\\n",
    "#   --config chinese_L-12_H-768_A-12/bert_config.json \\\n",
    "#   --pytorch_dump_output chinese_L-12_H-768_A-12/pytorch_model.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b10230",
   "metadata": {
    "id": "brave-purse"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a2ab09",
   "metadata": {
    "id": "polish-substance"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb58565",
   "metadata": {
    "id": "institutional-strike"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6117413",
   "metadata": {
    "id": "skilled-possibility"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0786ea75",
   "metadata": {
    "id": "voluntary-pittsburgh"
   },
   "source": [
    "# 多任务分支模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d971136",
   "metadata": {
    "id": "protective-script"
   },
   "source": [
    "## 构建数据迭代器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88eff7b1",
   "metadata": {
    "id": "minimal-sierra"
   },
   "outputs": [],
   "source": [
    "label_type_to_id = {'labelA':0, 'labelB':1}\n",
    "label_to_id = {'0':0, '1':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1adda0c",
   "metadata": {
    "id": "hungry-principal"
   },
   "outputs": [],
   "source": [
    "def get_text_iterator(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            yield line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1144b266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _transform_text(text):\n",
    "   text = text.strip().replace('\\n', '。').replace('\\t', '').replace('\\u3000', '')\n",
    "   return re.sub(r'。+', '。', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ad80235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary(text, senc_num=20):\n",
    "    a = HanLP.extractSummary(text, 20)\n",
    "    a_ = str(a)\n",
    "    return a_[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36f1931e",
   "metadata": {
    "id": "pregnant-force"
   },
   "outputs": [],
   "source": [
    "def get_data_iterator(data_path, file_names):\n",
    "    # TODO: 随机取\n",
    "    file_iters = []\n",
    "    for file_name in file_names:\n",
    "      for category in os.listdir(data_path):\n",
    "          category_path = os.path.join(data_path, category)\n",
    "          if not os.path.isdir(category_path):\n",
    "              continue\n",
    "              \n",
    "          file_path = os.path.join(category_path, file_name)\n",
    "          if not os.path.isfile(file_path):\n",
    "              continue\n",
    "              \n",
    "          \n",
    "          file_iter = get_text_iterator(file_path)\n",
    "          cat_source = 0\n",
    "          if category[0] == '长':\n",
    "            cat_source = 1\n",
    "          cat_target = 0\n",
    "          if category[1] == '长':\n",
    "            cat_target = 1\n",
    "          file_iters.append((file_iter, cat_source, cat_target))\n",
    "        \n",
    "    while len(file_iters) > 0:\n",
    "        i = random.randrange(len(file_iters))\n",
    "        line = next(file_iters[i][0], None)\n",
    "        cat_source = file_iters[i][1]\n",
    "        cat_target = file_iters[i][2]\n",
    "        if line is None:\n",
    "            del file_iters[i]\n",
    "            continue\n",
    "            \n",
    "        data = json.loads(line)\n",
    "\n",
    "        data['source'] = _transform_text(data['source'])\n",
    "        if len(data['source']) == 0:\n",
    "            print('source:', line, data)\n",
    "            break\n",
    "#                     continue\n",
    "\n",
    "        data['target'] = _transform_text(data['target'])\n",
    "        if len(data['target']) == 0:\n",
    "            print('target:', line, data)\n",
    "            break\n",
    "#                     continue\n",
    "\n",
    "        label_name_list = list(key for key in data.keys() if key[:5]=='label')\n",
    "        if len(label_name_list) != 1:\n",
    "            print('label_name_list:', line, data)\n",
    "            break\n",
    "#                     continue\n",
    "        label_name = label_name_list[0]\n",
    "        if data[label_name] not in label_to_id.keys():\n",
    "            print('label_name:', line, data, label_name)\n",
    "            break\n",
    "#                     continue\n",
    "        \n",
    "        label_dict = {key: -1 for key in label_type_to_id.keys()}\n",
    "        label_dict[label_name] = label_to_id[data[label_name]]\n",
    "        if label_dict['labelA'] == 0:\n",
    "            label_dict['labelB'] = 0\n",
    "        if label_dict['labelB'] == 1:\n",
    "            label_dict['labelA'] = 1\n",
    "\n",
    "        yield data['source'], data['target'], cat_source, cat_target, label_dict['labelA'], label_dict['labelB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f3eda7d",
   "metadata": {
    "id": "advised-breathing"
   },
   "outputs": [],
   "source": [
    "it = get_data_iterator(data_path, [\"train.txt\", \"valid.txt\", \"round2.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ecb1fb58",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fossil-recording",
    "outputId": "0a9d7206-30af-4b7b-b33a-f62f9674c52c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('北京：储存温度0℃以下的进口冷链食品全追溯',\n",
       " '新京报快讯（记者陈琳）“北京冷链”平台自去年11月1日正式上线运行，进口冷藏冷冻肉类、水产品管理全面纳入电子追溯管理。2月1日，记者从北京市市场监管局了解到，截至去年12月31日，“北京冷链”累计注册完成企业10522家，累计流通产品24.43万吨，平均每日新增记录产品2000余吨。 自2月12日起，“北京冷链”追溯食品品种范围将扩大到全部储存温度在0℃以下（含0℃）的进口冷链食品，包括冷冻粮食制品（如速冻米面制品）、冷冻乳制品（如黄油、奶酪）、冷冻蔬菜、冷冻水果、冷冻饮品（如冰激凌、冷冻果汁）等进口冷链食品品种。 按照相关要求，北京市从事进口冷链食品进口贸易、生产加工、流通销售、物流仓储、餐饮服务等相关单位或个人，应按照《中华人民共和国食品安全法》《中华人民共和国传染病防治法》的规定，严格落实食品追溯和疫情防控主体责任，保证所经营的进口冷链食品全部实现追溯，严格遵守“四无五不”管理要求，即对无检验检疫证明、无核酸检测报告、无消毒证明、无追溯信息的进口冷链食品，做到不采购、不经营、不使用、不运输、不存储。 据北京市市场监管局统计，截至去年12月31日，“北京冷链”累计注册完成企业10522家，其中经营进口冷链食品（以进口冷藏冷冻肉类、水产品为主）的企业4521家。累计记录产品品种23978个，商品批次42103个，涉及93个国家和地区、国内30个省份，累计流通产品24.43万吨，平均每日新增记录产品2000余吨。 新京报记者陈琳 编辑白爽校对刘军',\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " -1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c99d37f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_num(data_path, file_names):\n",
    "    count = 0\n",
    "    it = get_data_iterator(data_path, file_names)\n",
    "    for data in tqdm(it):\n",
    "        count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ee80423",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pediatric-bookmark",
    "outputId": "d0df3034-3e43-4cc6-98ac-ecb131b7dee2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "119168it [00:05, 21173.62it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "119168"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_count = get_sample_num(data_path, [\"train.txt\", \"valid.txt\", \"round2.txt\"])\n",
    "sample_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ea8ccf2",
   "metadata": {
    "id": "2zHQy8QwageB",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_sample_y(data_path, file_names):\n",
    "    labelA_list = []\n",
    "    labelB_list = []\n",
    "    it = get_data_iterator(data_path, file_names)\n",
    "    for source, target, cat_source, cat_target, labelA, labelB in tqdm(it):\n",
    "        if labelA != -1:\n",
    "          labelA_list.append(labelA)\n",
    "        if labelB != -1:\n",
    "          labelB_list.append(labelB)\n",
    "    return labelA_list, labelB_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7985a92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.unique(labelA_list), labelA_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a44e2e7f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "whljl4m-cn3l",
    "outputId": "e2ada33c-7ae4-437e-b984-b555d060d7ab"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "69578it [00:03, 22005.64it/s]\n",
      "/data1/wangchenyue/zsd/code/sohu2021/conda_env/lib/python3.7/site-packages/sklearn/utils/validation.py:72: FutureWarning: Pass classes=[0 1], y=[1 0 0 ... 0 1 0] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n",
      "/data1/wangchenyue/zsd/code/sohu2021/conda_env/lib/python3.7/site-packages/sklearn/utils/validation.py:72: FutureWarning: Pass classes=[0 1], y=[0 0 0 ... 0 0 0] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.9232596 , 1.09065406]), array([0.55924623, 4.71967812]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelA_list, labelB_list = get_sample_y(data_path, [\"train.txt\", \"valid.txt\"])\n",
    "labelA_class_weights = class_weight.compute_class_weight('balanced', np.unique(labelA_list), np.array(labelA_list))\n",
    "labelB_class_weights = class_weight.compute_class_weight('balanced', np.unique(labelB_list), np.array(labelB_list))\n",
    "labelA_class_weights, labelB_class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28d13b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(bert_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76ca366c",
   "metadata": {
    "id": "streaming-amsterdam"
   },
   "outputs": [],
   "source": [
    "def _get_indices(text, text_pair=None):\n",
    "    return tokenizer.encode_plus(text=text,\n",
    "                            text_pair=text_pair,\n",
    "                            max_length=text_max_length, \n",
    "                            add_special_tokens=True, \n",
    "                            padding='max_length', \n",
    "#                             truncation_strategy='longest_first', \n",
    "                            truncation=True,\n",
    "#                                          return_tensors='tf',\n",
    "                            return_token_type_ids=True\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "82d9886e",
   "metadata": {
    "id": "paperback-dynamics"
   },
   "outputs": [],
   "source": [
    "def get_keras_bert_iterator(data_path, file_names, tokenizer):\n",
    "    while True:\n",
    "        data_it = get_data_iterator(data_path, file_names)\n",
    "        for source, target, cat_source, cat_target, labelA, labelB in data_it:\n",
    "            data_source = _get_indices(text=source)\n",
    "            data_target = _get_indices(text=target)\n",
    "#             print(indices, type(indices), len(indices))\n",
    "            seg_source = jieba.lcut(source)\n",
    "            seg_target = jieba.lcut(target)\n",
    "            bm25 = calculate_bm25_similarity(bm25Model, seg_source, seg_target)\n",
    "            tf_cosine = calculate_tf_cosine_similarity(seg_source, seg_target)\n",
    "            tfidf_cosine = calculate_tfidf_cosine_similarity(seg_source, seg_target, bm25Model.idf)\n",
    "            yield data_source['input_ids'], data_source['token_type_ids'], data_source['attention_mask'], \\\n",
    "                  data_target['input_ids'], data_target['token_type_ids'], data_target['attention_mask'], \\\n",
    "                  bm25, tf_cosine, tfidf_cosine, \\\n",
    "                  cat_source, cat_target, \\\n",
    "                  labelA, labelB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "59d2eaf5",
   "metadata": {
    "id": "chinese-persian"
   },
   "outputs": [],
   "source": [
    "it = get_keras_bert_iterator(data_path, [\"train.txt\", \"valid.txt\", \"round2.txt\"], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c52c06ac",
   "metadata": {
    "id": "complete-explanation"
   },
   "outputs": [],
   "source": [
    "# next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "09459e1d",
   "metadata": {
    "id": "suburban-priority"
   },
   "outputs": [],
   "source": [
    "def batch_iter(data_path, file_names, tokenizer, batch_size=64, shuffle=True):\n",
    "    \"\"\"生成批次数据\"\"\"\n",
    "    keras_bert_iter = get_keras_bert_iterator(data_path, file_names, tokenizer)\n",
    "    while True:\n",
    "        data_list = []\n",
    "        for _ in range(batch_size):\n",
    "            data = next(keras_bert_iter)\n",
    "#             print(data)\n",
    "            data_list.append(data)\n",
    "        if shuffle:\n",
    "            random.shuffle(data_list)\n",
    "#         print(data_list)\n",
    "        \n",
    "        input_ids_texta_list = []\n",
    "        token_type_ids_texta_list = []\n",
    "        attention_mask_texta_list = []\n",
    "        input_ids_textb_list = []\n",
    "        token_type_ids_textb_list = []\n",
    "        attention_mask_textb_list = []\n",
    "        bm25_list = []\n",
    "        tf_cosine_list = []\n",
    "        tfidf_cosine_list = []\n",
    "        cat_texta_list = []\n",
    "        cat_textb_list = []\n",
    "        labelA_list = []\n",
    "        labelB_list = []\n",
    "        for data in data_list:\n",
    "            input_ids_texta, token_type_ids_texta, attention_mask_texta, \\\n",
    "            input_ids_textb, token_type_ids_textb, attention_mask_textb, \\\n",
    "            bm25, tf_cosine, tfidf_cosine, \\\n",
    "            cat_texta, cat_textb, \\\n",
    "            labelA, labelB = data\n",
    "#             print(indices, type(indices))\n",
    "            input_ids_texta_list.append(input_ids_texta)\n",
    "            token_type_ids_texta_list.append(token_type_ids_texta)\n",
    "            attention_mask_texta_list.append(attention_mask_texta)\n",
    "            input_ids_textb_list.append(input_ids_textb)\n",
    "            token_type_ids_textb_list.append(token_type_ids_textb)\n",
    "            attention_mask_textb_list.append(attention_mask_textb)\n",
    "            bm25_list.append(bm25)\n",
    "            tf_cosine_list.append(tf_cosine)\n",
    "            tfidf_cosine_list.append(tfidf_cosine)\n",
    "            cat_texta_list.append(cat_texta)\n",
    "            cat_textb_list.append(cat_textb)\n",
    "            labelA_list.append(labelA)\n",
    "            labelB_list.append(labelB)\n",
    "\n",
    "        yield [np.array(input_ids_texta_list), np.array(token_type_ids_texta_list), np.array(attention_mask_texta_list), \n",
    "               np.array(input_ids_textb_list), np.array(token_type_ids_textb_list), np.array(attention_mask_textb_list), \n",
    "               np.array(bm25_list), np.array(tf_cosine_list), np.array(tfidf_cosine_list),\n",
    "               np.array(cat_texta_list), np.array(cat_textb_list)], \\\n",
    "            [np.array(labelA_list, dtype=np.int32), np.array(labelB_list, dtype=np.int32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e3ae9ba2",
   "metadata": {
    "id": "artificial-truck"
   },
   "outputs": [],
   "source": [
    "it = batch_iter(data_path, [\"train.txt\", \"valid.txt\", \"round2.txt\"], tokenizer, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "07384be8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "broken-guinea",
    "outputId": "d308a85d-fe68-4f14-dc2a-138e1e387f22"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.630 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([array([[ 101, 3341, 3975, ..., 2110, 1355,  102],\n",
       "         [ 101, 2207, 5686, ...,    0,    0,    0]]),\n",
       "  array([[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]]),\n",
       "  array([[1, 1, 1, ..., 1, 1, 1],\n",
       "         [1, 1, 1, ..., 0, 0, 0]]),\n",
       "  array([[ 101,  704, 1744, ..., 5905, 5344,  102],\n",
       "         [ 101, 7676, 3949, ...,    0,    0,    0]]),\n",
       "  array([[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]]),\n",
       "  array([[1, 1, 1, ..., 1, 1, 1],\n",
       "         [1, 1, 1, ..., 0, 0, 0]]),\n",
       "  array([0.14489078, 0.09745711]),\n",
       "  array([0.59073833, 0.04029115]),\n",
       "  array([0.1103738 , 0.04669186]),\n",
       "  array([1, 0]),\n",
       "  array([1, 0])],\n",
       " [array([-1,  0], dtype=int32), array([0, 0], dtype=int32)])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542787d0",
   "metadata": {
    "id": "ZcPnAGELrtyP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e00670",
   "metadata": {
    "id": "oOc8KEL8rt7q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2668e453",
   "metadata": {
    "id": "o1Yyn--vrt_z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a1adb4",
   "metadata": {
    "id": "ZFA6-3RYruEk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "03a8f3d2",
   "metadata": {
    "id": "qhRn-MB4ruH6"
   },
   "outputs": [],
   "source": [
    "def get_test_data_iterator(data_path, file_name):\n",
    "  # print(data_path)\n",
    "  for category in os.listdir(data_path):\n",
    "    category_path = os.path.join(data_path, category)\n",
    "    # print(category_path)\n",
    "    if not os.path.isdir(category_path):\n",
    "      # print(f\"{category_path} not dir\")\n",
    "      continue\n",
    "        \n",
    "    file_path = os.path.join(category_path, file_name)\n",
    "    # print(file_path)\n",
    "    if not os.path.isfile(file_path):\n",
    "      # print(f\"{file_path} not file\")\n",
    "      continue\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "      for line in f:\n",
    "        # print(line)\n",
    "        data = json.loads(line)\n",
    "        \n",
    "        data['source'] = _transform_text(data['source'])\n",
    "        if len(data['source']) == 0:\n",
    "          print('source:', line, data)\n",
    "          break\n",
    "            \n",
    "        data['target'] = _transform_text(data['target'])\n",
    "        if len(data['target']) == 0:\n",
    "          print('target:', line, data)\n",
    "          break\n",
    "\n",
    "        cat_source = 0\n",
    "        if category[0] == '长':\n",
    "          cat_source = 1\n",
    "        cat_target = 0\n",
    "        if category[1] == '长':\n",
    "          cat_target = 1\n",
    "            \n",
    "        yield data['source'], data['target'], cat_source, cat_target, data['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1c91a4c1",
   "metadata": {
    "id": "8jsAuwC8ruSJ"
   },
   "outputs": [],
   "source": [
    "def get_test_keras_bert_iterator(data_path, file_name):\n",
    "  it = get_test_data_iterator(data_path, file_name)\n",
    "  for source, target, cat_source, cat_target, id in it:\n",
    "    data_source = _get_indices(text=source)\n",
    "    data_target = _get_indices(text=target)\n",
    "    \n",
    "    seg_source = jieba.lcut(source)\n",
    "    seg_target = jieba.lcut(target)\n",
    "    bm25 = calculate_bm25_similarity(bm25Model, seg_source, seg_target)\n",
    "    tf_cosine = calculate_tf_cosine_similarity(seg_source, seg_target)\n",
    "    tfidf_cosine = calculate_tfidf_cosine_similarity(seg_source, seg_target, bm25Model.idf)\n",
    "    \n",
    "    yield data_source['input_ids'], data_source['token_type_ids'], data_source['attention_mask'], \\\n",
    "          data_target['input_ids'], data_target['token_type_ids'], data_target['attention_mask'], \\\n",
    "          bm25, tf_cosine, tfidf_cosine, \\\n",
    "          cat_source, cat_target, \\\n",
    "          id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9a3586c9",
   "metadata": {
    "id": "A3aaKzFtrubI"
   },
   "outputs": [],
   "source": [
    "def get_test_iterator(data_path, file_name):\n",
    "  it = get_test_keras_bert_iterator(data_path, file_name)\n",
    "  for input_ids_texta, token_type_ids_texta, attention_mask_texta, \\\n",
    "      input_ids_textb, token_type_ids_textb, attention_mask_textb, \\\n",
    "      bm25, tf_cosine, tfidf_cosine, \\\n",
    "      cat_source, cat_target, \\\n",
    "      id in it:\n",
    "    yield [np.array([input_ids_texta]), np.array([token_type_ids_texta]), np.array([attention_mask_texta]), \n",
    "           np.array([input_ids_textb]), np.array([token_type_ids_textb]), np.array([attention_mask_textb]), \n",
    "           np.array([bm25]), np.array([tf_cosine]), np.array([tfidf_cosine]),\n",
    "           np.array([cat_source]), np.array([cat_target])], id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "613142c1",
   "metadata": {
    "id": "NTmqSZRDv065"
   },
   "outputs": [],
   "source": [
    "it = get_test_iterator(data_path, \"test_with_id.txt\")\n",
    "# next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5ef8279f",
   "metadata": {
    "id": "iH42Aoz9r8Sz"
   },
   "outputs": [],
   "source": [
    "def save_test_result(model, result_file):\n",
    "  it = get_test_iterator(data_path, \"test_with_id.txt\")\n",
    "#   print(\"      \", end=\"\")\n",
    "#   count = 0\n",
    "  with open(result_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(f\"id,label\\n\")\n",
    "    for data, id in tqdm(it):\n",
    "      predict = model.predict(data)\n",
    "      if id[-1] == 'a':\n",
    "        predict_cls = 1 if predict[0][0][0] > 0.5 else 0\n",
    "      elif id[-1] == 'b':\n",
    "        predict_cls = 1 if predict[1][0][0] > 0.5 else 0\n",
    "      else:\n",
    "        print(id)\n",
    "        continue\n",
    "      f.write(f\"{id},{predict_cls}\\n\")\n",
    "#       count += 1\n",
    "#       print(f\"\\b\\b\\b\\b\\b\\b{count}\", end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a122dbd",
   "metadata": {
    "id": "e6O6gbcVr8ZC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72181c4",
   "metadata": {
    "id": "jSB9mibcr8cP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179fc94b",
   "metadata": {
    "id": "zHTMitEQr8gT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6517af8a",
   "metadata": {
    "id": "oIln0I3Aru0R"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47df24ef",
   "metadata": {
    "id": "recent-yahoo"
   },
   "source": [
    "## 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "34262f19",
   "metadata": {
    "id": "kOcnb7QxgxZX"
   },
   "outputs": [],
   "source": [
    "def variant_focal_loss(gamma=2., alpha=0.5, rescale = False):\n",
    "\n",
    "    gamma = float(gamma)\n",
    "    alpha = float(alpha)\n",
    "\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        # print(y_true)\n",
    "        \"\"\"\n",
    "        Focal loss for bianry-classification\n",
    "        FL(p_t)=-rescaled_factor*alpha_t*(1-p_t)^{gamma}log(p_t)\n",
    "        \n",
    "        Notice: \n",
    "        y_pred is probability after sigmoid\n",
    "\n",
    "        Arguments:\n",
    "            y_true {tensor} -- groud truth label, shape of [batch_size, 1]\n",
    "            y_pred {tensor} -- predicted label, shape of [batch_size, 1]\n",
    "\n",
    "        Keyword Arguments:\n",
    "            gamma {float} -- (default: {2.0})  \n",
    "            alpha {float} -- (default: {0.5})\n",
    "\n",
    "        Returns:\n",
    "            [tensor] -- loss.\n",
    "        \"\"\"\n",
    "        epsilon = 1.e-9  \n",
    "        y_true = tf.convert_to_tensor(y_true, tf.float32)\n",
    "        y_pred = tf.convert_to_tensor(y_pred, tf.float32)\n",
    "        model_out = tf.clip_by_value(y_pred, epsilon, 1.-epsilon)  # to advoid numeric underflow\n",
    "        \n",
    "        # compute cross entropy ce = ce_0 + ce_1 = - (1-y)*log(1-y_hat) - y*log(y_hat)\n",
    "        ce_0 = tf.multiply(tf.subtract(1., y_true), -tf.math.log(tf.subtract(1., model_out)))\n",
    "        ce_1 = tf.multiply(y_true, -tf.math.log(model_out))\n",
    "\n",
    "        # compute focal loss fl = fl_0 + fl_1\n",
    "        # obviously fl < ce because of the down-weighting, we can fix it by rescaling\n",
    "        # fl_0 = -(1-y_true)*(1-alpha)*((y_hat)^gamma)*log(1-y_hat) = (1-alpha)*((y_hat)^gamma)*ce_0\n",
    "        fl_0 = tf.multiply(tf.pow(model_out, gamma), ce_0)\n",
    "        fl_0 = tf.multiply(1.-alpha, fl_0)\n",
    "        # fl_1= -y_true*alpha*((1-y_hat)^gamma)*log(y_hat) = alpha*((1-y_hat)^gamma*ce_1\n",
    "        fl_1 = tf.multiply(tf.pow(tf.subtract(1., model_out), gamma), ce_1)\n",
    "        fl_1 = tf.multiply(alpha, fl_1)\n",
    "        fl = tf.add(fl_0, fl_1)\n",
    "        f1_avg = tf.reduce_mean(fl)\n",
    "        \n",
    "        if rescale:\n",
    "            # rescale f1 to keep the quantity as ce\n",
    "            ce = tf.add(ce_0, ce_1)\n",
    "            ce_avg = tf.reduce_mean(ce)\n",
    "            rescaled_factor = tf.divide(ce_avg, f1_avg + epsilon)\n",
    "            f1_avg = tf.multiply(rescaled_factor, f1_avg)\n",
    "        \n",
    "        return f1_avg\n",
    "    \n",
    "    return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3a0dc7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_loss(y_true, y_pred):\n",
    "    # y_true:真实标签0或者1；y_pred:为正类的概率\n",
    "    loss = 2 * tf.reduce_sum(y_true * y_pred) / tf.reduce_sum(y_true + y_pred) + K.epsilon()\n",
    "    return -loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "189fb6d2",
   "metadata": {
    "id": "signal-assets"
   },
   "outputs": [],
   "source": [
    "def transform_y(y_true, y_pred):\n",
    "    mask_value = tf.constant(-1)\n",
    "    mask_y_true = tf.not_equal(tf.cast(y_true, dtype=tf.int32), tf.cast(mask_value, dtype=tf.int32))\n",
    "#     print(f\"mask_y_true:{mask_y_true}\")\n",
    "#     y_true_ = tf.cond(tf.equal(y_true, mask_value), lambda: 0, lambda: y_true)\n",
    "    y_true_ = tf.cast(y_true, dtype=tf.int32) * tf.cast(mask_y_true, dtype=tf.int32)\n",
    "    y_pred_ = tf.cast(y_pred, dtype=tf.float32) * tf.cast(mask_y_true, dtype=tf.float32)\n",
    "#     print(f\"y_true_:{y_true_}, y_pred_:{y_pred_}\")\n",
    "    \n",
    "    return tf.cast(y_true_, dtype=tf.float32), tf.cast(y_pred_, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "204a37af",
   "metadata": {
    "id": "strong-assessment"
   },
   "outputs": [],
   "source": [
    "def my_binary_crossentropy(y_true, y_pred):\n",
    "    # print(f\"y_true:{y_true}, y_pred:{y_pred}\")\n",
    "    \n",
    "    y_true, y_pred = transform_y(y_true, y_pred)\n",
    "    # print(f\"y_true_:{y_true}, y_pred_:{y_pred}\")\n",
    "\n",
    "    loss = binary_crossentropy(y_true, y_pred)\n",
    "#     loss = variant_focal_loss()(y_true, y_pred)\n",
    "#     loss = f1_loss(y_true, y_pred)\n",
    "\n",
    "    # y_true0 = tf.constant([y_true.numpy()[0]])\n",
    "    # y_true1 = tf.constant([y_true.numpy()[1]])\n",
    "    # y_pred0 = tf.constant([y_pred.numpy()[0]])\n",
    "    # y_pred1 = tf.constant([y_pred.numpy()[1]])\n",
    "    # loss0 = variant_focal_loss()(y_true0, y_pred0)\n",
    "    # loss1 = variant_focal_loss()(y_true1, y_pred1)\n",
    "    # print(f\"y_true_0:{y_true0}, y_pred_0:{y_pred0}\")\n",
    "    # print(f\"y_true_1:{y_true1}, y_pred_1:{y_pred1}\")\n",
    "    # print(f\"loss0:{loss0}\")\n",
    "    # print(f\"loss1:{loss1}\")\n",
    "\n",
    "    # print(f\"loss:{loss}\")\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0c54df05",
   "metadata": {
    "id": "confused-acoustic"
   },
   "outputs": [],
   "source": [
    "def tarnsform_metrics(y_true, y_pred):\n",
    "    y_true_, y_pred_ = y_true.numpy(), y_pred.numpy()\n",
    "    for i in range(y_true_.shape[0]):\n",
    "        for j in range(y_true_.shape[1]):\n",
    "            if y_true_[i][j] == -1:\n",
    "                y_true_[i][j] = 0\n",
    "                y_pred_[i][j] = random.choice([0, 1])\n",
    "            if y_pred_[i][j] > 0.5:\n",
    "                y_pred_[i][j] = 1\n",
    "            else:\n",
    "                y_pred_[i][j] = 0\n",
    "    return y_true_, y_pred_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ab1bd6ee",
   "metadata": {
    "id": "abroad-monaco"
   },
   "outputs": [],
   "source": [
    "def my_binary_accuracy(y_true, y_pred):\n",
    "#     print(\"my_binary_accuracy\")\n",
    "#     print(f\"y_true:{y_true}, y_pred:{y_pred}\")\n",
    "    \n",
    "    y_true_, y_pred_ = tarnsform_metrics(y_true, y_pred)\n",
    "#     print(f\"y_true_:{y_true_}, y_pred_:{y_pred_}\")\n",
    "\n",
    "    accuracy = binary_accuracy(y_true_, y_pred_)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b0166607",
   "metadata": {
    "id": "radical-chuck"
   },
   "outputs": [],
   "source": [
    "def my_f1_score(y_true, y_pred):\n",
    "#     print(\"my_f1_score\")\n",
    "#     print(f\"y_true:{y_true}, y_pred:{y_pred}\")\n",
    "    \n",
    "    y_true_, y_pred_ = tarnsform_metrics(y_true, y_pred)\n",
    "#     print(f\"y_true_:{y_true_}, y_pred_:{y_pred_}\")\n",
    "\n",
    "    return f1_score(y_true_, y_pred_, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "085a062a",
   "metadata": {
    "id": "standard-tomorrow"
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    K.clear_session()\n",
    "    \n",
    "    bert_model = TFBertModel.from_pretrained(bert_path, from_pt=True, trainable=True)\n",
    "    for l in bert_model.layers:\n",
    "        l.trainable = True\n",
    " \n",
    "    input_ids_texta = Input(shape=(None,), dtype='int32', name='input_ids_texta')\n",
    "    input_token_type_ids_texta = Input(shape=(None,), dtype='int32', name='input_token_type_ids_texta')\n",
    "    input_attention_mask_texta = Input(shape=(None,), dtype='int32', name='input_attention_mask_texta')\n",
    "    input_ids_textb = Input(shape=(None,), dtype='int32', name='input_ids_textb')\n",
    "    input_token_type_ids_textb = Input(shape=(None,), dtype='int32', name='input_token_type_ids_textb')\n",
    "    input_attention_mask_textb = Input(shape=(None,), dtype='int32', name='input_attention_mask_textb')\n",
    "    input_token_type_ids_textb = Input(shape=(None,), dtype='int32', name='input_token_type_ids_textb')\n",
    "    input_bm25 = Input(shape=(1), dtype='float32', name='input_bm25')\n",
    "    input_tf_cosine = Input(shape=(1), dtype='float32', name='input_tf_cosine')\n",
    "    input_tfidf_cosine = Input(shape=(1), dtype='float32', name='input_tfidf_cosine')\n",
    "    input_cat_texta = Input(shape=(1), dtype='float32', name='input_cat_texta')\n",
    "    input_cat_textb = Input(shape=(1), dtype='float32', name='input_cat_textb')\n",
    " \n",
    "    bert_output_texta = bert_model({'input_ids':input_ids_texta, 'token_type_ids':input_token_type_ids_texta, 'attention_mask':input_attention_mask_texta}, return_dict=False, training=True)\n",
    "    projection_logits_texta = bert_output_texta[0]\n",
    "    bert_cls_texta = Lambda(lambda x: x[:, 0])(projection_logits_texta) # 取出[CLS]对应的向量用来做分类\n",
    "\n",
    "    bert_output_textb = bert_model({'input_ids':input_ids_textb, 'token_type_ids':input_token_type_ids_textb, 'attention_mask':input_attention_mask_textb}, return_dict=False, training=True)\n",
    "    projection_logits_textb = bert_output_textb[0]\n",
    "    bert_cls_textb = Lambda(lambda x: x[:, 0])(projection_logits_textb) # 取出[CLS]对应的向量用来做分类\n",
    "    \n",
    "    subtracted = Subtract()([bert_cls_texta, bert_cls_textb])\n",
    "    cos = Dot(axes=1, normalize=True)([bert_cls_texta, bert_cls_textb]) # dot=1按行点积，normalize=True输出余弦相似度\n",
    "\n",
    "    bert_cls = concatenate([bert_cls_texta, bert_cls_textb, subtracted, cos, input_bm25, input_tf_cosine, input_tfidf_cosine, input_cat_texta, input_cat_textb], axis=-1)\n",
    "    \n",
    "    dense_A_0 = Dense(256, activation='relu')(bert_cls)\n",
    "    dropout_A_0 = Dropout(0.2)(dense_A_0)\n",
    "    dense_A_1 = Dense(32, activation='relu')(dropout_A_0)\n",
    "    dropout_A_1 = Dropout(0.2)(dense_A_1)\n",
    "    output_A = Dense(1, activation='sigmoid', name='output_A')(dropout_A_1)\n",
    "    \n",
    "    dense_B_0 = Dense(256, activation='relu')(bert_cls)\n",
    "    dropout_B_0 = Dropout(0.2)(dense_B_0)\n",
    "    dense_B_1 = Dense(32, activation='relu')(dropout_B_0)\n",
    "    dropout_B_1 = Dropout(0.2)(dense_B_1)\n",
    "    output_B = Dense(1, activation='sigmoid', name='output_B')(dropout_B_1)\n",
    " \n",
    "    model = Model([input_ids_texta, input_token_type_ids_texta, input_attention_mask_texta, input_ids_textb, input_token_type_ids_textb, input_attention_mask_textb, input_bm25, input_tf_cosine, input_tfidf_cosine, input_cat_texta, input_cat_textb], [output_A, output_B])\n",
    "    model.compile(\n",
    "#                   loss=my_binary_crossentropy,\n",
    "                  loss={\n",
    "                      'output_A':my_binary_crossentropy,\n",
    "                      'output_B':my_binary_crossentropy,\n",
    "                  },\n",
    "#                   loss='binary_crossentropy',\n",
    "#                   loss=binary_crossentropy,\n",
    "                  optimizer=Adam(1e-5),    #用足够小的学习率\n",
    "                  metrics=[my_binary_accuracy, my_f1_score]\n",
    "#                   metrics='accuracy'\n",
    "                 )\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9167033b",
   "metadata": {
    "id": "foreign-tribe"
   },
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='loss', patience=3)   #早停法，防止过拟合\n",
    "plateau = ReduceLROnPlateau(monitor=\"loss\", verbose=1, factor=0.5, patience=2) #当评价指标不在提升时，减少学习率\n",
    "checkpoint = ModelCheckpoint(check_point_path, monitor='loss', verbose=2, save_best_only=True, save_weights_only=True) #保存最好的模型\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./logs\", update_freq=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124f8d5b",
   "metadata": {
    "id": "rotary-journey"
   },
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6ccc1a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_step(sample_count, batch_size):\n",
    "    step = sample_count // batch_size\n",
    "    if sample_count % batch_size != 0:\n",
    "        step += 1\n",
    "    return step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5ec8fc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 2\n",
    "# epochs = 10\n",
    "\n",
    "# train_dataset_iterator = batch_iter(data_path, [\"train.txt\", \"valid.txt\", \"round2.txt\"], tokenizer, batch_size)\n",
    "# train_step = get_step(sample_count, batch_size)\n",
    "\n",
    "# model = get_model()\n",
    "# plot_model(model, \"keras_bert_transformers_two_text_input_SubStract_bm25cosine_1.png\", show_shapes=True)\n",
    "\n",
    "# model.fit(\n",
    "#     train_dataset_iterator,\n",
    "#     # steps_per_epoch=10,\n",
    "#     steps_per_epoch=train_step,\n",
    "#     epochs=epochs,\n",
    "# #       validation_data=dev_dataset_iterator,\n",
    "#   # validation_steps=2,\n",
    "# #       validation_steps=dev_step,\n",
    "# #     validation_split=0.2,\n",
    "# #     class_weight={\n",
    "# #         'output_A':labelA_class_weights,\n",
    "# #         'output_B':labelB_class_weights,\n",
    "# #     },\n",
    "#     callbacks=[early_stopping, plateau, checkpoint, tensorboard_callback],\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# model.save_weights(weights_path)\n",
    "# # model_json = model.to_json()\n",
    "# # with open(config_path, 'w', encoding='utf-8') as file:\n",
    "# #     file.write(model_json)\n",
    "\n",
    "# save_test_result(model, result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc707ba",
   "metadata": {
    "id": "hiM3cmiokFMQ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'bert.embeddings.position_ids', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_attention_mask_texta (Inp [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_ids_texta (InputLayer)    [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_token_type_ids_texta (Inp [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_attention_mask_textb (Inp [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_ids_textb (InputLayer)    [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_token_type_ids_textb (Inp [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     TFBaseModelOutputWit 102267648   input_attention_mask_texta[0][0] \n",
      "                                                                 input_ids_texta[0][0]            \n",
      "                                                                 input_token_type_ids_texta[0][0] \n",
      "                                                                 input_attention_mask_textb[0][0] \n",
      "                                                                 input_ids_textb[0][0]            \n",
      "                                                                 input_token_type_ids_textb[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 768)          0           tf_bert_model[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 768)          0           tf_bert_model[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "subtract (Subtract)             (None, 768)          0           lambda[0][0]                     \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot (Dot)                       (None, 1)            0           lambda[0][0]                     \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_bm25 (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_tf_cosine (InputLayer)    [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_tfidf_cosine (InputLayer) [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_cat_texta (InputLayer)    [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_cat_textb (InputLayer)    [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 2310)         0           lambda[0][0]                     \n",
      "                                                                 lambda_1[0][0]                   \n",
      "                                                                 subtract[0][0]                   \n",
      "                                                                 dot[0][0]                        \n",
      "                                                                 input_bm25[0][0]                 \n",
      "                                                                 input_tf_cosine[0][0]            \n",
      "                                                                 input_tfidf_cosine[0][0]         \n",
      "                                                                 input_cat_texta[0][0]            \n",
      "                                                                 input_cat_textb[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          591616      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          591616      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 256)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)            (None, 256)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 32)           8224        dropout_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 32)           8224        dropout_39[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 32)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)            (None, 32)           0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output_A (Dense)                (None, 1)            33          dropout_38[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "output_B (Dense)                (None, 1)            33          dropout_40[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 103,467,394\n",
      "Trainable params: 103,467,394\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/data1/wangchenyue/zsd/code/sohu2021/conda_env/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:3504: UserWarning: Even though the tf.config.experimental_run_functions_eagerly option is set, this option does not apply to tf.data functions. tf.data functions are still traced and executed as graphs.\n",
      "  \"Even though the tf.config.experimental_run_functions_eagerly \"\n",
      "8846it [1:59:07,  2.02it/s]"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "# with open(config_path, 'r', encoding='utf-8') as json_file:\n",
    "#     loaded_model_json = json_file.read()\n",
    "# model = model_from_json(loaded_model_json)\n",
    "model.load_weights(check_point_path)\n",
    "save_test_result(model, \"trained_model_substract_1/multi_keras_bert_sohu_test_result_epoch6.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2dfd6d",
   "metadata": {
    "id": "virgin-quarterly"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6287ab7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f4d343",
   "metadata": {
    "id": "tribal-banking"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b9d489",
   "metadata": {
    "id": "likely-celebration"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06708aa7",
   "metadata": {
    "id": "latest-biology"
   },
   "source": [
    "# 模型加载及测试"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7177b6",
   "metadata": {
    "id": "reserved-nightmare"
   },
   "source": [
    "## load_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6520f679",
   "metadata": {
    "id": "sufficient-shadow"
   },
   "source": [
    "## load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00163e3",
   "metadata": {
    "id": "white-adventure"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e635340d",
   "metadata": {
    "id": "structural-shark"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e88531",
   "metadata": {
    "id": "o7jaIjHXf1Pk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ffc019",
   "metadata": {
    "id": "9jQK0PLYf1TC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd15ed44",
   "metadata": {
    "id": "Me0AC8uru-dN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f8e7aa",
   "metadata": {
    "id": "CJbObtBvxoH2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "reserved-nightmare"
   ],
   "name": "keras_bert_transformers_two_text_input.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
